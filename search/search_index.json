{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SQLStream","text":"<p>A lightweight, pure-Python SQL query engine for CSV and Parquet files with lazy evaluation and intelligent optimizations.</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Query a CSV file\n$ sqlstream query \"SELECT * FROM 'data.csv' WHERE age &gt; 25\"\n\n# Join multiple files\n$ sqlstream query \"SELECT c.name, o.total FROM 'customers.csv' c JOIN 'orders.csv' o ON c.id = o.customer_id\"\n\n# Interactive shell with full TUI\n$ sqlstream shell\n\n# Query S3 files\n$ sqlstream query \"SELECT * FROM 's3://my-bucket/data.parquet' WHERE date &gt; '2024-01-01'\"\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p> Pure Python</p> <p>No database installation required. Works anywhere Python runs.</p> </li> <li> <p> Multiple Formats</p> <p>Support for CSV, Parquet files, HTTP URLs, and S3 buckets.</p> </li> <li> <p> 10-100x Faster</p> <p>Optional pandas backend for massive performance boost.</p> </li> <li> <p> JOIN Support</p> <p>INNER, LEFT, RIGHT joins across multiple files.</p> </li> <li> <p> Aggregations</p> <p>GROUP BY with COUNT, SUM, AVG, MIN, MAX functions.</p> </li> <li> <p> Beautiful Output</p> <p>Rich tables, JSON, CSV with syntax highlighting.</p> </li> <li> <p> Interactive Shell</p> <p>Full-featured TUI with modal dialogs, file browser, query plan visualization.</p> </li> <li> <p> Inline File Paths</p> <p>Specify files directly in SQL queries (Phase 7.6).</p> </li> <li> <p> Smart Optimizations</p> <p>Column pruning, predicate pushdown, lazy evaluation.</p> </li> </ul>"},{"location":"#installation","title":"Installation","text":"Basic (CSV only)With ParquetWith Pandas (10-100x faster)All Features <pre><code>pip install sqlstream\n</code></pre> <pre><code>pip install \"sqlstream[parquet]\"\n</code></pre> <pre><code>pip install \"sqlstream[pandas]\"\n</code></pre> <pre><code>pip install \"sqlstream[all]\"\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#cli-usage","title":"CLI Usage","text":"<pre><code># Simple query\n$ sqlstream query data.csv \"SELECT name, age FROM data WHERE age &gt; 25\"\n\n# With output format\n$ sqlstream query data.csv \"SELECT * FROM data\" --format json\n\n# Show execution time\n$ sqlstream query data.csv \"SELECT * FROM data\" --time\n\n# Use pandas backend for performance\n$ sqlstream query data.parquet \"SELECT * FROM data\" --backend pandas\n</code></pre>"},{"location":"#python-api","title":"Python API","text":"<pre><code>from sqlstream import query\n\n# Execute query\nresults = query(\"data.csv\").sql(\"SELECT * FROM data WHERE age &gt; 25\")\n\n# Iterate over results (lazy evaluation)\nfor row in results:\n    print(row)\n\n# Or convert to list\nresults_list = query(\"data.csv\").sql(\"SELECT * FROM data\").to_list()\n</code></pre>"},{"location":"#why-sqlstream","title":"Why SQLStream?","text":"<p>Perfect For</p> <ul> <li>Data Exploration: Quick analysis without database setup</li> <li>ETL Pipelines: Process CSV/Parquet files with SQL</li> <li>Data Science: Filter and join datasets before pandas</li> <li>DevOps: Query logs and data files in CI/CD</li> <li>Learning: Understand query execution internals</li> </ul> <p>Not For</p> <ul> <li>Large Databases: Use PostgreSQL, MySQL instead</li> <li>Real-time Analytics: Use ClickHouse, DuckDB</li> <li>Production OLTP: SQLStream is read-only</li> </ul>"},{"location":"#performance","title":"Performance","text":"<p>SQLStream offers two execution backends:</p> Backend Speed Use Case Python Baseline Learning, small files (&lt;100K rows) Pandas 10-100x faster Production, large files (&gt;100K rows) <p>Performance Tips</p> <ul> <li>Use <code>--backend pandas</code> for files &gt;100K rows</li> <li>Use column pruning: <code>SELECT name, age</code> instead of <code>SELECT *</code></li> <li>Add WHERE filters to reduce data scanned</li> <li>Use Parquet format for better compression</li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li> <p> Quick Start Guide</p> <p>Get up and running in 5 minutes with hands-on examples.</p> </li> <li> <p> SQL Reference</p> <p>Learn about supported SQL syntax and features.</p> </li> <li> <p> CLI Reference</p> <p>Complete guide to the command-line interface.</p> </li> <li> <p> Python API</p> <p>Deep dive into the programmatic API.</p> </li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>SQLStream is in active development. Current phase: 8</p> <ul> <li>\u2705 Phase 0-2: Core query engine with Volcano model</li> <li>\u2705 Phase 3: Parquet support</li> <li>\u2705 Phase 4: Aggregations &amp; GROUP BY</li> <li>\u2705 Phase 5: JOIN operations (INNER, LEFT, RIGHT)</li> <li>\u2705 Phase 5.5: Pandas backend (10-100x speedup)</li> <li>\u2705 Phase 6: HTTP data sources</li> <li>\u2705 Phase 7: CLI with beautiful output</li> <li>\u2705 Phase 7.5: Interactive shell with Textual</li> <li>\u2705 Phase 7.6: Inline file path support</li> <li>\u2705 Phase 7.7: S3 Support for CSV and Parquet</li> <li>\u2705 Phase 8: Type system &amp; schema inference</li> <li>\ud83d\udea7 Phase 9: Enhanced interactive shell (modal dialogs, file browser, query plan)</li> <li>\ud83d\udea7 Phase 10: Error handling &amp; user feedback</li> </ul>"},{"location":"#license","title":"License","text":"<p>SQLStream is licensed under the MIT License.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! See the Contributing Guide for details.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions to SQLStream are welcome!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code>git clone https://github.com/subhayu99/sqlstream.git\ncd sqlstream\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code>pytest tests/\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<pre><code>ruff check .\nruff format .\n</code></pre>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add tests</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#issues","title":"Issues","text":"<p>Report bugs and request features at GitHub Issues.</p>"},{"location":"api/advanced/","title":"Advanced Usage","text":""},{"location":"api/advanced/#backend-selection","title":"Backend Selection","text":"<p>You can explicitly choose the execution backend to optimize for your specific use case.</p> <pre><code># Force pure Python backend (streaming, low memory)\nquery(\"large_file.csv\").sql(\"SELECT *\", backend=\"python\")\n\n# Force Pandas backend (fast, high memory)\nquery(\"data.csv\").sql(\"SELECT *\", backend=\"pandas\")\n</code></pre>"},{"location":"api/advanced/#s3-configuration","title":"S3 Configuration","text":"<p>To access S3, ensure you have <code>s3fs</code> installed and your AWS credentials configured.</p> <pre><code>pip install sqlstream[s3]\nexport AWS_ACCESS_KEY_ID=...\nexport AWS_SECRET_ACCESS_KEY=...\n</code></pre> <p>Then simply use <code>s3://</code> paths.</p>"},{"location":"api/advanced/#custom-readers","title":"Custom Readers","text":"<p>You can implement custom readers by inheriting from <code>BaseReader</code>. This allows you to support proprietary formats or custom data sources.</p> <pre><code>from sqlstream.readers.base import BaseReader\n\nclass MyCustomReader(BaseReader):\n    def read_lazy(self):\n        yield {\"col\": \"value\"}\n</code></pre>"},{"location":"api/overview/","title":"Python API Overview","text":"<p>Use SQLStream programmatically in your Python code.</p>"},{"location":"api/overview/#basic-usage","title":"Basic Usage","text":"<pre><code>from sqlstream import query\n\n# Execute query\nresults = query(\"data.csv\").sql(\"SELECT * FROM data WHERE age &gt; 25\")\n\n# Iterate (lazy)\nfor row in results:\n    print(row)\n\n# Or convert to list (eager)\nresults_list = query(\"data.csv\").sql(\"SELECT * FROM data\").to_list()\n</code></pre>"},{"location":"api/overview/#api-reference","title":"API Reference","text":"<ul> <li>Query Class</li> <li>Readers</li> <li>Advanced Usage</li> </ul>"},{"location":"api/query/","title":"Query API","text":"<p>The <code>Query</code> and <code>QueryInline</code> classes are the primary interfaces for executing SQL against data files.</p>"},{"location":"api/query/#sqlstreamquery","title":"<code>sqlstream.query</code>","text":"<p>A convenience function to create a <code>Query</code> object.</p> <pre><code>def query(source: str) -&gt; Query:\n    ...\n</code></pre> <p>Arguments: - <code>source</code>: Path to the data file (local path, URL, or S3 URI).</p>"},{"location":"api/query/#query-class","title":"<code>Query</code> Class","text":"<p>Represents a query builder for a specific data source.</p>"},{"location":"api/query/#__init__source-str","title":"<code>__init__(source: str)</code>","text":"<p>Initializes the query with a source file.</p>"},{"location":"api/query/#sqlquery-str-backend-str-auto-queryresult","title":"<code>sql(query: str, backend: str = \"auto\") -&gt; QueryResult</code>","text":"<p>Executes a SQL query on the source.</p> <p>Arguments: - <code>query</code>: The SQL query string. - <code>backend</code>: Execution backend (\"auto\", \"pandas\", \"python\").</p> <p>Returns: - <code>QueryResult</code>: An iterable object containing the results.</p>"},{"location":"api/query/#schema-optionalschema","title":"<code>schema() -&gt; Optional[Schema]</code>","text":"<p>Returns the inferred schema of the data source.</p>"},{"location":"api/query/#queryinline-class","title":"<code>QueryInline</code> Class","text":"<p>Allows querying files specified directly in the SQL string.</p>"},{"location":"api/query/#sqlquery-str-backend-str-auto-queryresult_1","title":"<code>sql(query: str, backend: str = \"auto\") -&gt; QueryResult</code>","text":"<p>Executes a SQL query. The source files must be specified in the <code>FROM</code> clause (e.g., <code>FROM 'file.csv'</code>).</p>"},{"location":"api/query/#queryresult-class","title":"<code>QueryResult</code> Class","text":"<p>Represents the result of a query execution. It is lazy and iterable.</p>"},{"location":"api/query/#__iter__","title":"<code>__iter__()</code>","text":"<p>Yields result rows as dictionaries.</p>"},{"location":"api/query/#to_list-listdictstr-any","title":"<code>to_list() -&gt; List[Dict[str, Any]]</code>","text":"<p>Materializes all results into a list.</p>"},{"location":"api/query/#explain-str","title":"<code>explain() -&gt; str</code>","text":"<p>Returns the execution plan for the query.</p>"},{"location":"api/readers/","title":"Readers API","text":"<p>Readers are responsible for abstracting data access from different file formats and storage systems.</p>"},{"location":"api/readers/#basereader","title":"<code>BaseReader</code>","text":"<p>The abstract base class for all readers.</p>"},{"location":"api/readers/#read_lazy-iteratordictstr-any","title":"<code>read_lazy() -&gt; Iterator[Dict[str, Any]]</code>","text":"<p>Yields rows one by one from the source.</p>"},{"location":"api/readers/#get_schema-optionalschema","title":"<code>get_schema() -&gt; Optional[Schema]</code>","text":"<p>Returns the schema of the data source, if available.</p>"},{"location":"api/readers/#csvreader","title":"<code>CSVReader</code>","text":"<p>Reads Comma-Separated Values files.</p> <ul> <li>Source: Local files, URLs, S3.</li> <li>Features: Type inference, header detection.</li> </ul>"},{"location":"api/readers/#parquetreader","title":"<code>ParquetReader</code>","text":"<p>Reads Apache Parquet files.</p> <ul> <li>Source: Local files, URLs, S3.</li> <li>Features: Column pruning (reads only requested columns), predicate pushdown (filters data at the storage level).</li> </ul>"},{"location":"api/readers/#httpreader","title":"<code>HTTPReader</code>","text":"<p>Reads data from HTTP/HTTPS URLs. Usually wraps another reader (like CSVReader) to handle the content format.</p>"},{"location":"architecture/design/","title":"Design Overview","text":"<p>SQLStream is designed as a lightweight, modular SQL query engine. It follows a classic database architecture but is optimized for querying files directly rather than managing storage.</p>"},{"location":"architecture/design/#components","title":"Components","text":"<ol> <li>Parser: Converts SQL strings into an Abstract Syntax Tree (AST).</li> <li>Planner: Converts the AST into a Logical Plan.</li> <li>Optimizer: Applies optimizations like predicate pushdown to the Logical Plan.</li> <li>Executor: Converts the optimized plan into a Physical Plan and executes it.<ul> <li>Volcano Executor: Pure Python, streaming iterator-based.</li> <li>Pandas Executor: Vectorized, in-memory execution.</li> </ul> </li> <li>Readers: Abstractions for reading data from various sources (CSV, Parquet, S3).</li> </ol>"},{"location":"architecture/design/#data-flow","title":"Data Flow","text":"<pre><code>graph LR\n    SQL[SQL Query] --&gt; Parser\n    Parser --&gt; AST\n    AST --&gt; Planner\n    Planner --&gt; LogicalPlan\n    LogicalPlan --&gt; Optimizer\n    Optimizer --&gt; OptimizedPlan\n    OptimizedPlan --&gt; Executor\n    Executor --&gt; Results\n    Readers --&gt; Executor\n</code></pre>"},{"location":"architecture/optimizations/","title":"Query Optimizations","text":"<p>SQLStream implements a pipeline-based optimization framework to ensure efficient query execution.</p>"},{"location":"architecture/optimizations/#optimizer-architecture","title":"Optimizer Architecture","text":"<p>The optimizer module uses a modular pipeline design where each optimization rule is a separate class:</p> <pre><code>from sqlstream.optimizers import QueryPlanner\n\nplanner = QueryPlanner()\nplanner.optimize(ast, reader)\nprint(planner.get_optimization_summary())\n</code></pre> <p>Pipeline Order: 1. Predicate Pushdown (reduce data read) 2. Column Pruning (narrow columns) 3. Limit Pushdown (early termination) 4. Projection Pushdown (transform at source - future)</p>"},{"location":"architecture/optimizations/#1-predicate-pushdown","title":"1. Predicate Pushdown","text":"<p>Filters (WHERE clauses) are \"pushed down\" to the data source for early filtering.</p> <p>Benefits: - Reduces I/O by filtering at the source - Reduces memory usage - Especially effective for columnar formats (Parquet) - Can leverage indexes if available</p> <p>Example: <pre><code>SELECT * FROM data.csv WHERE age &gt; 30\n</code></pre></p> <p>Without pushdown: <pre><code>Read all rows \u2192 Filter in memory \u2192 Return results\n</code></pre></p> <p>With pushdown: <pre><code>Filter while reading \u2192 Return only matching rows\n</code></pre></p> <p>Implementation by Reader:</p> <ul> <li>CSV: Rows are filtered immediately after reading, before processing</li> <li>Parquet: Filters can skip entire row groups based on statistics (min/max values), significantly reducing I/O</li> <li>HTTP: Filters applied after download but before buffering</li> </ul> <p>Limitations: - Currently only supports simple comparisons (column op value) - Does not support complex expressions (e.g., <code>LENGTH(name) &gt; 5</code>) - Does not support cross-column comparisons (e.g., <code>age &gt; salary</code>) - Disabled for JOIN queries (needs smarter per-table analysis)</p>"},{"location":"architecture/optimizations/#2-column-pruning","title":"2. Column Pruning","text":"<p>Only columns required for the query are read from disk.</p> <p>Benefits: - Massive I/O reduction for wide tables - Reduces memory usage - Critical for columnar formats (Parquet, ORC) - Can read 10x faster if selecting 1 column from 10</p> <p>Example: <pre><code>SELECT name, age FROM employees  -- 100 columns total\n</code></pre></p> <p>Without pruning: <pre><code>Read all 100 columns \u2192 Project to 2 columns\n</code></pre></p> <p>With pruning: <pre><code>Read only 2 columns \u2192 Much faster\n</code></pre></p> <p>Implementation by Reader:</p> <ul> <li>Parquet: Only decodes requested columns from file</li> <li>CSV: Whole line is read, but only relevant fields are parsed and kept</li> <li>HTTP: Entire response read, but only needed columns extracted</li> </ul> <p>Column Analysis:</p> <p>The optimizer analyzes which columns are needed from: - SELECT clause - WHERE clause - GROUP BY clause - ORDER BY clause - Aggregate functions - JOIN conditions</p> <p>Limitations: - Cannot prune with <code>SELECT *</code> - CSV still reads full lines (just parses fewer fields)</p>"},{"location":"architecture/optimizations/#3-limit-pushdown","title":"3. Limit Pushdown","text":"<p>LIMIT clauses enable early termination of data reading.</p> <p>Benefits: - Stop reading after N rows - Massive speedup for large files - Reduces memory usage</p> <p>Example: <pre><code>SELECT * FROM large_file.csv LIMIT 10\n</code></pre></p> <p>Without pushdown: <pre><code>Read entire file \u2192 Take first 10 rows\n</code></pre></p> <p>With pushdown: <pre><code>Stop reading after 10 rows \u2192 Much faster\n</code></pre></p> <p>Limitations (Current Implementation): - Not yet implemented in readers (placeholder for future work) - Cannot push down with ORDER BY (need all rows to sort) - Cannot push down with GROUP BY (need all rows to group) - Cannot push down with aggregates (need all rows) - Cannot push down with JOINs (complex - may need all rows)</p> <p>Status: \u26a0\ufe0f Optimizer detects opportunities but readers don't implement yet</p>"},{"location":"architecture/optimizations/#4-projection-pushdown","title":"4. Projection Pushdown","text":"<p>Push computed expressions to the data source for evaluation.</p> <p>Benefits (when implemented): - Evaluate expressions at read time - Reduce data movement - Leverage native database/engine functions</p> <p>Example (future): <pre><code>SELECT UPPER(name), age * 2 FROM data\n</code></pre></p> <p>With pushdown: <pre><code>Reader evaluates UPPER() and age*2 \u2192 Return transformed data\n</code></pre></p> <p>Status: \u26a0\ufe0f Not yet implemented - placeholder for future work</p>"},{"location":"architecture/optimizations/#vectorized-execution-pandas-backend","title":"Vectorized Execution (Pandas Backend)","text":"<p>When using the Pandas backend (<code>backend=\"pandas\"</code>), SQLStream leverages:</p> <ul> <li>SIMD Instructions: CPU-level parallelism for array operations</li> <li>Efficient Memory Layout: Columnar storage in memory</li> <li>Optimized Algorithms: C-optimized implementations of joins and aggregations</li> <li>NumPy: Highly optimized numerical operations</li> </ul> <p>When to use: <pre><code># For large datasets or complex aggregations\nquery(\"data.csv\").sql(\"SELECT * FROM data\", backend=\"pandas\")\n</code></pre></p>"},{"location":"architecture/optimizations/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Query results are iterators - execution only happens when you consume results.</p> <p>Benefits: - Early Termination: <code>LIMIT</code> clauses stop execution as soon as enough rows are found - Streaming: Start processing first results while rest of query runs - Memory Efficient: Don't load entire result set into memory</p> <p>Example: <pre><code>results = query(\"data.csv\").sql(\"SELECT * FROM data LIMIT 10\")\n# No execution yet\n\nfor row in results:  # Execution starts here\n    print(row)\n</code></pre></p>"},{"location":"architecture/optimizations/#optimization-summary","title":"Optimization Summary","text":"<p>You can see which optimizations were applied:</p> <pre><code>plan = query(\"data.csv\").sql(\"\"\"\n    SELECT name, age\n    FROM data\n    WHERE age &gt; 30\n\"\"\", backend=\"python\").explain()\n\nprint(plan)\n</code></pre> <p>Output: <pre><code>Query Plan:\n  Scan: data.csv\n  Filter: age &gt; 30\n  Project: name, age\n\nOptimizations applied:\n  - Predicate pushdown: 1 condition(s)\n  - Column pruning: 3 column(s) selected\n</code></pre></p>"},{"location":"architecture/optimizations/#custom-optimizers","title":"Custom Optimizers","text":"<p>You can add custom optimization rules:</p> <pre><code>from sqlstream.optimizers import QueryPlanner, Optimizer\n\nclass MyCustomOptimizer(Optimizer):\n    def get_name(self) -&gt; str:\n        return \"My custom rule\"\n\n    def can_optimize(self, ast, reader) -&gt; bool:\n        # Check if optimization applies\n        return True\n\n    def optimize(self, ast, reader) -&gt; None:\n        # Apply optimization\n        self.applied = True\n        self.description = \"did something cool\"\n\nplanner = QueryPlanner()\nplanner.add_optimizer(MyCustomOptimizer())\n</code></pre>"},{"location":"architecture/optimizations/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Parquet for wide tables: Column pruning is most effective</li> <li>Use WHERE early: Predicate pushdown reduces data read</li> <li>Select specific columns: Avoid <code>SELECT *</code> when possible</li> <li>Use LIMIT for exploration: Quick previews of large files</li> <li>Use Pandas backend for aggregations: Faster for GROUP BY queries</li> <li>Check explain plans: Use <code>.explain()</code> to see which optimizations applied</li> </ol>"},{"location":"architecture/optimizations/#future-optimizations-roadmap","title":"Future Optimizations (Roadmap)","text":"<ul> <li>\u2705 Predicate pushdown (implemented)</li> <li>\u2705 Column pruning (implemented)</li> <li>\u23f3 Limit pushdown (detected but not implemented in readers)</li> <li>\u23f3 Projection pushdown (placeholder)</li> <li>\u23f3 Partition pruning (for partitioned Parquet)</li> <li>\u23f3 Join reordering (optimize join order)</li> <li>\u23f3 Aggregate pushdown (push GROUP BY to readers)</li> <li>\u23f3 Index usage (when indexes available)</li> <li>\u23f3 Parallel execution (multi-threaded readers)</li> <li>\u23f3 Adaptive query execution (runtime optimization)</li> </ul>"},{"location":"architecture/volcano-model/","title":"Volcano Model","text":"<p>The default execution model in SQLStream is based on the Volcano Iterator Model (also known as the Open-Next-Close model).</p>"},{"location":"architecture/volcano-model/#how-it-works","title":"How it Works","text":"<p>Each operator in the query plan (Scan, Filter, Project, Join) implements a standard interface with three methods:</p> <ol> <li><code>open()</code>: Initialize the operator.</li> <li><code>next()</code>: Retrieve the next tuple (row).</li> <li><code>close()</code>: Clean up resources.</li> </ol>"},{"location":"architecture/volcano-model/#execution-flow","title":"Execution Flow","text":"<p>When a query is executed, the top-level operator calls <code>next()</code> on its child, which calls <code>next()</code> on its child, and so on, down to the Scan operator which reads from the file.</p> <pre><code># Simplified representation\nclass FilterOperator:\n    def next(self):\n        while True:\n            row = self.child.next()\n            if row is None:\n                return None\n            if self.predicate(row):\n                return row\n</code></pre>"},{"location":"architecture/volcano-model/#benefits","title":"Benefits","text":"<ul> <li>Low Memory Footprint: Data is processed one row at a time. The entire dataset does not need to be loaded into memory.</li> <li>Pipelining: No intermediate results need to be materialized (except for blocking operators like Sort or Aggregate).</li> <li>Simplicity: Easy to implement and extend.</li> </ul>"},{"location":"architecture/volcano-model/#trade-offs","title":"Trade-offs","text":"<ul> <li>CPU Overhead: Function call overhead for every row can be significant in Python.</li> <li>Performance: Slower than vectorized execution for large datasets. This is why SQLStream also offers a Pandas backend.</li> </ul>"},{"location":"cli/interactive-mode/","title":"Interactive SQL Shell","text":"<p>SQLStream includes a powerful interactive shell built with Textual, providing a modern terminal UI for exploring and querying data.</p>"},{"location":"cli/interactive-mode/#features","title":"Features","text":"<ul> <li>\ud83c\udfa8 Syntax Highlighting - Dracula theme for SQL queries</li> <li>\ud83d\udcca Scrollable Results - Zebra-striped table with smooth scrolling</li> <li>\ud83d\udcdc Query History - Navigate previous queries with keyboard shortcuts</li> <li>\ud83d\uddc2\ufe0f Schema Browser - View file schemas with column types</li> <li>\ud83d\udcc4 Pagination - Handle large result sets (100 rows per page)</li> <li>\ud83d\udd00 Column Sorting - Click headers to sort ascending/descending</li> <li>\ud83d\udcbe Multi-Format Export - Save results as CSV, JSON, and Parquet</li> <li>\ud83d\udd0d Filtering - Search across all columns</li> <li>\u2601\ufe0f S3 Support - Query files directly from S3 buckets</li> <li>\u26a1 Fast - Execution time display and row counts</li> </ul>"},{"location":"cli/interactive-mode/#installation","title":"Installation","text":"<p>The interactive shell requires the <code>textual</code> library:</p> <pre><code># Install with CLI support\npip install \"sqlstream[cli]\"\n\n# Or install all features\npip install \"sqlstream[all]\"\n</code></pre>"},{"location":"cli/interactive-mode/#getting-started","title":"Getting Started","text":""},{"location":"cli/interactive-mode/#launch-the-shell","title":"Launch the Shell","text":"<pre><code># Empty shell\nsqlstream shell\n\n# With initial file\nsqlstream shell employees.csv\n\n# Custom history location\nsqlstream shell --history-file ~/.my_sqlstream_history\n</code></pre>"},{"location":"cli/interactive-mode/#basic-usage","title":"Basic Usage","text":"<ol> <li>Write a query in the editor (supports multi-line)</li> <li>Execute with <code>Ctrl+Enter</code> or <code>Ctrl+E</code></li> <li>View results in the table below</li> <li>Navigate large result sets with pagination</li> <li>Export results with <code>Ctrl+X</code></li> </ol>"},{"location":"cli/interactive-mode/#keybindings","title":"Keybindings","text":"Key Action Description <code>Ctrl+Enter</code> Execute Query Run the query in editor <code>Ctrl+E</code> Execute Query Alternative execution key <code>Ctrl+L</code> Clear Editor Clear query text <code>Ctrl+D</code> Exit Close the shell <code>F1</code> Help Show help message <code>F2</code> Toggle Schema Show/hide schema browser <code>F4</code> Explain Mode Show query plan <code>Ctrl+O</code> Open File Browse files to add to query <code>Ctrl+X</code> Export Export with custom filename <code>Ctrl+F</code> Filter Filter current results <code>[</code> Previous Page Navigate to previous page <code>]</code> Next Page Navigate to next page <code>Ctrl+Up</code> Prev Query Load previous from history <code>Ctrl+Down</code> Next Query Load next from history Click Header Sort Column Sort by column (click again to reverse)"},{"location":"cli/interactive-mode/#query-examples","title":"Query Examples","text":""},{"location":"cli/interactive-mode/#local-files","title":"Local Files","text":"<pre><code>-- Simple query\nSELECT * FROM 'employees.csv' WHERE age &gt; 30\n\n-- Aggregations\nSELECT department, COUNT(*) as count, AVG(salary) as avg_salary\nFROM 'employees.csv'\nGROUP BY department\nORDER BY avg_salary DESC\n\n-- JOINs\nSELECT e.name, e.salary, d.department_name\nFROM 'employees.csv' e\nJOIN 'departments.csv' d ON e.dept_id = d.id\n</code></pre>"},{"location":"cli/interactive-mode/#s3-files","title":"S3 Files","text":"<pre><code>-- Query S3 CSV\nSELECT * FROM 's3://my-bucket/data.csv' WHERE date &gt; '2024-01-01'\n\n-- Query S3 Parquet with aggregation\nSELECT product_id, SUM(revenue) as total\nFROM 's3://my-bucket/sales.parquet'\nWHERE date &gt; '2024-01-01'\nGROUP BY product_id\nORDER BY total DESC\nLIMIT 10\n</code></pre>"},{"location":"cli/interactive-mode/#http-files","title":"HTTP Files","text":"<pre><code>SELECT * FROM 'https://example.com/data.csv'\nWHERE category = 'electronics'\n</code></pre>"},{"location":"cli/interactive-mode/#advanced-features","title":"Advanced Features","text":""},{"location":"cli/interactive-mode/#1-query-history","title":"1. Query History","text":"<p>The shell maintains a persistent history of your queries (up to 100 queries).</p> <p>Location: <code>~/.sqlstream_history</code> (or custom with <code>--history-file</code>)</p> <p>Navigation: - <code>Ctrl+Up</code> - Load previous query - <code>Ctrl+Down</code> - Load next query</p> <p>Behavior: - History loads automatically on startup - Each executed query is saved - Navigate through history without re-executing</p>"},{"location":"cli/interactive-mode/#2-schema-browser","title":"2. Schema Browser","text":"<p>Press <code>F2</code> to toggle the schema browser panel.</p> <p>Shows: - All loaded files - Column names (green) - Data types (dim text) - Errors (red)</p> <p>Example: <pre><code>Data Sources\n\u251c\u2500 employees.csv\n\u2502  \u251c\u2500 name: string\n\u2502  \u251c\u2500 age: int\n\u2502  \u251c\u2500 city: string\n\u2502  \u2514\u2500 salary: float\n\u2514\u2500 sales.parquet\n   \u251c\u2500 product_id: int\n   \u251c\u2500 revenue: float\n   \u2514\u2500 date: date\n</code></pre></p> <p>Features: - Asynchronous schema loading (non-blocking) - Updates automatically when querying new files - Helps discover available columns before writing queries</p>"},{"location":"cli/interactive-mode/#3-pagination","title":"3. Pagination","text":"<p>When a query returns more than 100 rows, results are automatically paginated.</p> <p>Status Display: <pre><code>Showing 101-200 of 450 rows | Page 2/5\n</code></pre></p> <p>Navigation: - <code>Ctrl+N</code> or <code>]</code> - Next page - <code>Ctrl+P</code> or <code>[</code> - Previous page - Sorting and filtering reset to page 1</p> <p>Performance: - Only 100 rows rendered at a time - Instant navigation between pages - Handles millions of rows efficiently</p>"},{"location":"cli/interactive-mode/#4-column-sorting","title":"4. Column Sorting","text":"<p>Click any column header to sort results.</p> <p>Behavior: 1. First click: Sort ascending (\u2191) 2. Second click: Sort descending (\u2193) 3. Click another column: Sort by that column</p> <p>Status Display: <pre><code>Sorted by salary \u2193\n</code></pre></p> <p>Notes: - Sorting works across all pages - Resets current page to 1 - Works with filtered results</p>"},{"location":"cli/interactive-mode/#5-multi-format-export","title":"5. Multi-Format Export","text":"<p>Press <code>Ctrl+X</code> to export current results to multiple formats simultaneously.</p> <p>Exported Formats: - CSV: <code>results_YYYYMMDD_HHMMSS.csv</code> - JSON: <code>results_YYYYMMDD_HHMMSS.json</code> (pretty-printed) - Parquet: <code>results_YYYYMMDD_HHMMSS.parquet</code> (if <code>pyarrow</code> installed)</p> <p>Example: <pre><code>Exported to: CSV (results_20241130_143022.csv),\n             JSON (results_20241130_143022.json),\n             Parquet (results_20241130_143022.parquet)\n</code></pre></p> <p>Notes: - Exports current page or filtered results - Timestamped filenames prevent overwrites - Parquet export requires <code>pip install pyarrow</code></p>"},{"location":"cli/interactive-mode/#6-filtering","title":"6. Filtering","text":"<p>Press <code>Ctrl+F</code> to filter current results.</p> <p>Features: - Case-insensitive search - Searches across all columns - Updates row count in status bar</p> <p>Example: <pre><code>Filtered to 45 rows (from 450 total)\n</code></pre></p>"},{"location":"cli/interactive-mode/#performance","title":"Performance","text":"Feature Performance Pagination Shows first 100 rows instantly Sorting In-memory sort of all results Filtering Scans all rows once, then cached S3 Loading Streams data, doesn't load all into memory Schema Loading Async worker, doesn't block UI"},{"location":"cli/interactive-mode/#examples","title":"Examples","text":""},{"location":"cli/interactive-mode/#example-1-explore-large-dataset","title":"Example 1: Explore Large Dataset","text":"<pre><code>-- Load first 1000 rows\nSELECT * FROM 'big_file.csv' LIMIT 1000\n\n-- Results: 1000 rows \u2192 10 pages\n-- Use ] to navigate pages\n-- Click 'revenue' header to sort\n-- Use Ctrl+X to export\n</code></pre>"},{"location":"cli/interactive-mode/#example-2-s3-analytics","title":"Example 2: S3 Analytics","text":"<pre><code>-- Query S3 Parquet\nSELECT\n    category,\n    COUNT(*) as count,\n    AVG(price) as avg_price\nFROM 's3://my-bucket/products.parquet'\nGROUP BY category\nORDER BY count DESC\n\n-- Click 'count' to sort\n-- Export to CSV for sharing\n</code></pre>"},{"location":"cli/interactive-mode/#example-3-schema-exploration","title":"Example 3: Schema Exploration","text":"<pre><code>-- Press F2 to see schema\n-- Write query with column names visible\nSELECT name, age, city\nFROM 'employees.csv'\nWHERE age &gt; 25\nORDER BY name\n\n-- Sort by different columns using headers\n-- Export to JSON for API use\n</code></pre>"},{"location":"cli/interactive-mode/#tips-tricks","title":"Tips &amp; Tricks","text":"<ol> <li>Large Datasets: Use <code>LIMIT</code> to preview data quickly</li> <li>S3 Performance: Use partitioned Parquet files for best performance</li> <li>History: Use <code>Ctrl+Up</code> to quickly re-run previous queries</li> <li>Sorting: Click column headers to explore data patterns</li> <li>Export: Export to Parquet for best compression</li> <li>Schema: Press F2 before writing queries to see available columns</li> </ol>"},{"location":"cli/interactive-mode/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/interactive-mode/#issue-footer-visibility","title":"Issue: Footer Visibility","text":"<p>Problem: Footer may be clipped on small terminal windows</p> <p>Solution: Increase terminal height or scroll down</p>"},{"location":"cli/interactive-mode/#issue-keybinding-conflicts","title":"Issue: Keybinding Conflicts","text":"<p>Problem: Some keybindings don't work in VSCode terminal</p> <p>Solution: Use a native terminal (gnome-terminal, iTerm2, Windows Terminal)</p>"},{"location":"cli/interactive-mode/#issue-textual-not-installed","title":"Issue: Textual Not Installed","text":"<p>Problem: <code>ImportError: No module named 'textual'</code></p> <p>Solution: <code>pip install \"sqlstream[cli]\"</code></p>"},{"location":"cli/interactive-mode/#next-steps","title":"Next Steps","text":"<ul> <li>Query Command - Learn about non-interactive queries</li> <li>Output Formats - Formatting options</li> <li>S3 Support - Query cloud data</li> <li>SQL Support - Supported SQL syntax</li> </ul>"},{"location":"cli/output-formats/","title":"Output Formats","text":"<p>The SQLStream CLI supports multiple output formats to suit different needs, from human-readable tables to machine-parsable JSON.</p>"},{"location":"cli/output-formats/#specifying-output-format","title":"Specifying Output Format","text":"<p>Use the <code>--format</code> option with the <code>query</code> command.</p> <pre><code>sqlstream query \"SELECT * FROM 'data.csv'\" --format &lt;format&gt;\n</code></pre>"},{"location":"cli/output-formats/#supported-formats","title":"Supported Formats","text":""},{"location":"cli/output-formats/#table-default","title":"Table (Default)","text":"<p>Displays results in a formatted ASCII table using the <code>rich</code> library. Best for human inspection.</p> <pre><code>sqlstream query \"SELECT name, age FROM 'users.csv'\"\n</code></pre> <p>Output: <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 name \u2503 age \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Alice\u2502 30  \u2502\n\u2502 Bob  \u2502 25  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"cli/output-formats/#csv","title":"CSV","text":"<p>Outputs standard Comma-Separated Values. Useful for piping to other tools or saving to files.</p> <pre><code>sqlstream query \"SELECT * FROM 'data.csv'\" --format csv\n</code></pre> <p>Output: <pre><code>name,age\nAlice,30\nBob,25\n</code></pre></p>"},{"location":"cli/output-formats/#json","title":"JSON","text":"<p>Outputs a JSON array of objects. Ideal for web applications or processing with <code>jq</code>.</p> <pre><code>sqlstream query \"SELECT * FROM 'data.csv'\" --format json\n</code></pre> <p>Output: <pre><code>[\n  {\"name\": \"Alice\", \"age\": 30},\n  {\"name\": \"Bob\", \"age\": 25}\n]\n</code></pre></p>"},{"location":"cli/output-formats/#markdown","title":"Markdown","text":"<p>Outputs a Markdown-formatted table. Great for generating documentation.</p> <pre><code>sqlstream query \"SELECT * FROM 'data.csv'\" --format markdown\n</code></pre> <p>Output: <pre><code>| name | age |\n|------|-----|\n| Alice| 30  |\n| Bob  | 25  |\n</code></pre></p>"},{"location":"cli/output-formats/#interactive-shell-export","title":"Interactive Shell Export","text":"<p>In the interactive shell (<code>sqlstream shell</code>), you can export results using <code>Ctrl+X</code>. This automatically exports the current result set to CSV, JSON, and Parquet (if available) simultaneously, saving them with a timestamped filename.</p>"},{"location":"cli/overview/","title":"CLI Overview","text":"<p>The <code>sqlstream</code> command-line interface provides an easy way to query data files with SQL.</p>"},{"location":"cli/overview/#available-commands","title":"Available Commands","text":""},{"location":"cli/overview/#query-execute-sql-queries","title":"<code>query</code> - Execute SQL Queries","text":"<p>Execute SQL queries on data files and display results.</p> <pre><code>sqlstream query [FILE] &lt;SQL&gt; [OPTIONS]\n</code></pre> <p>Examples: <pre><code># Query a file\nsqlstream query data.csv \"SELECT * FROM data WHERE age &gt; 25\"\n\n# Inline file path\nsqlstream query \"SELECT * FROM 'data.csv' WHERE age &gt; 25\"\n\n# JSON output\nsqlstream query data.csv \"SELECT * FROM data\" --format json\n\n# S3 files\nsqlstream query \"SELECT * FROM 's3://bucket/data.parquet' LIMIT 100\"\n</code></pre></p> <p>See Query Command for full documentation.</p>"},{"location":"cli/overview/#shell-interactive-sql-shell","title":"<code>shell</code> - Interactive SQL Shell","text":"<p>Launch the interactive shell with full TUI (Terminal User Interface).</p> <pre><code>sqlstream shell [FILE]\n</code></pre> <p>Features: - Modal dialogs for filtering, export, file selection - File browser (<code>Ctrl+O</code>) to select files - Query execution plan visualization (<code>F4</code>) - Multi-format export (<code>Ctrl+X</code>) - Live filtering (<code>Ctrl+F</code>) - Schema browser (<code>F2</code>) - Query history with multiline support</p> <p>See Interactive Mode for full documentation.</p>"},{"location":"cli/overview/#global-options","title":"Global Options","text":"<ul> <li><code>--version</code> - Show SQLStream version</li> <li><code>--help</code> - Show help message</li> </ul>"},{"location":"cli/overview/#quick-start","title":"Quick Start","text":"<pre><code># Simple query\nsqlstream query employees.csv \"SELECT * FROM employees WHERE salary &gt; 80000\"\n\n# With pandas backend for performance\nsqlstream query large.csv \"SELECT * FROM large\" --backend pandas\n\n# Launch interactive shell\nsqlstream shell employees.csv\n</code></pre>"},{"location":"cli/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Query Command Reference - Detailed query command options</li> <li>Interactive Shell Guide - Full interactive shell documentation</li> <li>Output Formats - Available output formats</li> </ul>"},{"location":"cli/query-command/","title":"Query Command","text":"<p>Execute SQL queries on CSV and Parquet files.</p>"},{"location":"cli/query-command/#syntax","title":"Syntax","text":"<pre><code>sqlstream query [FILE_OR_SQL] [SQL] [OPTIONS]\n</code></pre>"},{"location":"cli/query-command/#arguments","title":"Arguments","text":"<ul> <li><code>FILE_OR_SQL</code> - File path or SQL query (optional if SQL contains inline paths)</li> <li><code>SQL</code> - SQL query string (optional if using inline paths)</li> </ul>"},{"location":"cli/query-command/#options","title":"Options","text":""},{"location":"cli/query-command/#output-format","title":"Output Format","text":"<ul> <li><code>-f, --format [table|json|csv]</code> - Output format (default: table)</li> <li><code>-o, --output FILE</code> - Write output to file</li> </ul>"},{"location":"cli/query-command/#performance","title":"Performance","text":"<ul> <li><code>-b, --backend [auto|pandas|python]</code> - Execution backend (default: auto)</li> <li><code>-l, --limit N</code> - Limit displayed rows</li> </ul>"},{"location":"cli/query-command/#display","title":"Display","text":"<ul> <li><code>--no-color</code> - Disable colored output</li> <li><code>-i, --interactive</code> - Force interactive mode</li> <li><code>--no-interactive</code> - Disable interactive mode</li> <li><code>-t, --time</code> - Show execution time</li> </ul>"},{"location":"cli/query-command/#debugging","title":"Debugging","text":"<ul> <li><code>--explain</code> - Show query execution plan</li> </ul>"},{"location":"cli/query-command/#examples","title":"Examples","text":"<p>See Query Examples</p>"},{"location":"examples/aggregations/","title":"Aggregation Examples","text":""},{"location":"examples/aggregations/#basic-counting","title":"Basic Counting","text":"<p>Count total rows in a file.</p> <pre><code>SELECT COUNT(*) FROM 'logs.csv'\n</code></pre>"},{"location":"examples/aggregations/#grouping-by-category","title":"Grouping by Category","text":"<p>Calculate average salary by department.</p> <pre><code>SELECT \n    department, \n    AVG(salary) as avg_salary \nFROM 'employees.csv' \nGROUP BY department\n</code></pre>"},{"location":"examples/aggregations/#multiple-aggregations","title":"Multiple Aggregations","text":"<p>Compute multiple statistics at once.</p> <pre><code>SELECT \n    category, \n    COUNT(*) as item_count, \n    MIN(price) as min_price, \n    MAX(price) as max_price, \n    AVG(price) as avg_price \nFROM 'products.csv' \nGROUP BY category\n</code></pre>"},{"location":"examples/basic-queries/","title":"Basic Query Examples","text":"<p>Common query patterns and examples.</p>"},{"location":"examples/basic-queries/#filtering","title":"Filtering","text":"<pre><code>SELECT * FROM data WHERE age &gt; 25\nSELECT * FROM data WHERE city = 'NYC'\nSELECT * FROM data WHERE salary &gt;= 80000\n</code></pre>"},{"location":"examples/basic-queries/#sorting","title":"Sorting","text":"<pre><code>SELECT * FROM data ORDER BY age DESC\nSELECT * FROM data ORDER BY salary DESC LIMIT 10\n</code></pre>"},{"location":"examples/basic-queries/#aggregations","title":"Aggregations","text":"<pre><code>SELECT COUNT(*) FROM data\nSELECT AVG(salary) FROM data\nSELECT department, COUNT(*) FROM data GROUP BY department\n</code></pre> <p>See more: Join Examples | Aggregations</p>"},{"location":"examples/joins/","title":"JOIN Examples","text":""},{"location":"examples/joins/#joining-csv-files","title":"Joining CSV Files","text":"<p>Suppose you have <code>users.csv</code> and <code>orders.csv</code>.</p> <p>users.csv <pre><code>id,name,email\n1,Alice,alice@example.com\n2,Bob,bob@example.com\n</code></pre></p> <p>orders.csv <pre><code>order_id,user_id,amount\n101,1,50.00\n102,1,25.00\n103,2,100.00\n</code></pre></p> <p>Query: <pre><code>SELECT \n    u.name, \n    o.amount \nFROM 'users.csv' u \nJOIN 'orders.csv' o \nON u.id = o.user_id\n</code></pre></p>"},{"location":"examples/joins/#joining-csv-and-parquet","title":"Joining CSV and Parquet","text":"<p>Query: <pre><code>SELECT \n    p.product_name, \n    s.quantity \nFROM 'products.csv' p \nJOIN 'sales.parquet' s \nON p.id = s.product_id\n</code></pre></p>"},{"location":"examples/joins/#self-join","title":"Self Join","text":"<p>Query: <pre><code>SELECT \n    e1.name as employee, \n    e2.name as manager \nFROM 'employees.csv' e1 \nJOIN 'employees.csv' e2 \nON e1.manager_id = e2.id\n</code></pre></p>"},{"location":"examples/real-world/","title":"Real-World Use Cases","text":""},{"location":"examples/real-world/#sales-analysis","title":"Sales Analysis","text":"<p>Analyze sales data spread across daily CSV files.</p> <p><pre><code>SELECT \n    product_id, \n    SUM(amount) as total_sales \nFROM 'sales_*.csv' \nGROUP BY product_id \nORDER BY total_sales DESC \nLIMIT 5\n</code></pre> (Note: Wildcard support depends on shell expansion or specific reader implementation)</p>"},{"location":"examples/real-world/#log-analysis","title":"Log Analysis","text":"<p>Find the top 10 IP addresses with the most 404 errors from a web server log.</p> <pre><code>SELECT \n    ip_address, \n    COUNT(*) as error_count \nFROM 'access_logs.csv' \nWHERE status_code = 404 \nGROUP BY ip_address \nORDER BY error_count DESC \nLIMIT 10\n</code></pre>"},{"location":"examples/real-world/#data-quality-check","title":"Data Quality Check","text":"<p>Identify records with missing critical information.</p> <pre><code>SELECT * \nFROM 'users.parquet' \nWHERE email IS NULL OR phone IS NULL\n</code></pre>"},{"location":"examples/real-world/#cross-reference-data","title":"Cross-Reference Data","text":"<p>Check which users in your CSV database have placed orders recorded in an S3 Parquet data lake.</p> <pre><code>SELECT \n    u.email \nFROM 'local_users.csv' u \nJOIN 's3://datalake/orders.parquet' o \nON u.id = o.user_id \nGROUP BY u.email\n</code></pre>"},{"location":"features/aggregations/","title":"Aggregations","text":"<p>SQLStream supports standard SQL aggregation functions, allowing you to perform calculations on your data.</p>"},{"location":"features/aggregations/#supported-functions","title":"Supported Functions","text":"<ul> <li>COUNT(*): Counts the total number of rows.</li> <li>COUNT(column): Counts the number of non-null values in a column.</li> <li>SUM(column): Calculates the sum of a numeric column.</li> <li>AVG(column): Calculates the average value of a numeric column.</li> <li>MIN(column): Finds the minimum value.</li> <li>MAX(column): Finds the maximum value.</li> </ul>"},{"location":"features/aggregations/#group-by","title":"GROUP BY","text":"<p>You can group results by one or more columns using the <code>GROUP BY</code> clause.</p> <pre><code>SELECT \n    category, \n    COUNT(*) as count, \n    AVG(price) as avg_price \nFROM 'products.csv' \nGROUP BY category\n</code></pre>"},{"location":"features/aggregations/#backend-differences","title":"Backend Differences","text":"<ul> <li>Python Backend: Computes aggregations in a streaming fashion where possible, or accumulates state for grouping.</li> <li>Pandas Backend: Uses optimized Pandas <code>groupby</code> and aggregation methods for high performance.</li> </ul>"},{"location":"features/data-sources/","title":"Data Sources","text":"<p>SQLStream supports multiple data source types.</p>"},{"location":"features/data-sources/#csv-files","title":"CSV Files","text":"<pre><code>from sqlstream import query\n\n# Local CSV\nresults = query(\"data.csv\").sql(\"SELECT * FROM data\")\n\n# CSV with custom delimiter\n# Auto-detected: comma, tab, pipe, semicolon\n</code></pre>"},{"location":"features/data-sources/#parquet-files","title":"Parquet Files","text":"<pre><code>pip install \"sqlstream[parquet]\"\n</code></pre> <pre><code>results = query(\"data.parquet\").sql(\"SELECT * FROM data\")\n</code></pre>"},{"location":"features/data-sources/#http-urls","title":"HTTP URLs","text":"<pre><code>pip install \"sqlstream[http]\"\n</code></pre> <pre><code>results = query(\"https://example.com/data.csv\").sql(\"SELECT * FROM data\")\n</code></pre>"},{"location":"features/data-sources/#inline-paths","title":"Inline Paths","text":"<pre><code>sqlstream query \"SELECT * FROM 'data.csv'\"\nsqlstream query \"SELECT * FROM 'data.parquet'\"\nsqlstream query \"SELECT * FROM 'https://example.com/data.csv'\"\n</code></pre>"},{"location":"features/inline-paths/","title":"Inline File Paths","text":"<p>One of SQLStream's most powerful features is the ability to query files directly by specifying their paths in the SQL query. This eliminates the need to define \"tables\" or load data beforehand.</p>"},{"location":"features/inline-paths/#syntax","title":"Syntax","text":"<p>Simply enclose the file path in single quotes <code>'</code> within the <code>FROM</code> clause.</p> <pre><code>SELECT * FROM 'path/to/file.csv'\n</code></pre>"},{"location":"features/inline-paths/#supported-path-types","title":"Supported Path Types","text":""},{"location":"features/inline-paths/#local-files","title":"Local Files","text":"<p>Relative or absolute paths to local files.</p> <pre><code>SELECT * FROM 'data.csv'\nSELECT * FROM '/home/user/datasets/sales.parquet'\n</code></pre>"},{"location":"features/inline-paths/#httphttps-urls","title":"HTTP/HTTPS URLs","text":"<p>You can query data directly from the web.</p> <pre><code>SELECT * FROM 'https://raw.githubusercontent.com/datasets/covid-19/master/data/countries-aggregated.csv'\n</code></pre>"},{"location":"features/inline-paths/#s3-buckets","title":"S3 Buckets","text":"<p>If you have the <code>s3fs</code> library installed (<code>pip install sqlstream[s3]</code>), you can query files directly from S3.</p> <pre><code>SELECT * FROM 's3://my-bucket/my-data.parquet'\n</code></pre>"},{"location":"features/inline-paths/#multi-file-queries","title":"Multi-File Queries","text":"<p>You can join data from different locations and formats in a single query.</p> <pre><code>SELECT \n    local.id, \n    remote.value \nFROM 'local_data.csv' local\nJOIN 's3://bucket/remote_data.parquet' remote\nON local.id = remote.id\n</code></pre>"},{"location":"features/joins/","title":"JOIN Operations","text":"<p>SQLStream supports joining data from multiple files, allowing you to combine datasets based on common columns. This is particularly powerful as it allows you to treat separate files (CSV, Parquet) as if they were tables in a relational database.</p>"},{"location":"features/joins/#syntax","title":"Syntax","text":"<p>The syntax follows standard SQL conventions. You can specify the files directly in the query using string literals.</p> <pre><code>SELECT \n    t1.col1, \n    t2.col2 \nFROM 'file1.csv' t1 \nJOIN 'file2.csv' t2 \nON t1.id = t2.id\n</code></pre>"},{"location":"features/joins/#supported-join-types","title":"Supported Join Types","text":"<p>SQLStream currently supports:</p> <ul> <li>INNER JOIN: Returns records that have matching values in both tables.</li> <li>LEFT JOIN: Returns all records from the left table, and the matched records from the right table.</li> </ul>"},{"location":"features/joins/#cross-format-joins","title":"Cross-Format Joins","text":"<p>You can join files of different formats. For example, you can join a CSV file with a Parquet file:</p> <pre><code>SELECT \n    users.name, \n    orders.amount \nFROM 'users.csv' users \nJOIN 'orders.parquet' orders \nON users.user_id = orders.user_id\n</code></pre>"},{"location":"features/joins/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Backend: Using the <code>pandas</code> backend is generally faster for joins on larger datasets as it leverages optimized merge algorithms.</li> <li>Memory: The Python backend uses a nested-loop or hash join implementation which streams data, making it memory efficient but potentially slower for large datasets compared to Pandas.</li> </ul>"},{"location":"features/pandas-backend/","title":"Pandas Backend","text":"<p>SQLStream includes a high-performance execution backend powered by pandas. This backend is designed for scenarios where performance is critical and the dataset fits into memory.</p>"},{"location":"features/pandas-backend/#enabling-the-pandas-backend","title":"Enabling the Pandas Backend","text":"<p>You can specify the backend when executing a query:</p>"},{"location":"features/pandas-backend/#python-api","title":"Python API","text":"<pre><code>from sqlstream import query\n\n# Force pandas backend\nresult = query(\"data.csv\").sql(\"SELECT * FROM data\", backend=\"pandas\")\n\n# Auto mode (default) - uses pandas if available\nresult = query(\"data.csv\").sql(\"SELECT * FROM data\", backend=\"auto\")\n</code></pre>"},{"location":"features/pandas-backend/#cli","title":"CLI","text":"<p>The CLI automatically attempts to use the pandas backend if pandas is installed.</p>"},{"location":"features/pandas-backend/#benefits","title":"Benefits","text":"<ol> <li>Vectorized Execution: Operations are performed on entire arrays at once rather than row-by-row, leading to significant speedups.</li> <li>Optimized Joins: Leverages pandas' highly optimized merge algorithms.</li> <li>Efficient Aggregations: Grouping and aggregation are much faster.</li> </ol>"},{"location":"features/pandas-backend/#fallback-mechanism","title":"Fallback Mechanism","text":"<p>When <code>backend=\"auto\"</code> is used (the default), SQLStream checks if <code>pandas</code> is installed. - If installed: It uses the PandasExecutor. - If not installed: It falls back to the pure Python VolcanoExecutor.</p> <p>This ensures that SQLStream remains lightweight and functional even without heavy dependencies, while offering performance when they are available.</p>"},{"location":"features/pandas-backend/#limitations","title":"Limitations","text":"<ul> <li>Memory Usage: The pandas backend loads data into memory (DataFrames). For datasets larger than available RAM, the streaming Python backend might be more appropriate (though slower).</li> </ul>"},{"location":"features/s3-support/","title":"S3 Support","text":"<p>SQLStream can read CSV and Parquet files directly from Amazon S3 buckets, enabling you to query cloud-stored data without downloading files locally.</p>"},{"location":"features/s3-support/#installation","title":"Installation","text":"<p>S3 support requires the <code>s3fs</code> library:</p> <pre><code># Install with S3 support\npip install \"sqlstream[s3]\"\n\n# Or install all features\npip install \"sqlstream[all]\"\n</code></pre>"},{"location":"features/s3-support/#authentication","title":"Authentication","text":"<p>SQLStream uses your AWS credentials through <code>s3fs</code>. Configure credentials using any of these methods:</p>"},{"location":"features/s3-support/#option-1-aws-credentials-file","title":"Option 1: AWS Credentials File","text":"<pre><code># ~/.aws/credentials\n[default]\naws_access_key_id = YOUR_ACCESS_KEY\naws_secret_access_key = YOUR_SECRET_KEY\n</code></pre>"},{"location":"features/s3-support/#option-2-environment-variables","title":"Option 2: Environment Variables","text":"<pre><code>export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY\nexport AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre>"},{"location":"features/s3-support/#option-3-iam-roles","title":"Option 3: IAM Roles","text":"<p>When running on EC2 or ECS, SQLStream automatically uses IAM role credentials.</p>"},{"location":"features/s3-support/#basic-usage","title":"Basic Usage","text":""},{"location":"features/s3-support/#cli-usage","title":"CLI Usage","text":"<p>Query S3 files using <code>s3://</code> URLs:</p> <pre><code># CSV files\nsqlstream query \"SELECT * FROM 's3://my-bucket/data.csv' WHERE age &gt; 25\"\n\n# Parquet files\nsqlstream query \"SELECT * FROM 's3://my-bucket/data.parquet' LIMIT 100\"\n\n# With output formatting\nsqlstream query \"SELECT * FROM 's3://my-bucket/sales.csv'\" --format json\n\n# Using pandas backend for performance\nsqlstream query \"SELECT * FROM 's3://my-bucket/large.parquet'\" --backend pandas\n</code></pre>"},{"location":"features/s3-support/#python-api","title":"Python API","text":"<pre><code>from sqlstream import query\n\n# Query S3 CSV\nresults = query(\"s3://my-bucket/employees.csv\").sql(\"\"\"\n    SELECT name, salary\n    FROM data\n    WHERE department = 'Engineering'\n    ORDER BY salary DESC\n\"\"\")\n\nfor row in results:\n    print(row)\n</code></pre>"},{"location":"features/s3-support/#interactive-shell","title":"Interactive Shell","text":"<pre><code># Launch shell and query S3\nsqlstream shell\n\n# Then run queries\nSELECT * FROM 's3://my-bucket/data.csv' WHERE date &gt; '2024-01-01'\n</code></pre>"},{"location":"features/s3-support/#advanced-examples","title":"Advanced Examples","text":""},{"location":"features/s3-support/#example-1-aggregations-on-s3-data","title":"Example 1: Aggregations on S3 Data","text":"<pre><code>from sqlstream import query\n\n# Sales analysis from S3\nresults = query(\"s3://analytics-bucket/sales-2024.parquet\").sql(\"\"\"\n    SELECT\n        product_category,\n        COUNT(*) as num_sales,\n        SUM(amount) as total_revenue,\n        AVG(amount) as avg_sale\n    FROM data\n    WHERE sale_date &gt;= '2024-01-01'\n    GROUP BY product_category\n    ORDER BY total_revenue DESC\n\"\"\", backend=\"pandas\")\n\nfor row in results:\n    print(f\"{row['product_category']}: ${row['total_revenue']:,.2f}\")\n</code></pre>"},{"location":"features/s3-support/#example-2-join-s3-and-local-files","title":"Example 2: JOIN S3 and Local Files","text":"<pre><code>from sqlstream.core.query import QueryInline\n\nq = QueryInline()\n\n# Join S3 data with local reference data\nresults = q.sql(\"\"\"\n    SELECT\n        s.customer_id,\n        s.order_total,\n        c.customer_name,\n        c.region\n    FROM 's3://orders-bucket/orders.parquet' s\n    JOIN 'customers.csv' c ON s.customer_id = c.id\n    WHERE s.order_date = '2024-11-30'\n\"\"\")\n\nfor row in results:\n    print(row)\n</code></pre>"},{"location":"features/s3-support/#performance-tips","title":"Performance Tips","text":""},{"location":"features/s3-support/#1-use-parquet-for-large-datasets","title":"1. Use Parquet for Large Datasets","text":"<p>Parquet files offer: - Faster queries (columnar format, only read needed columns) - Smaller size (better compression than CSV) - Row group pruning (skip irrelevant data blocks)</p>"},{"location":"features/s3-support/#2-leverage-column-pruning","title":"2. Leverage Column Pruning","text":"<pre><code># \u2705 GOOD: Select specific columns\nSELECT name, email FROM data\n\n# \u274c SLOW: Select all columns\nSELECT * FROM data\n</code></pre>"},{"location":"features/s3-support/#3-use-pandas-backend-for-aggregations","title":"3. Use Pandas Backend for Aggregations","text":"<pre><code>results = query(\"s3://bucket/sales.parquet\").sql(\"\"\"\n    SELECT region, SUM(revenue) as total\n    FROM data\n    GROUP BY region\n\"\"\", backend=\"pandas\")  # 10-100x faster!\n</code></pre>"},{"location":"features/s3-support/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/s3-support/#missing-s3fs-package","title":"Missing s3fs Package","text":"<pre><code>ImportError: s3fs is required for S3 support. Install with: pip install sqlstream[s3]\n</code></pre> <p>Solution: <code>pip install \"sqlstream[s3]\"</code></p>"},{"location":"features/s3-support/#access-denied","title":"Access Denied","text":"<p>Ensure your AWS credentials have <code>s3:GetObject</code> permission for the bucket.</p>"},{"location":"features/s3-support/#no-credentials-configured","title":"No Credentials Configured","text":"<p>Configure AWS credentials using one of the methods described in the Authentication section.</p>"},{"location":"features/s3-support/#next-steps","title":"Next Steps","text":"<ul> <li>Interactive Shell - Query S3 interactively</li> <li>Data Sources - Learn about other supported formats</li> <li>Performance - Optimize your queries</li> </ul>"},{"location":"features/sql-support/","title":"SQL Support","text":"<p>SQLStream supports a practical subset of SQL designed for data exploration and ETL tasks.</p>"},{"location":"features/sql-support/#supported-syntax","title":"Supported Syntax","text":""},{"location":"features/sql-support/#select","title":"SELECT","text":"<pre><code>-- Select all columns\nSELECT * FROM data\n\n-- Select specific columns\nSELECT name, age, city FROM data\n\n-- With table alias\nSELECT d.name, d.age FROM data d\n</code></pre>"},{"location":"features/sql-support/#where","title":"WHERE","text":"<pre><code>-- Simple conditions\nSELECT * FROM data WHERE age &gt; 25\nSELECT * FROM data WHERE name = 'Alice'\nSELECT * FROM data WHERE salary &gt;= 80000\n\n-- Multiple conditions with AND\nSELECT * FROM data WHERE age &gt; 25 AND city = 'NYC'\nSELECT * FROM data WHERE salary &gt; 80000 AND department = 'Engineering'\n</code></pre> <p>Supported operators: <code>=</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code>, <code>!=</code>, <code>&lt;&gt;</code></p>"},{"location":"features/sql-support/#group-by","title":"GROUP BY","text":"<pre><code>-- Simple grouping\nSELECT city, COUNT(*) FROM data GROUP BY city\n\n-- Multiple columns\nSELECT department, city, AVG(salary) FROM data GROUP BY department, city\n\n-- With WHERE\nSELECT city, COUNT(*) FROM data WHERE age &gt; 25 GROUP BY city\n</code></pre>"},{"location":"features/sql-support/#aggregate-functions","title":"Aggregate Functions","text":"<pre><code>SELECT COUNT(*) FROM data\nSELECT COUNT(id) FROM data\nSELECT SUM(salary) FROM data\nSELECT AVG(age) FROM data\nSELECT MIN(salary) FROM data\nSELECT MAX(salary) FROM data\n</code></pre> <p>With aliases: <pre><code>SELECT department, COUNT(*) AS employee_count, AVG(salary) AS avg_salary\nFROM data\nGROUP BY department\n</code></pre></p>"},{"location":"features/sql-support/#join","title":"JOIN","text":"<pre><code>-- INNER JOIN\nSELECT * FROM employees e\nINNER JOIN departments d ON e.dept_id = d.id\n\n-- LEFT JOIN\nSELECT * FROM employees e\nLEFT JOIN departments d ON e.dept_id = d.id\n\n-- RIGHT JOIN\nSELECT * FROM employees e\nRIGHT JOIN departments d ON e.dept_id = d.id\n</code></pre>"},{"location":"features/sql-support/#order-by","title":"ORDER BY","text":"<pre><code>-- Ascending (default)\nSELECT * FROM data ORDER BY age\nSELECT * FROM data ORDER BY age ASC\n\n-- Descending\nSELECT * FROM data ORDER BY salary DESC\n\n-- Multiple columns\nSELECT * FROM data ORDER BY city ASC, age DESC\n</code></pre>"},{"location":"features/sql-support/#limit","title":"LIMIT","text":"<pre><code>-- Top 10 rows\nSELECT * FROM data LIMIT 10\n\n-- With ORDER BY\nSELECT * FROM data ORDER BY salary DESC LIMIT 5\n</code></pre>"},{"location":"features/sql-support/#complete-example","title":"Complete Example","text":"<pre><code>SELECT\n    department,\n    COUNT(*) AS employee_count,\n    AVG(salary) AS avg_salary,\n    MIN(salary) AS min_salary,\n    MAX(salary) AS max_salary\nFROM employees\nWHERE hire_date &gt; '2020-01-01'\n  AND status = 'active'\nGROUP BY department\nORDER BY avg_salary DESC\nLIMIT 10\n</code></pre>"},{"location":"features/sql-support/#inline-file-paths-phase-76","title":"Inline File Paths (Phase 7.6)","text":"<pre><code># Single file\nsqlstream query \"SELECT * FROM 'data.csv' WHERE age &gt; 25\"\n\n# Multiple files with JOIN\nsqlstream query \"SELECT c.name, o.total FROM 'customers.csv' c JOIN 'orders.csv' o ON c.id = o.customer_id\"\n\n# Quoted paths (for spaces)\nsqlstream query \"SELECT * FROM '/path/with spaces/data.csv'\"\n</code></pre>"},{"location":"features/sql-support/#not-yet-supported","title":"Not Yet Supported","text":"<p>The following SQL features are planned but not yet implemented:</p> <ul> <li>\u274c Subqueries</li> <li>\u274c UNION/INTERSECT/EXCEPT</li> <li>\u274c HAVING clause</li> <li>\u274c CASE expressions</li> <li>\u274c String functions (UPPER, LOWER, etc.)</li> <li>\u274c Date functions</li> <li>\u274c Window functions</li> <li>\u274c Common Table Expressions (WITH)</li> </ul>"},{"location":"features/sql-support/#next-steps","title":"Next Steps","text":"<ul> <li>JOIN Examples</li> <li>Aggregation Examples</li> <li>Inline File Paths</li> </ul>"},{"location":"features/type-system/","title":"Type System &amp; Schema Inference","text":"<p>SQLStream includes a robust type system that automatically infers data types from your files and validates operations.</p>"},{"location":"features/type-system/#supported-types","title":"Supported Types","text":"<p>SQLStream supports six core data types:</p> Type Python Types Example Values <code>INTEGER</code> <code>int</code> <code>42</code>, <code>-100</code>, <code>0</code> <code>FLOAT</code> <code>float</code> <code>3.14</code>, <code>-2.5</code>, <code>0.0</code> <code>STRING</code> <code>str</code> <code>\"hello\"</code>, <code>\"Alice\"</code> <code>BOOLEAN</code> <code>bool</code> <code>true</code>, <code>false</code> <code>DATE</code> <code>date</code>, <code>datetime</code> <code>2024-01-15</code>, <code>2023-12-31T12:34:56.789</code> <code>NULL</code> <code>None</code> Empty values"},{"location":"features/type-system/#automatic-type-inference","title":"Automatic Type Inference","text":"<p>SQLStream automatically infers types from your data:</p>"},{"location":"features/type-system/#from-python-values","title":"From Python Values","text":"<pre><code>from sqlstream.core.types import infer_type, DataType\n\ninfer_type(42)           # DataType.INTEGER\ninfer_type(3.14)         # DataType.FLOAT\ninfer_type(\"hello\")      # DataType.STRING\ninfer_type(True)         # DataType.BOOLEAN\ninfer_type(None)         # DataType.NULL\n</code></pre>"},{"location":"features/type-system/#from-string-values","title":"From String Values","text":"<p>When reading CSV files, SQLStream tries to infer the most specific type:</p> <pre><code>infer_type(\"42\")         # DataType.INTEGER (not STRING)\ninfer_type(\"3.14\")       # DataType.FLOAT\ninfer_type(\"true\")       # DataType.BOOLEAN\ninfer_type(\"2024-01-15\") # DataType.DATE\ninfer_type(\"hello\")      # DataType.STRING\n</code></pre>"},{"location":"features/type-system/#handling-mixed-types","title":"Handling Mixed Types","text":"<p>When a column has mixed types, SQLStream promotes to the most general compatible type:</p> <pre><code>from sqlstream.core.types import infer_common_type\n\n# Mixed integers and floats -&gt; FLOAT\ninfer_common_type([1, 2.5, 3])           # DataType.FLOAT\n\n# Mixed types -&gt; STRING\ninfer_common_type([1, \"hello\", 3])       # DataType.STRING\n\n# NULL values are ignored\ninfer_common_type([1, None, 3])          # DataType.INTEGER\n</code></pre>"},{"location":"features/type-system/#schema-inference","title":"Schema Inference","text":"<p>SQLStream automatically infers the schema (column names and types) when reading files.</p>"},{"location":"features/type-system/#basic-usage","title":"Basic Usage","text":"<pre><code>from sqlstream import query\n\n# Create query object\nq = query(\"employees.csv\")\n\n# Get inferred schema\nschema = q.schema()\n\n# Check column types\nprint(schema[\"name\"])    # DataType.STRING\nprint(schema[\"age\"])     # DataType.INTEGER\nprint(schema[\"salary\"])  # DataType.FLOAT\n</code></pre>"},{"location":"features/type-system/#schema-object","title":"Schema Object","text":"<p>The <code>Schema</code> object provides helpful methods:</p> <pre><code># Get all column names\ncolumns = schema.get_column_names()\n# ['name', 'age', 'salary', 'hire_date']\n\n# Get type of a column\nage_type = schema.get_column_type(\"age\")\n# DataType.INTEGER\n\n# Check if column exists\nif \"email\" in schema:\n    print(\"Email column exists\")\n\n# Validate column\ntry:\n    schema.validate_column(\"invalid_column\")\nexcept ValueError as e:\n    print(e)  # Column 'invalid_column' not found\n</code></pre>"},{"location":"features/type-system/#sample-size","title":"Sample Size","text":"<p>By default, SQLStream samples 100 rows to infer types. You can adjust this:</p> <pre><code>from sqlstream.readers.csv_reader import CSVReader\n\nreader = CSVReader(\"large_file.csv\")\n\n# Sample only 10 rows (faster)\nschema = reader.get_schema(sample_size=10)\n\n# Sample 1000 rows (more accurate)\nschema = reader.get_schema(sample_size=1000)\n</code></pre>"},{"location":"features/type-system/#type-checking","title":"Type Checking","text":""},{"location":"features/type-system/#numeric-types","title":"Numeric Types","text":"<p>Check if a type is numeric:</p> <pre><code>DataType.INTEGER.is_numeric()  # True\nDataType.FLOAT.is_numeric()    # True\nDataType.STRING.is_numeric()   # False\n</code></pre>"},{"location":"features/type-system/#type-compatibility","title":"Type Compatibility","text":"<p>Check if two types can be compared:</p> <pre><code># Same types are compatible\nDataType.INTEGER.is_comparable(DataType.INTEGER)  # True\n\n# Numeric types are compatible\nDataType.INTEGER.is_comparable(DataType.FLOAT)    # True\n\n# String and number are not compatible\nDataType.STRING.is_comparable(DataType.INTEGER)   # False\n\n# NULL is compatible with everything\nDataType.NULL.is_comparable(DataType.STRING)      # True\n</code></pre>"},{"location":"features/type-system/#type-coercion","title":"Type Coercion","text":"<p>When mixing types, SQLStream promotes to the more general type:</p> <pre><code># INT + FLOAT -&gt; FLOAT\nDataType.INTEGER.coerce_to(DataType.FLOAT)  # DataType.FLOAT\n\n# NULL + anything -&gt; that type\nDataType.NULL.coerce_to(DataType.INTEGER)   # DataType.INTEGER\n\n# Incompatible types -&gt; STRING\nDataType.INTEGER.coerce_to(DataType.STRING) # DataType.STRING\n</code></pre>"},{"location":"features/type-system/#practical-examples","title":"Practical Examples","text":""},{"location":"features/type-system/#example-1-validate-query-columns","title":"Example 1: Validate Query Columns","text":"<pre><code>from sqlstream import query\n\nq = query(\"employees.csv\")\nschema = q.schema()\n\n# Validate SELECT columns before executing\nselect_cols = [\"name\", \"age\", \"salary\"]\nfor col in select_cols:\n    try:\n        schema.validate_column(col)\n    except ValueError:\n        print(f\"Column '{col}' doesn't exist!\")\n</code></pre>"},{"location":"features/type-system/#example-2-check-column-types","title":"Example 2: Check Column Types","text":"<pre><code>from sqlstream import query\n\nq = query(\"sales.csv\")\nschema = q.schema()\n\n# Find all numeric columns\nnumeric_cols = [\n    col for col in schema.get_column_names()\n    if schema[col].is_numeric()\n]\nprint(f\"Numeric columns: {numeric_cols}\")\n</code></pre>"},{"location":"features/type-system/#example-3-type-safe-filtering","title":"Example 3: Type-Safe Filtering","text":"<pre><code>from sqlstream import query\nfrom sqlstream.core.types import DataType\n\nq = query(\"products.csv\")\nschema = q.schema()\n\n# Only filter on numeric columns\nif schema[\"price\"].is_numeric():\n    results = q.sql(\"SELECT * FROM data WHERE price &gt; 100\")\nelse:\n    print(\"Price column is not numeric!\")\n</code></pre>"},{"location":"features/type-system/#schema-merging","title":"Schema Merging","text":"<p>When working with multiple files (e.g., in JOINs), SQLStream can merge schemas:</p> <pre><code>from sqlstream.core.types import Schema, DataType\n\n# Two schemas with overlapping columns\nschema1 = Schema({\n    \"id\": DataType.INTEGER,\n    \"value\": DataType.INTEGER\n})\n\nschema2 = Schema({\n    \"id\": DataType.INTEGER,\n    \"value\": DataType.FLOAT  # Different type!\n})\n\n# Merge schemas\nmerged = schema1.merge(schema2)\n\n# 'value' column is promoted to FLOAT\nprint(merged[\"value\"])  # DataType.FLOAT\n</code></pre>"},{"location":"features/type-system/#best-practices","title":"Best Practices","text":""},{"location":"features/type-system/#1-check-schema-before-querying","title":"1. Check Schema Before Querying","text":"<pre><code>schema = query(\"data.csv\").schema()\n\n# Verify expected columns exist\nrequired = [\"id\", \"name\", \"amount\"]\nfor col in required:\n    schema.validate_column(col)  # Raises error if missing\n</code></pre>"},{"location":"features/type-system/#2-use-type-information","title":"2. Use Type Information","text":"<pre><code>schema = query(\"data.csv\").schema()\n\n# Only perform numeric operations on numeric columns\nif schema[\"age\"].is_numeric():\n    results = query(\"data.csv\").sql(\"SELECT AVG(age) FROM data\")\n</code></pre>"},{"location":"features/type-system/#3-handle-null-values","title":"3. Handle NULL Values","text":"<pre><code>from sqlstream.core.types import DataType\n\nschema = query(\"data.csv\").schema()\n\n# Check if column might have nulls\nif schema[\"optional_field\"] == DataType.NULL:\n    print(\"This column is all nulls!\")\n</code></pre>"},{"location":"features/type-system/#4-sample-size-tradeoff","title":"4. Sample Size Tradeoff","text":"<pre><code>from sqlstream.readers.csv_reader import CSVReader\n\n# Small sample (fast, less accurate)\nschema = CSVReader(\"file.csv\").get_schema(sample_size=10)\n\n# Large sample (slower, more accurate)\nschema = CSVReader(\"file.csv\").get_schema(sample_size=1000)\n</code></pre>"},{"location":"features/type-system/#type-system-api-reference","title":"Type System API Reference","text":""},{"location":"features/type-system/#datatype-methods","title":"DataType Methods","text":"<ul> <li><code>is_numeric()</code> - Check if type is INTEGER or FLOAT</li> <li><code>is_comparable(other)</code> - Check if compatible for comparison</li> <li><code>coerce_to(other)</code> - Determine result type of coercion</li> </ul>"},{"location":"features/type-system/#schema-methods","title":"Schema Methods","text":"<ul> <li><code>get_column_names()</code> - Get list of column names</li> <li><code>get_column_type(column)</code> - Get type of column (or None)</li> <li><code>validate_column(column)</code> - Raise error if column doesn't exist</li> <li><code>merge(other)</code> - Merge two schemas with type coercion</li> <li><code>from_row(row)</code> - Create schema from single row</li> <li><code>from_rows(rows)</code> - Create schema from multiple rows (more accurate)</li> </ul>"},{"location":"features/type-system/#type-inference-functions","title":"Type Inference Functions","text":"<ul> <li><code>infer_type(value)</code> - Infer type from Python value</li> <li><code>infer_common_type(values)</code> - Infer common type from list of values</li> </ul>"},{"location":"features/type-system/#next-steps","title":"Next Steps","text":"<ul> <li>SQL Support - See what SQL features use the type system</li> <li>Data Sources - Learn about different file formats</li> <li>Python API - Use the type system programmatically</li> </ul>"},{"location":"getting-started/core-concepts/","title":"Core Concepts","text":"<p>Understanding the key concepts behind SQLStream will help you use it more effectively.</p>"},{"location":"getting-started/core-concepts/#architecture-overview","title":"Architecture Overview","text":"<p>SQLStream is built around these core components:</p> <pre><code>graph LR\n    A[SQL Query] --&gt; B[Parser]\n    B --&gt; C[AST]\n    C --&gt; D[Planner]\n    D --&gt; E[Optimized Plan]\n    E --&gt; F[Executor]\n    F --&gt; G[Operators]\n    G --&gt; H[Readers]\n    H --&gt; I[Data Sources]\n</code></pre>"},{"location":"getting-started/core-concepts/#the-volcano-model","title":"The Volcano Model","text":"<p>SQLStream uses the Volcano iterator model for query execution:</p>"},{"location":"getting-started/core-concepts/#what-is-it","title":"What is it?","text":"<p>Each operator in the query plan implements two methods:</p> <ul> <li><code>open()</code>: Initialize the operator</li> <li><code>next()</code>: Return the next row (or None when done)</li> </ul>"},{"location":"getting-started/core-concepts/#why-volcano","title":"Why Volcano?","text":"<p>\u2705 Lazy Evaluation: Rows are produced on-demand \u2705 Low Memory: Only one row in memory at a time \u2705 Composable: Operators stack like LEGO blocks \u2705 Predictable: Easy to understand and debug</p>"},{"location":"getting-started/core-concepts/#example","title":"Example","text":"<pre><code>SELECT name FROM employees WHERE salary &gt; 80000 LIMIT 5\n</code></pre> <p>Execution flow:</p> <pre><code>Limit(5)\n  \u2193 next()\nProject(name)\n  \u2193 next()\nFilter(salary &gt; 80000)\n  \u2193 next()\nScan(employees.csv)\n  \u2193 next()\nCSV Reader\n</code></pre> <p>Each operator calls <code>next()</code> on the operator below it until it gets a row.</p>"},{"location":"getting-started/core-concepts/#execution-backends","title":"Execution Backends","text":"<p>SQLStream offers two execution backends:</p>"},{"location":"getting-started/core-concepts/#python-backend","title":"Python Backend","text":"<p>How it works: Pure Python implementation using the Volcano model</p> <p>Pros:</p> <ul> <li>\u2705 No dependencies</li> <li>\u2705 Easy to understand</li> <li>\u2705 Works everywhere</li> </ul> <p>Cons:</p> <ul> <li>\u274c Slower for large datasets</li> <li>\u274c Row-at-a-time processing</li> </ul> <p>Best for:</p> <ul> <li>Learning and education</li> <li>Small files (&lt;100K rows)</li> <li>Quick prototyping</li> </ul>"},{"location":"getting-started/core-concepts/#pandas-backend","title":"Pandas Backend","text":"<p>How it works: Translates SQL to pandas operations</p> <p>Pros:</p> <ul> <li>\u2705 10-100x faster than Python backend</li> <li>\u2705 Vectorized operations</li> <li>\u2705 Optimized C code</li> </ul> <p>Cons:</p> <ul> <li>\u274c Requires pandas dependency</li> <li>\u274c Loads full dataset into memory</li> </ul> <p>Best for:</p> <ul> <li>Production workloads</li> <li>Large files (&gt;100K rows)</li> <li>Performance-critical applications</li> </ul>"},{"location":"getting-started/core-concepts/#query-optimizations","title":"Query Optimizations","text":"<p>SQLStream applies several optimizations automatically:</p>"},{"location":"getting-started/core-concepts/#1-column-pruning","title":"1. Column Pruning","text":"<p>What: Only read columns that are actually used</p> <p>Example:</p> <pre><code>SELECT name FROM employees\n</code></pre> <p>SQLStream only reads the <code>name</code> column from the CSV, not all columns.</p> <p>Benefit: Faster I/O, less memory</p>"},{"location":"getting-started/core-concepts/#2-predicate-pushdown","title":"2. Predicate Pushdown","text":"<p>What: Apply filters as early as possible</p> <p>Example:</p> <pre><code>SELECT name FROM employees WHERE department = 'Engineering'\n</code></pre> <p>The filter is applied during the scan, not after loading all rows.</p> <p>Benefit: Fewer rows to process</p>"},{"location":"getting-started/core-concepts/#3-lazy-evaluation","title":"3. Lazy Evaluation","text":"<p>What: Only compute results when needed</p> <p>Example:</p> <pre><code>results = query(\"data.csv\").sql(\"SELECT * FROM data LIMIT 10\")\n# Nothing executed yet!\n\nfor row in results:\n    print(row)  # Now it executes, one row at a time\n</code></pre> <p>Benefit: Save computation for unused results</p>"},{"location":"getting-started/core-concepts/#data-sources","title":"Data Sources","text":"<p>SQLStream supports multiple data sources:</p>"},{"location":"getting-started/core-concepts/#csv-files","title":"CSV Files","text":"<pre><code>query(\"data.csv\")\n</code></pre> <ul> <li>Automatic delimiter detection</li> <li>Header row inference</li> <li>Type inference (strings, numbers)</li> </ul>"},{"location":"getting-started/core-concepts/#parquet-files","title":"Parquet Files","text":"<pre><code>query(\"data.parquet\")  # Requires: pip install \"sqlstream[parquet]\"\n</code></pre> <ul> <li>Columnar storage</li> <li>Better compression</li> <li>Schema included</li> </ul>"},{"location":"getting-started/core-concepts/#http-urls","title":"HTTP URLs","text":"<pre><code>query(\"https://example.com/data.csv\")  # Requires: pip install \"sqlstream[http]\"\n</code></pre> <ul> <li>Streaming support</li> <li>Automatic format detection</li> <li>Caching (planned)</li> </ul>"},{"location":"getting-started/core-concepts/#inline-paths-phase-76","title":"Inline Paths (Phase 7.6)","text":"<pre><code>sqlstream query \"SELECT * FROM 'data.csv'\"\n</code></pre> <ul> <li>No need to pre-specify file</li> <li>Multi-file queries</li> <li>More intuitive</li> </ul>"},{"location":"getting-started/core-concepts/#lazy-vs-eager-evaluation","title":"Lazy vs. Eager Evaluation","text":""},{"location":"getting-started/core-concepts/#lazy-default","title":"Lazy (Default)","text":"<pre><code>result = query(\"data.csv\").sql(\"SELECT * FROM data\")\n# \u2705 Nothing executed yet\n\nfor row in result:  # Executes one row at a time\n    print(row)\n</code></pre> <p>Advantages:</p> <ul> <li>Low memory usage</li> <li>Can process infinite streams</li> <li>Early termination possible</li> </ul>"},{"location":"getting-started/core-concepts/#eager","title":"Eager","text":"<pre><code>result = query(\"data.csv\").sql(\"SELECT * FROM data\").to_list()\n# \u274c Executes immediately, loads all data\n</code></pre> <p>Advantages:</p> <ul> <li>Random access to results</li> <li>Easier to work with</li> <li>Can get length with <code>len()</code></li> </ul> <p>Choose lazy when:</p> <ul> <li>Processing large files</li> <li>Only need first N results</li> <li>Streaming to another system</li> </ul> <p>Choose eager when:</p> <ul> <li>Results fit in memory</li> <li>Need to access results multiple times</li> <li>Using with pandas/numpy</li> </ul>"},{"location":"getting-started/core-concepts/#query-lifecycle","title":"Query Lifecycle","text":"<ol> <li>Parse: SQL string \u2192 AST (Abstract Syntax Tree)</li> <li>Plan: AST \u2192 Execution plan</li> <li>Optimize: Apply optimizations (column pruning, etc.)</li> <li>Execute: Build operator pipeline</li> <li>Iterate: Pull rows through the pipeline</li> </ol>"},{"location":"getting-started/core-concepts/#memory-model","title":"Memory Model","text":""},{"location":"getting-started/core-concepts/#python-backend_1","title":"Python Backend","text":"<pre><code>Memory Usage = O(1)  # One row at a time\n</code></pre> <p>Perfect for:</p> <ul> <li>Large files that don't fit in RAM</li> <li>Streaming applications</li> <li>Long-running processes</li> </ul>"},{"location":"getting-started/core-concepts/#pandas-backend_1","title":"Pandas Backend","text":"<pre><code>Memory Usage = O(n)  # Full dataset in memory\n</code></pre> <p>Perfect for:</p> <ul> <li>Files that fit in RAM</li> <li>Multiple passes over data</li> <li>Complex aggregations</li> </ul>"},{"location":"getting-started/core-concepts/#error-handling","title":"Error Handling","text":"<p>SQLStream provides helpful error messages:</p>"},{"location":"getting-started/core-concepts/#parse-errors","title":"Parse Errors","text":"<pre><code>SELECT * FORM data  # Typo: FORM instead of FROM\n</code></pre> <pre><code>Error: Expected 'FROM' but got 'FORM' at position 9\n</code></pre>"},{"location":"getting-started/core-concepts/#file-not-found","title":"File Not Found","text":"<pre><code>query(\"missing.csv\")\n</code></pre> <pre><code>Error: File not found - missing.csv\n</code></pre>"},{"location":"getting-started/core-concepts/#type-errors","title":"Type Errors","text":"<pre><code>SELECT * FROM data WHERE age &gt; 'thirty'  # Comparing number to string\n</code></pre> <pre><code>Error: Cannot compare age (int) with 'thirty' (str)\n</code></pre>"},{"location":"getting-started/core-concepts/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/core-concepts/#1-use-column-names","title":"1. Use Column Names","text":"<p>\u2705 Good: <pre><code>SELECT name, age FROM employees\n</code></pre></p> <p>\u274c Bad: <pre><code>SELECT * FROM employees\n</code></pre></p>"},{"location":"getting-started/core-concepts/#2-add-where-clauses","title":"2. Add WHERE Clauses","text":"<p>\u2705 Good: <pre><code>SELECT * FROM logs WHERE date = '2024-01-01'\n</code></pre></p> <p>\u274c Bad: <pre><code>SELECT * FROM logs  # Processes all rows\n</code></pre></p>"},{"location":"getting-started/core-concepts/#3-choose-the-right-backend","title":"3. Choose the Right Backend","text":"<pre><code># Small files (&lt;100K rows)\nquery(\"data.csv\").sql(\"SELECT * FROM data\", backend=\"python\")\n\n# Large files (&gt;100K rows)\nquery(\"data.csv\").sql(\"SELECT * FROM data\", backend=\"pandas\")\n</code></pre>"},{"location":"getting-started/core-concepts/#4-limit-results-early","title":"4. Limit Results Early","text":"<p>\u2705 Good: <pre><code>SELECT * FROM data WHERE active = true LIMIT 100\n</code></pre></p> <p>\u274c Bad: <pre><code>results = query(\"data.csv\").sql(\"SELECT * FROM data\").to_list()[:100]\n</code></pre></p>"},{"location":"getting-started/core-concepts/#5-use-lazy-evaluation","title":"5. Use Lazy Evaluation","text":"<p>\u2705 Good: <pre><code>for row in query(\"large.csv\").sql(\"SELECT * FROM large\"):\n    process(row)  # One row at a time\n</code></pre></p> <p>\u274c Bad: <pre><code>rows = query(\"large.csv\").sql(\"SELECT * FROM large\").to_list()\nfor row in rows:  # All rows in memory!\n    process(row)\n</code></pre></p>"},{"location":"getting-started/core-concepts/#next-steps","title":"Next Steps","text":"<ul> <li>SQL Support - Learn supported SQL syntax</li> <li>Pandas Backend - Deep dive into performance</li> <li>Architecture - Understand the internals</li> <li>Optimizations - How optimizations work</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>SQLStream offers multiple installation options depending on your needs.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>OS: Linux, macOS, Windows</li> </ul>"},{"location":"getting-started/installation/#installation-options","title":"Installation Options","text":""},{"location":"getting-started/installation/#basic-installation-csv-only","title":"Basic Installation (CSV only)","text":"<p>For querying CSV files only:</p> <pre><code>pip install sqlstream\n</code></pre> <p>This gives you:</p> <ul> <li>\u2705 Core SQL engine</li> <li>\u2705 CSV file support</li> <li>\u2705 Basic CLI commands</li> <li>\u274c No Parquet support</li> <li>\u274c No performance optimizations</li> </ul>"},{"location":"getting-started/installation/#with-parquet-support","title":"With Parquet Support","text":"<p>To query both CSV and Parquet files:</p> <pre><code>pip install \"sqlstream[parquet]\"\n</code></pre> <p>Additional features:</p> <ul> <li>\u2705 Everything from basic install</li> <li>\u2705 Parquet file support via PyArrow</li> <li>\u2705 Better compression and performance</li> </ul>"},{"location":"getting-started/installation/#with-pandas-backend-recommended","title":"With Pandas Backend (Recommended)","text":"<p>For 10-100x performance boost with large files:</p> <pre><code>pip install \"sqlstream[pandas]\"\n</code></pre> <p>Additional features:</p> <ul> <li>\u2705 Everything from basic install</li> <li>\u2705 Parquet support</li> <li>\u2705 Pandas-powered execution (10-100x faster)</li> <li>\u2705 Optimized for large datasets (&gt;100K rows)</li> </ul>"},{"location":"getting-started/installation/#with-http-support","title":"With HTTP Support","text":"<p>To query CSV/Parquet files from URLs:</p> <pre><code>pip install \"sqlstream[http]\"\n</code></pre> <p>Additional features:</p> <ul> <li>\u2705 Query files from HTTP/HTTPS URLs</li> <li>\u2705 Automatic format detection</li> <li>\u2705 Streaming support for large remote files</li> </ul>"},{"location":"getting-started/installation/#with-cli-features","title":"With CLI Features","text":"<p>For beautiful terminal output and interactive mode:</p> <pre><code>pip install \"sqlstream[cli]\"\n</code></pre> <p>Additional features:</p> <ul> <li>\u2705 Rich table formatting</li> <li>\u2705 Interactive scrollable table viewer</li> <li>\u2705 Syntax highlighting</li> <li>\u2705 Multiple output formats (JSON, CSV, table)</li> </ul>"},{"location":"getting-started/installation/#all-features","title":"All Features","text":"<p>To install everything:</p> <pre><code>pip install \"sqlstream[all]\"\n</code></pre> <p>This includes:</p> <ul> <li>\u2705 CSV and Parquet support</li> <li>\u2705 Pandas backend</li> <li>\u2705 HTTP data sources</li> <li>\u2705 Full CLI with interactive mode</li> <li>\u2705 All output formats</li> </ul>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing or development:</p> <pre><code># Clone the repository\ngit clone https://github.com/subhayu99/sqlstream.git\ncd sqlstream\n\n# Install in development mode with all dev dependencies\npip install -e \".[dev]\"\n</code></pre> <p>This includes:</p> <ul> <li>Testing: pytest, pytest-cov</li> <li>Linting: ruff</li> <li>Type checking: mypy</li> <li>Documentation: mkdocs, mkdocs-material</li> </ul>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify SQLStream is working:</p> <pre><code># Check version\nsqlstream --version\n\n# Run a quick query (requires a CSV file)\necho \"name,age\\nAlice,30\\nBob,25\" &gt; test.csv\nsqlstream query test.csv \"SELECT * FROM test\"\n</code></pre> <p>Expected output:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name  \u2502 age \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Alice \u2502  30 \u2502\n\u2502 Bob   \u2502  25 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n2 rows\n</code></pre>"},{"location":"getting-started/installation/#upgrading","title":"Upgrading","text":"<p>To upgrade to the latest version:</p> <pre><code>pip install --upgrade sqlstream\n</code></pre> <p>To upgrade with all features:</p> <pre><code>pip install --upgrade \"sqlstream[all]\"\n</code></pre>"},{"location":"getting-started/installation/#uninstalling","title":"Uninstalling","text":"<p>To remove SQLStream:</p> <pre><code>pip uninstall sqlstream\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If you see <code>ModuleNotFoundError</code>:</p> <pre><code># Reinstall with verbose output\npip install --verbose \"sqlstream[all]\"\n</code></pre>"},{"location":"getting-started/installation/#pandas-not-found","title":"Pandas Not Found","text":"<p>If you get \"pandas backend requested but pandas is not installed\":</p> <pre><code>pip install \"sqlstream[pandas]\"\n</code></pre>"},{"location":"getting-started/installation/#cli-not-working","title":"CLI Not Working","text":"<p>If <code>sqlstream</code> command is not found:</p> <pre><code># Check if it's in your PATH\nwhich sqlstream\n\n# Try running as module\npython -m sqlstream.cli.main --help\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started in 5 minutes</li> <li>Core Concepts - Understand the basics</li> <li>SQL Support - Learn supported SQL syntax</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with SQLStream in 5 minutes!</p>"},{"location":"getting-started/quickstart/#step-1-install-sqlstream","title":"Step 1: Install SQLStream","text":"<pre><code>pip install \"sqlstream[all]\"\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-create-sample-data","title":"Step 2: Create Sample Data","text":"<p>Create a sample CSV file:</p> <pre><code>cat &gt; employees.csv &lt;&lt; EOF\nid,name,department,salary,hire_date\n1,Alice,Engineering,95000,2020-01-15\n2,Bob,Sales,75000,2019-06-01\n3,Charlie,Engineering,105000,2018-03-20\n4,Diana,Marketing,68000,2021-02-14\n5,Eve,Sales,82000,2020-09-10\nEOF\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-your-first-query","title":"Step 3: Your First Query","text":""},{"location":"getting-started/quickstart/#cli-usage","title":"CLI Usage","text":"<pre><code># Select all rows\n$ sqlstream query employees.csv \"SELECT * FROM employees\"\n\n# Filter by department\n$ sqlstream query employees.csv \"SELECT name, salary FROM employees WHERE department = 'Engineering'\"\n\n# Sort by salary\n$ sqlstream query employees.csv \"SELECT * FROM employees ORDER BY salary DESC LIMIT 3\"\n</code></pre>"},{"location":"getting-started/quickstart/#python-api","title":"Python API","text":"<pre><code>from sqlstream import query\n\n# Simple query\nresults = query(\"employees.csv\").sql(\"SELECT * FROM employees WHERE salary &gt; 80000\")\n\n# Print results\nfor row in results:\n    print(f\"{row['name']}: ${row['salary']:,}\")\n</code></pre> <p>Output: <pre><code>Alice: $95,000\nCharlie: $105,000\nEve: $82,000\n</code></pre></p>"},{"location":"getting-started/quickstart/#step-4-advanced-features","title":"Step 4: Advanced Features","text":""},{"location":"getting-started/quickstart/#aggregations","title":"Aggregations","text":"<pre><code># Count employees by department\n$ sqlstream query employees.csv \"SELECT department, COUNT(*) AS count FROM employees GROUP BY department\"\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 department  \u2502 count \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Engineering \u2502     2 \u2502\n\u2502 Sales       \u2502     2 \u2502\n\u2502 Marketing   \u2502     1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"getting-started/quickstart/#joins","title":"Joins","text":"<p>Create another file for orders:</p> <pre><code>cat &gt; orders.csv &lt;&lt; EOF\norder_id,employee_id,amount\n101,1,1500\n102,2,2300\n103,1,1800\n104,3,2100\nEOF\n</code></pre> <p>Join the two files:</p> <pre><code>$ sqlstream query \"SELECT e.name, o.amount FROM 'employees.csv' e JOIN 'orders.csv' o ON e.id = o.employee_id\"\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name    \u2502 amount \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Alice   \u2502   1500 \u2502\n\u2502 Bob     \u2502   2300 \u2502\n\u2502 Alice   \u2502   1800 \u2502\n\u2502 Charlie \u2502   2100 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"getting-started/quickstart/#output-formats","title":"Output Formats","text":"Table (default)JSONCSV <pre><code>$ sqlstream query employees.csv \"SELECT * FROM employees LIMIT 2\"\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 name  \u2502 department  \u2502 salary \u2502 hire_date  \u2502\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  1 \u2502 Alice \u2502 Engineering \u2502  95000 \u2502 2020-01-15 \u2502\n\u2502  2 \u2502 Bob   \u2502 Sales       \u2502  75000 \u2502 2019-06-01 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>$ sqlstream query employees.csv \"SELECT * FROM employees LIMIT 2\" --format json\n</code></pre> <pre><code>[\n  {\n    \"id\": 1,\n    \"name\": \"Alice\",\n    \"department\": \"Engineering\",\n    \"salary\": 95000,\n    \"hire_date\": \"2020-01-15\"\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Bob\",\n    \"department\": \"Sales\",\n    \"salary\": 75000,\n    \"hire_date\": \"2019-06-01\"\n  }\n]\n</code></pre> <pre><code>$ sqlstream query employees.csv \"SELECT * FROM employees LIMIT 2\" --format csv\n</code></pre> <pre><code>id,name,department,salary,hire_date\n1,Alice,Engineering,95000,2020-01-15\n2,Bob,Sales,75000,2019-06-01\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-performance-boost","title":"Step 5: Performance Boost","text":"<p>For large files, use the pandas backend:</p> <pre><code>$ sqlstream query large_file.csv \"SELECT * FROM large_file WHERE amount &gt; 1000\" --backend pandas\n</code></pre> <p>Performance comparison:</p> Rows Python Backend Pandas Backend Speedup 10K 0.5s 0.05s 10x 100K 5.2s 0.15s 35x 1M 52s 0.8s 65x"},{"location":"getting-started/quickstart/#step-6-interactive-shell","title":"Step 6: Interactive Shell","text":"<p>For a full interactive experience, use the shell command:</p> <pre><code>$ sqlstream shell employees.csv\n</code></pre> <p>This launches a powerful TUI (Terminal User Interface) with:</p> <ul> <li>Query Editor: Multi-line editing with syntax highlighting.</li> <li>Results Viewer: Scrollable table with pagination.</li> <li>Schema Browser: Press <code>F2</code> to see available tables and columns.</li> <li>Export: Press <code>Ctrl+X</code> to export results to CSV/JSON/Parquet.</li> <li>History: Use <code>Ctrl+Up</code>/<code>Down</code> to navigate previous queries.</li> </ul>"},{"location":"getting-started/quickstart/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/quickstart/#data-exploration","title":"Data Exploration","text":"<pre><code># Check file structure\n$ head -5 data.csv\n\n# Count rows\n$ sqlstream query data.csv \"SELECT COUNT(*) FROM data\"\n\n# Show unique values\n$ sqlstream query data.csv \"SELECT DISTINCT category FROM data\"\n\n# Summary statistics\n$ sqlstream query data.csv \"SELECT MIN(price), MAX(price), AVG(price) FROM data\"\n</code></pre>"},{"location":"getting-started/quickstart/#data-cleaning","title":"Data Cleaning","text":"<pre><code>from sqlstream import query\n\n# Remove duplicates and filter nulls\nresults = query(\"messy_data.csv\").sql(\"\"\"\n    SELECT DISTINCT *\n    FROM messy_data\n    WHERE name IS NOT NULL\n      AND age &gt; 0\n    ORDER BY id\n\"\"\")\n\n# Export cleaned data\nimport csv\nwith open(\"clean_data.csv\", \"w\") as f:\n    writer = csv.DictWriter(f, fieldnames=results.to_list()[0].keys())\n    writer.writeheader()\n    writer.writerows(results.to_list())\n</code></pre>"},{"location":"getting-started/quickstart/#etl-pipeline","title":"ETL Pipeline","text":"<pre><code>from sqlstream import query\n\n# Extract\ncustomers = query(\"customers.csv\")\norders = query(\"orders.csv\")\n\n# Transform: Calculate total orders per customer\nresult = query(\"customers.csv\").sql(\"\"\"\n    SELECT c.name, COUNT(o.order_id) as total_orders\n    FROM customers c\n    JOIN orders o ON c.id = o.customer_id\n    GROUP BY c.name\n    ORDER BY total_orders DESC\n\"\"\")\n\n# Load\nfor row in result:\n    # Send to database, API, etc.\n    print(row)\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<p>Now that you're familiar with the basics, explore:</p> <ul> <li>Core Concepts - Understand how SQLStream works</li> <li>SQL Support - Learn all supported SQL features</li> <li>CLI Reference - Master the command-line interface</li> <li>Python API - Deep dive into the programmatic API</li> <li>Examples - More real-world examples</li> </ul>"},{"location":"getting-started/quickstart/#need-help","title":"Need Help?","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udc1b Report Issues</li> <li>\ud83d\udcac Discussions</li> </ul>"}]}