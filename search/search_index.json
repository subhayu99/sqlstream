{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SQLStream","text":"<p>A lightweight, pure-Python SQL query engine for CSV and Parquet files with lazy evaluation and intelligent optimizations.</p>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code># Query a CSV file\n$ sqlstream query \"SELECT * FROM 'data.csv' WHERE age &gt; 25\"\n\n# Join multiple files\n$ sqlstream query \"SELECT c.name, o.total FROM 'customers.csv' c JOIN 'orders.csv' o ON c.id = o.customer_id\"\n\n# Interactive shell with full TUI\n$ sqlstream shell\n\n# Query S3 files\n$ sqlstream query \"SELECT * FROM 's3://my-bucket/data.parquet' WHERE date &gt; '2024-01-01'\"\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> <p> Pure Python</p> <p>No database installation required. Works anywhere Python runs.</p> </li> <li> <p> Multiple Formats</p> <p>Support for CSV, Parquet files, HTTP URLs, and S3 buckets.</p> </li> <li> <p> 10-100x Faster</p> <p>Optional pandas backend for massive performance boost.</p> </li> <li> <p> JOIN Support</p> <p>INNER, LEFT, RIGHT joins across multiple files.</p> </li> <li> <p> Aggregations</p> <p>GROUP BY with COUNT, SUM, AVG, MIN, MAX functions.</p> </li> <li> <p> Beautiful Output</p> <p>Rich tables, JSON, CSV with syntax highlighting.</p> </li> <li> <p> Interactive Shell</p> <p>Full-featured TUI with multiple tabs, state persistence, file browser, and query plan visualization.</p> </li> <li> <p> Inline File Paths</p> <p>Specify files directly in SQL queries (Phase 7.6).</p> </li> <li> <p> Smart Optimizations</p> <p>Column pruning, predicate pushdown, lazy evaluation.</p> </li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#using-uv-recommended","title":"Using <code>uv</code> (recommended)","text":"Basic (CSV only)With Parquet and Pandas supportAll FeaturesMultiple Sub Dependencies <pre><code>uv tool install sqlstream\n</code></pre> <pre><code>uv tool install \"sqlstream[parquet,pandas]\"\n</code></pre> <pre><code>uv tool install \"sqlstream[all]\"\n</code></pre> <pre><code>uv tool install \"sqlstream[interactive,pandas,s3,http,html,duckdb]\"\n</code></pre>"},{"location":"#using-pip","title":"Using <code>pip</code>","text":"Basic (CSV only)All Features <pre><code>pip install sqlstream\n</code></pre> <pre><code>pip install \"sqlstream[all]\"\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#cli-usage","title":"CLI Usage","text":"<pre><code># Simple query\n$ sqlstream query data.csv \"SELECT name, age FROM data WHERE age &gt; 25\"\n\n# With output format\n$ sqlstream query data.csv \"SELECT * FROM data\" --format json\n\n# Show execution time\n$ sqlstream query data.csv \"SELECT * FROM data\" --time\n\n# Use pandas backend for performance\n$ sqlstream query data.parquet \"SELECT * FROM data\" --backend pandas\n</code></pre>"},{"location":"#python-api","title":"Python API","text":"<pre><code>from sqlstream import query\n\n# Execute query\nresults = query(\"data.csv\").sql(\"SELECT * FROM data WHERE age &gt; 25\")\n\n# Iterate over results (lazy evaluation)\nfor row in results:\n    print(row)\n\n# Or convert to list\nresults_list = query(\"data.csv\").sql(\"SELECT * FROM data\").to_list()\n</code></pre>"},{"location":"#why-sqlstream","title":"Why SQLStream?","text":"<p>Perfect For</p> <ul> <li>Data Exploration: Quick analysis without database setup</li> <li>ETL Pipelines: Process CSV/Parquet files with SQL</li> <li>Data Science: Filter and join datasets before pandas</li> <li>DevOps: Query logs and data files in CI/CD</li> <li>Learning: Understand query execution internals</li> </ul> <p>Not For</p> <ul> <li>Large Databases: Use PostgreSQL, MySQL instead</li> <li>Real-time Analytics: Use ClickHouse, DuckDB</li> <li>Production OLTP: SQLStream is read-only</li> </ul>"},{"location":"#performance","title":"Performance","text":"<p>SQLStream offers three execution backends:</p> Backend Speed Use Case Python Baseline Learning, small files (&lt;100K rows) Pandas 10-100x faster Basic queries, large files (&gt;100K rows) DuckDB 100x+ faster Complex SQL, analytics, huge files <p>Performance Tips</p> <ul> <li>Use <code>--backend duckdb</code> for complex SQL (CTEs, window functions)</li> <li>Use <code>--backend pandas</code> for simple queries on large files</li> <li>Use column pruning: <code>SELECT name, age</code> instead of <code>SELECT *</code></li> <li>Add WHERE filters to reduce data scanned</li> <li>Use Parquet format for better compression</li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li> <p> Quick Start Guide</p> <p>Get up and running in 5 minutes with hands-on examples.</p> </li> <li> <p> SQL Reference</p> <p>Learn about supported SQL syntax and features.</p> </li> <li> <p> CLI Reference</p> <p>Complete guide to the command-line interface.</p> </li> <li> <p> Python API</p> <p>Deep dive into the programmatic API.</p> </li> </ul>"},{"location":"#project-status","title":"Project Status","text":"<p>SQLStream is in active development. Current phase: 8</p> <ul> <li>\u2705 Phase 0-2: Core query engine with Volcano model</li> <li>\u2705 Phase 3: Parquet support</li> <li>\u2705 Phase 4: Aggregations &amp; GROUP BY</li> <li>\u2705 Phase 5: JOIN operations (INNER, LEFT, RIGHT)</li> <li>\u2705 Phase 5.5: Pandas backend (10-100x speedup)</li> <li>\u2705 Phase 6: HTTP data sources</li> <li>\u2705 Phase 7: CLI with beautiful output</li> <li>\u2705 Phase 7.5: Interactive shell with Textual</li> <li>\u2705 Phase 7.6: Inline file path support</li> <li>\u2705 Phase 7.7: S3 Support for CSV and Parquet</li> <li>\u2705 Phase 8: Type system &amp; schema inference</li> <li>\u2705 Phase 9: Enhanced interactive shell (multiple tabs, state persistence, file browser, query plan)</li> <li>\ud83d\udea7 Phase 10: Error handling &amp; user feedback</li> <li>\ud83d\udea7 Phase 11: Testing &amp; documentation</li> </ul>"},{"location":"#license","title":"License","text":"<p>SQLStream is licensed under the MIT License.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! See the Contributing Guide for details.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions to SQLStream are welcome!</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code>git clone https://github.com/subhayu99/sqlstream.git\ncd sqlstream\npip install -e \".[dev]\"\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code>pytest tests/\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<pre><code>ruff check .\nruff format .\n</code></pre>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Add tests</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#issues","title":"Issues","text":"<p>Report bugs and request features at GitHub Issues.</p>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":""},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#how-is-sqlstream-different-from-duckdb","title":"How is SQLStream different from DuckDB?","text":"<p>DuckDB is a fully-featured embedded database with a complete SQL implementation, optimized for OLAP workloads.</p> <p>SQLStream is a lightweight Python library focused on: - Simplicity: Pure Python, minimal dependencies - Learning: Understand query execution internals - Portability: Works anywhere Python runs - File-first: Query files without database setup</p> <p>Use DuckDB if: You need maximum performance, full SQL compatibility, or production OLAP workloads.</p> <p>Use SQLStream if: You want a lightweight tool for data exploration, learning query engines, or quick CSV/Parquet analysis.</p>"},{"location":"faq/#how-is-sqlstream-different-from-pandas","title":"How is SQLStream different from pandas?","text":"<p>pandas is a data manipulation library with DataFrame API.</p> <p>SQLStream provides SQL interface for: - Familiar syntax: Use SQL instead of DataFrame methods - Lazy evaluation: Process large files efficiently - Join multiple files: Combine datasets without loading all into memory - SQL users: Leverage existing SQL knowledge</p> <p>Interoperability: You can use pandas backend for performance (<code>--backend pandas</code>) or convert SQLStream results to pandas DataFrames.</p>"},{"location":"faq/#can-i-use-this-in-production","title":"Can I use this in production?","text":"<p>Current status: SQLStream is in active development and suitable for: - \u2705 Data exploration and analysis - \u2705 ETL scripts and data pipelines - \u2705 CI/CD data processing - \u2705 Learning and prototyping</p> <p>Considerations: - \ud83d\udea7 API may change between versions - \u2705 Full SQL support with DuckDB backend (CTEs, window functions, subqueries) - \ud83d\udea7 Limited SQL with Python/Pandas backends - \ud83d\udea7 No transaction support (read-only) - \u2705 Good test coverage (377 tests, 53%)</p> <p>Recommendation: Use DuckDB backend for production workloads requiring complex SQL. Python/Pandas backends suitable for simple queries and learning.</p>"},{"location":"faq/#whats-the-maximum-file-size","title":"What's the maximum file size?","text":"<p>Python Backend: - RAM limited: Can handle files up to available memory - Typical: 10-100 MB comfortably - Large files: Use <code>LIMIT</code> or pandas backend</p> <p>Pandas Backend: - Much larger: 100 MB - 10 GB+ depending on RAM - Chunk processing: Automatically handles larger-than-memory with streaming</p> <p>S3 files: - Streaming: Data is streamed, not loaded entirely into memory - Practical limit: 10 GB+ for Parquet, smaller for CSV</p> <p>Best practices: <pre><code># Preview large files\nsqlstream query large.csv \"SELECT * FROM large LIMIT 1000\" --backend pandas\n\n# Use Parquet for better compression\nsqlstream query large.parquet \"SELECT * FROM large WHERE date &gt; '2024-01-01'\" --backend pandas\n</code></pre></p>"},{"location":"faq/#does-it-work-on-windows","title":"Does it work on Windows?","text":"<p>Yes! SQLStream is pure Python and works on: - \u2705 Windows (Windows 10/11, Server) - \u2705 macOS (Intel and Apple Silicon) - \u2705 Linux (Ubuntu, Debian, CentOS, etc.)</p> <p>Platform-specific notes: - Windows: Use PowerShell or CMD - Interactive shell: Works best in Windows Terminal (not CMD) - File paths: Use forward slashes <code>/</code> or escape backslashes <code>\\\\</code></p>"},{"location":"faq/#can-i-use-it-with-jupyter-notebooks","title":"Can I use it with Jupyter notebooks?","text":"<p>Yes! SQLStream works great in Jupyter:</p> <pre><code># Install in notebook\n!pip install \"sqlstream[all]\"\n\n# Use in cells\nfrom sqlstream import query\n\nresults = query(\"data.csv\").sql(\"\"\"\n    SELECT department, AVG(salary) as avg_salary\n    FROM data\n    GROUP BY department\n\"\"\")\n\n# Display as DataFrame\nimport pandas as pd\ndf = pd.DataFrame(results.to_list())\ndf\n</code></pre> <p>Tips: - Use <code>to_list()</code> for small results - Use iteration for large results to avoid memory issues - Consider DuckDB for faster notebook performance</p> <p>See Jupyter Integration Guide (coming soon) for more details.</p>"},{"location":"faq/#how-do-i-report-bugs","title":"How do I report bugs?","text":"<ol> <li>Check existing issues: GitHub Issues</li> <li>Create new issue: Include:</li> <li>SQLStream version (<code>pip show sqlstream</code>)</li> <li>Python version</li> <li>Operating system</li> <li>Minimal reproduction code</li> <li>Error message with full traceback</li> <li>Expected vs actual behavior</li> </ol> <p>Example: <pre><code>## Bug Report\n\n**Environment:**\n- SQLStream: 0.2.5\n- Python: 3.11.0\n- OS: Ubuntu 22.04\n\n**Code:**\n\\```python\nfrom sqlstream import query\nresults = query(\"data.csv\").sql(\"SELECT * FROM data\")\n\\```\n\n**Error:**\n\\```\nTypeError: ...\n\\```\n\n**Expected:** Should return results\n**Actual:** Raises TypeError\n</code></pre></p>"},{"location":"faq/#how-do-i-request-features","title":"How do I request features?","text":"<ol> <li>Check roadmap: See Project Status</li> <li>Search discussions: GitHub Discussions</li> <li>Create discussion: Use \"Ideas\" category</li> <li>Include:</li> <li>Use case description</li> <li>Example of desired behavior</li> <li>Why existing features don't work</li> <li>Willingness to contribute</li> </ol> <p>What features are planned? - See Development Status for roadmap - Phase 10: Error handling &amp; user feedback - Phase 11: Testing &amp; documentation - Future: Enhanced Python/Pandas backend SQL support (window functions, CTEs currently available in DuckDB backend)</p>"},{"location":"faq/#is-there-commercial-support","title":"Is there commercial support?","text":"<p>Currently: No commercial support or SLA.</p> <p>Community support: - GitHub Discussions - GitHub Issues - Documentation: https://subhayu99.github.io/sqlstream</p> <p>For enterprises: Consider DuckDB, which has commercial support options.</p>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"faq/#what-dependencies-does-sqlstream-have","title":"What dependencies does SQLStream have?","text":"<p>Minimal (CSV only): <pre><code>pip install sqlstream\n# Only requires: Python standard library\n</code></pre></p> <p>Recommended (all features): <pre><code>pip install \"sqlstream[all]\"\n# Includes: pandas, pyarrow, s3fs, textual, httpx\n</code></pre></p> <p>By feature: - <code>sqlstream[parquet]</code> - Parquet support (pyarrow) - <code>sqlstream[pandas]</code> - Pandas backend (pandas) - <code>sqlstream[duckdb]</code> - DuckDB backend (duckdb) - <code>sqlstream[s3]</code> - S3 support (s3fs) - <code>sqlstream[cli]</code> - Basic CLI shell (click) - <code>sqlstream[interactive]</code> - Full blown TUI (textual) - <code>sqlstream[http]</code> - HTTP data sources (httpx) - <code>sqlstream[html]</code> - HTML parsing (lxml)</p>"},{"location":"faq/#how-do-i-upgrade-sqlstream","title":"How do I upgrade SQLStream?","text":"<pre><code># Upgrade to latest version\npip install --upgrade sqlstream\n\n# Upgrade with all features\npip install --upgrade \"sqlstream[all]\"\n\n# Check current version\npip show sqlstream\n# or\nsqlstream --version\n</code></pre> <p>Breaking changes? Check Changelog (coming soon).</p>"},{"location":"faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/#can-i-query-multiple-files-at-once","title":"Can I query multiple files at once?","text":"<p>Yes! Use JOINs:</p> <pre><code>SELECT e.name, d.department_name, e.salary\nFROM 'employees.csv' e\nJOIN 'departments.csv' d ON e.dept_id = d.id\nWHERE e.salary &gt; 80000\n</code></pre> <p>Limitations: - All files must be accessible (local or remote) - JOIN performance depends on file sizes - Use pandas backend for better performance</p>"},{"location":"faq/#can-i-use-aggregate-functions","title":"Can I use aggregate functions?","text":"<p>Yes! Supported aggregations:</p> <pre><code>SELECT\n    department,\n    COUNT(*) as count,\n    AVG(salary) as avg_salary,\n    MIN(salary) as min_salary,\n    MAX(salary) as max_salary,\n    SUM(salary) as total_salary\nFROM 'employees.csv'\nGROUP BY department\nORDER BY avg_salary DESC\n</code></pre> <p>Supported: <code>COUNT</code>, <code>SUM</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code> Not yet: <code>COUNT(DISTINCT)</code>, <code>STDDEV</code>, <code>VARIANCE</code>, custom aggregates</p>"},{"location":"faq/#can-i-use-subqueries-or-ctes","title":"Can I use subqueries or CTEs?","text":"<p>Yes, with the DuckDB backend!</p> <p>Supported (DuckDB backend): - \u2705 Subqueries in FROM clause - \u2705 Common Table Expressions (WITH clause) - \u2705 Correlated subqueries - \u2705 Subqueries in WHERE clause</p> <p>Example: <pre><code>from sqlstream import query\n\n# Use CTEs with DuckDB backend\nresults = query().sql(\"\"\"\n    WITH high_earners AS (\n        SELECT * FROM 'employees.csv'\n        WHERE salary &gt; 100000\n    )\n    SELECT \n        department,\n        AVG(salary) as avg_salary\n    FROM high_earners\n    GROUP BY department\n\"\"\", backend=\"duckdb\")\n</code></pre></p> <p>Not supported (Python/Pandas backends): - \u274c Python backend: Limited SQL support - \u274c Pandas backend: Basic queries only</p> <p>Recommendation: Use <code>--backend duckdb</code> for complex SQL features.</p> <p>See DuckDB Backend Guide for full details.</p>"},{"location":"faq/#how-do-i-specify-column-types-manually","title":"How do I specify column types manually?","text":"<p>Currently: Types are inferred automatically.</p> <p>CSV type inference: - Samples first 100 rows - Detects: int, float, string, date, datetime, boolean</p> <p>Parquet: Types come from file metadata</p> <p>Manual types: Not yet supported. Planned for future release.</p> <p>Workaround: Use CAST (if implemented) or process with pandas first.</p>"},{"location":"faq/#can-i-write-data-insert-update-delete","title":"Can I write data (INSERT, UPDATE, DELETE)?","text":"<p>No. SQLStream is read-only.</p> <p>Supported: <code>SELECT</code> queries only</p> <p>Not supported: - \u274c INSERT - \u274c UPDATE - \u274c DELETE - \u274c CREATE TABLE - \u274c ALTER TABLE</p> <p>For data modification: Use pandas, DuckDB, or traditional databases.</p>"},{"location":"faq/#performance-optimization","title":"Performance &amp; Optimization","text":""},{"location":"faq/#why-is-my-query-slow","title":"Why is my query slow?","text":"<p>Common reasons:</p> <ol> <li>Using Python backend on large files</li> <li> <p>Solution: Use <code>--backend pandas</code></p> </li> <li> <p>Not using WHERE filters</p> </li> <li> <p>Solution: Add filters to reduce data scanned</p> </li> <li> <p>Using SELECT *</p> </li> <li> <p>Solution: Select only needed columns</p> </li> <li> <p>Large JOINs</p> </li> <li> <p>Solution: Filter before joining</p> </li> <li> <p>CSV vs Parquet</p> </li> <li>Solution: Convert to Parquet for better performance</li> </ol> <p>Example optimization: <pre><code># Slow (Python backend, all columns, all rows)\nsqlstream query large.csv \"SELECT * FROM large\"\n\n# Fast (pandas, filtered, specific columns)\nsqlstream query large.parquet \\\n  \"SELECT name, salary FROM large WHERE date &gt; '2024-01-01'\" \\\n  --backend pandas\n</code></pre></p>"},{"location":"faq/#what-backends-are-supported","title":"What backends are supported?","text":"<p>SQLStream supports three execution backends:</p> <ol> <li>Python (Default/Educational): Pure Python implementation of the Volcano model. Great for learning how databases work, but slower for large data.</li> <li>Pandas: Translates SQL to pandas operations. 10-100x faster than Python backend. Best for general use.</li> <li>DuckDB (New!): Uses DuckDB's engine for full SQL support (window functions, CTEs, etc.) and maximum performance. 10-1000x faster.</li> </ol>"},{"location":"faq/#how-do-i-use-the-duckdb-backend","title":"How do I use the DuckDB backend?","text":"<p>Install with <code>pip install duckdb</code> or <code>pip install \"sqlstream[duckdb]\"</code>.</p> <p>Then use it in your code: <pre><code>query(\"data.csv\").sql(\"...\", backend=\"duckdb\")\n</code></pre></p> <p>Or via CLI: <pre><code>sqlstream query \"...\" --backend duckdb\n</code></pre></p>"},{"location":"faq/#does-duckdb-backend-support-all-file-formats","title":"Does DuckDB backend support all file formats?","text":"<p>Yes! The DuckDB backend uses SQLStream's unified reader architecture, so it supports: - CSV, Parquet, JSON - HTML tables, Markdown tables - S3 files (s3://) - HTTP/HTTPS URLs</p> <p>It automatically handles authentication and caching just like the other backends.</p> <p>See Performance Guide (coming soon) for details.</p>"},{"location":"faq/#should-i-use-csv-or-parquet","title":"Should I use CSV or Parquet?","text":"<p>Parquet is almost always better:</p> Feature CSV Parquet Read Speed Baseline 10-100x faster File Size Large 2-10x smaller Type Safety Inferred Stored in file Column Access Read all Columnar (faster) Compression None/gzip Snappy/Gzip/LZ4 <p>Use CSV when: - Need human-readable format - Editing files manually - Tool doesn't support Parquet</p> <p>Use Parquet when: - Performance matters - Large files - Production pipelines</p> <p>Convert CSV to Parquet: <pre><code>import pandas as pd\ndf = pd.read_csv('data.csv')\ndf.to_parquet('data.parquet')\n</code></pre></p>"},{"location":"faq/#error-messages","title":"Error Messages","text":""},{"location":"faq/#no-module-named-textual","title":"\"No module named 'textual'\"","text":"<p>Problem: Interactive shell not installed</p> <p>Solution: <pre><code>pip install \"sqlstream[cli]\"\n# or\npip install \"sqlstream[all]\"\n</code></pre></p>"},{"location":"faq/#no-module-named-pandas","title":"\"No module named 'pandas'\"","text":"<p>Problem: Pandas backend not installed</p> <p>Solution: <pre><code>pip install \"sqlstream[pandas]\"\n# or\npip install \"sqlstream[all]\"\n</code></pre></p>"},{"location":"faq/#file-not-found-error","title":"\"File not found\" error","text":"<p>Possible causes:</p> <ol> <li> <p>Wrong path:    <pre><code># Wrong\nsqlstream query data.csv \"SELECT * FROM data\"\n\n# Correct (use file path in query)\nsqlstream query \"SELECT * FROM 'data.csv'\"\n</code></pre></p> </li> <li> <p>Relative vs absolute path:    <pre><code># Use absolute path if unsure\nsqlstream query \"SELECT * FROM '/home/user/data.csv'\"\n</code></pre></p> </li> <li> <p>Path with spaces:    <pre><code>-- Use quotes\nSELECT * FROM 'my data.csv'\n</code></pre></p> </li> </ol>"},{"location":"faq/#s3-authentication-errors","title":"S3 authentication errors","text":"<p>Problem: Can't access S3 files</p> <p>Solutions:</p> <ol> <li> <p>Set AWS credentials:    <pre><code>export AWS_ACCESS_KEY_ID=your_key\nexport AWS_SECRET_ACCESS_KEY=your_secret\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre></p> </li> <li> <p>Use AWS config:    <pre><code>aws configure\n</code></pre></p> </li> <li> <p>Check bucket permissions: Ensure you have read access</p> </li> <li> <p>Check region: Some buckets require specific region</p> </li> </ol> <p>See S3 Support for details.</p>"},{"location":"faq/#contributing","title":"Contributing","text":""},{"location":"faq/#how-can-i-contribute","title":"How can I contribute?","text":"<p>Ways to contribute:</p> <ol> <li>Report bugs: See \"How do I report bugs?\" above</li> <li>Request features: See \"How do I request features?\" above</li> <li>Fix bugs: Pick an issue labeled \"good first issue\"</li> <li>Add features: Discuss first in GitHub Discussions</li> <li>Improve docs: Submit PRs for typos or clarifications</li> <li>Write tests: Increase test coverage</li> <li>Write examples: Add real-world examples</li> </ol> <p>Get started: <pre><code>git clone https://github.com/subhayu99/sqlstream.git\ncd sqlstream\npip install -e \".[dev]\"\npytest\n</code></pre></p> <p>See Contributing Guide for details.</p>"},{"location":"faq/#where-should-i-start-as-a-new-contributor","title":"Where should I start as a new contributor?","text":"<p>Good first issues: - Look for \"good first issue\" label - Documentation improvements - Test coverage improvements - Bug fixes with reproduction steps</p> <p>Learning path: 1. Read Architecture Guide 2. Read Volcano Model 3. Browse code in <code>sqlstream/</code> directory 4. Run tests to understand behavior 5. Pick a small issue to work on</p>"},{"location":"faq/#next-steps","title":"Next Steps","text":"<ul> <li>\ud83d\udcd6 Quick Start Guide</li> <li>\ud83d\udd27 Troubleshooting</li> <li>\ud83d\udcda Full Documentation</li> <li>\ud83d\udcac GitHub Discussions</li> </ul>"},{"location":"limitations/","title":"Limitations &amp; Known Issues","text":"<p>This document outlines current limitations and known issues in SQLStream to set proper expectations.</p>"},{"location":"limitations/#sql-features","title":"SQL Features","text":""},{"location":"limitations/#supported-sql-pythonpandas-backends","title":"Supported SQL (Python/Pandas Backends)","text":"<p>The default Python and Pandas backends support a subset of standard SQL-92:</p> <ul> <li><code>SELECT</code> with <code>DISTINCT</code></li> <li><code>FROM</code> (single table or JOINs)</li> <li><code>WHERE</code> (filtering with AND/OR)</li> <li><code>GROUP BY</code></li> <li><code>ORDER BY</code> (ASC/DESC)</li> <li><code>LIMIT</code></li> <li><code>JOIN</code> (INNER, LEFT, RIGHT, CROSS)</li> <li>Aggregates: <code>COUNT</code>, <code>SUM</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code></li> </ul>"},{"location":"limitations/#full-sql-support-duckdb-backend","title":"Full SQL Support (DuckDB Backend) \u2705","text":"<p>To get full SQL support, use the DuckDB backend!</p> <p>By using <code>backend=\"duckdb\"</code>, you unlock: - \u2705 Window Functions (<code>OVER</code>, <code>PARTITION BY</code>, <code>RANK</code>, etc.) - \u2705 Common Table Expressions (CTEs / <code>WITH</code> clause) - \u2705 Subqueries (in <code>FROM</code>, <code>WHERE</code>, <code>SELECT</code>) - \u2705 <code>HAVING</code> clause - \u2705 Set operations (<code>UNION</code>, <code>INTERSECT</code>, <code>EXCEPT</code>) - \u2705 Advanced functions (String, Date, Math)</p>"},{"location":"limitations/#not-supported-pythonpandas-backends","title":"Not Supported (Python/Pandas Backends)","text":"<p>If you are NOT using the DuckDB backend, the following are NOT supported:</p> <ul> <li>Window Functions (<code>OVER</code>, <code>PARTITION BY</code>)</li> <li>CTEs (<code>WITH</code>)</li> <li>Subqueries</li> <li><code>HAVING</code> clause</li> <li><code>UNION</code> / <code>INTERSECT</code> / <code>EXCEPT</code></li> <li><code>CASE</code> statements</li> <li>Complex expressions in <code>GROUP BY</code> or <code>ORDER BY</code>ers AS (     SELECT * FROM employees WHERE salary &gt; 100000 ) SELECT * FROM high_earners WHERE department = 'Engineering' Workaround: Break into multiple queries or use pandas for complex transformations.</li> </ul>"},{"location":"limitations/#window-functions","title":"Window Functions","text":"<pre><code>-- \u274c NOT SUPPORTED\nSELECT \n    name,\n    salary,\n    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rank\nFROM employees\n</code></pre> <p>Supported alternatives: Use pandas with <code>df.groupby().rank()</code> or DuckDB.</p> <p>Planned: Future roadmap item (Phase 12+)</p>"},{"location":"limitations/#advanced-aggregations","title":"Advanced Aggregations","text":"<pre><code>-- \u2705 SUPPORTED: Basic aggregations\nSELECT department, COUNT(*), AVG(salary) FROM employees GROUP BY department\n\n-- \u274c NOT SUPPORTED: DISTINCT in aggregates\nSELECT COUNT(DISTINCT department) FROM employees\n\n-- \u274c NOT SUPPORTED: HAVING\nSELECT department, AVG(salary) as avg_sal \nFROM employees \nGROUP BY department \nHAVING avg_sal &gt; 80000\n\n-- \u274c NOT SUPPORTED: Statistical functions\nSELECT STDDEV(salary), VARIANCE(salary) FROM employees\n</code></pre> <p>Workarounds: Use pandas for statistical functions, use WHERE after GROUP BY (inefficient).</p>"},{"location":"limitations/#unionintersectexcept","title":"UNION/INTERSECT/EXCEPT","text":"<pre><code>-- \u274c NOT SUPPORTED\nSELECT * FROM file1.csv\nUNION\nSELECT * FROM file2.csv\n</code></pre> <p>Workaround: Concatenate files manually or use pandas <code>pd.concat()</code>.</p>"},{"location":"limitations/#datetime-functions","title":"Date/Time Functions","text":"<pre><code>-- \u274c LIMITED SUPPORT: Date operations\nSELECT DATE_ADD(date_col, INTERVAL 1 DAY) FROM data  -- Not supported\nSELECT CURRENT_DATE()  -- Not supported\nSELECT EXTRACT(YEAR FROM date_col) FROM data  -- Not supported\n</code></pre> <p>Current support: Basic date comparisons only <pre><code>-- \u2705 SUPPORTED\nSELECT * FROM data WHERE date_col &gt; '2024-01-01'\nSELECT * FROM data WHERE date_col BETWEEN '2024-01-01' AND '2024-12-31'\n</code></pre></p> <p>Workaround: Preprocess dates with pandas.</p>"},{"location":"limitations/#string-functions","title":"String Functions","text":"<pre><code>-- \u274c LIMITED SUPPORT\nSELECT UPPER(name), LOWER(name), CONCAT(first, last) FROM data  -- Not supported\nSELECT SUBSTRING(name, 1, 5) FROM data  -- Not supported\nSELECT TRIM(name) FROM data  -- Not supported\n</code></pre> <p>Current support: Only basic column selection and filtering.</p> <p>Workaround: Use pandas string methods.</p>"},{"location":"limitations/#case-statements","title":"CASE Statements","text":"<pre><code>-- \u274c NOT SUPPORTED\nSELECT \n    name,\n    CASE \n        WHEN salary &gt; 100000 THEN 'High'\n        WHEN salary &gt; 50000 THEN 'Medium'\n        ELSE 'Low'\n    END as salary_category\nFROM employees\n</code></pre> <p>Workaround: Use pandas <code>apply()</code> or <code>np.select()</code>.</p>"},{"location":"limitations/#self-joins","title":"SELF JOINS","text":"<pre><code>-- \u274c NOT RELIABLY SUPPORTED\nSELECT a.name, b.name\nFROM employees a\nJOIN employees b ON a.manager_id = b.id\n</code></pre> <p>Status: May work in simple cases, not tested thoroughly.</p> <p>Workaround: Use DuckDB or pandas merge.</p>"},{"location":"limitations/#write-operations","title":"Write Operations","text":"<p>SQLStream is read-only. No support for:</p> <ul> <li>\u274c <code>INSERT INTO</code></li> <li>\u274c <code>UPDATE</code></li> <li>\u274c <code>DELETE</code></li> <li>\u274c <code>CREATE TABLE</code></li> <li>\u274c <code>ALTER TABLE</code></li> <li>\u274c <code>DROP TABLE</code></li> <li>\u274c <code>CREATE VIEW</code></li> <li>\u274c <code>CREATE INDEX</code></li> <li>\u274c Transactions (<code>BEGIN</code>, <code>COMMIT</code>, <code>ROLLBACK</code>)</li> </ul> <p>For data modification: Use pandas, DuckDB, or traditional databases.</p>"},{"location":"limitations/#file-format-limitations","title":"File Format Limitations","text":""},{"location":"limitations/#csv","title":"CSV","text":"<p>Size limits: - Python backend: ~100 MB comfortably (RAM limited) - Pandas backend: up to available RAM</p> <p>Encoding: - \u2705 UTF-8 supported - \u274c Auto-detection of other encodings not implemented - Workaround: Convert to UTF-8 first</p> <p>Delimiters: - \u2705 Comma (,) - default - \u274c Custom delimiters not configurable via SQL - Workaround: Use Python API with custom reader settings</p> <p>Special cases: - \u274c Multi-line cells with newlines - may cause parsing issues - \u274c Files without headers - not supported - \u274c Multiple CSV formats in one query - all must use same format</p>"},{"location":"limitations/#parquet","title":"Parquet","text":"<p>Size limits: - Typically 10x larger than CSV limit due to compression - Streaming supported for S3</p> <p>Version support: - Depends on pyarrow version - Older Parquet formats may not work</p> <p>Compression: - \u2705 Snappy, Gzip, LZ4, ZSTD (via pyarrow) - Decompression automatic</p> <p>Schema evolution: - \u274c Not supported - all files must have same schema for JOINs - \u274c Column addition/removal between versions not handled</p>"},{"location":"limitations/#html-tables","title":"HTML Tables","text":"<p>Limitations: - Only tables that <code>pandas.read_html()</code> can parse - \u274c JavaScript-rendered tables not supported - \u274c Complex nested tables may fail - \u274c No table selection by class/id (only by index)</p> <p>Workaround: Save HTML locally and inspect table indices.</p>"},{"location":"limitations/#markdown-tables","title":"Markdown Tables","text":"<p>Limitations: - Only GitHub Flavored Markdown tables - \u274c No support for HTML tables in markdown - \u274c Tables must be well-formed (aligned pipes) - \u274c No support for merged cells</p>"},{"location":"limitations/#data-source-limitations","title":"Data Source Limitations","text":""},{"location":"limitations/#s3","title":"S3","text":"<p>Authentication: - \u2705 AWS credentials (env vars, config, IAM roles) - \u274c Session tokens with expiration not well handled - \u274c SSO/SAML auth not supported</p> <p>Regions: - \u2705 Works with all AWS regions - \u26a0\ufe0f Cross-region access may be slow - \u26a0\ufe0f Some operations require correct region setting</p> <p>Performance: - First query downloads entire file (no streaming for CSV) - Parquet uses column-level streaming (better) - No support for S3 Select</p> <p>Permissions: - Requires <code>s3:GetObject</code> permission - Does not support bucket listing for discovery</p> <p>Features not supported: - \u274c S3 versioning - \u274c Requester pays buckets - \u274c S3 Glacier / Deep Archive - \u274c S3 Batch Operations</p> <p>Compatible services: - \u2705 MinIO (mostly compatible) - \u2705 DigitalOcean Spaces - \u274c Google Cloud Storage (use HTTP URLs instead) - \u274c Azure Blob Storage</p>"},{"location":"limitations/#httphttps","title":"HTTP/HTTPS","text":"<p>Limitations: - \u274c No authentication (Basic Auth, Bearer tokens, etc.) - \u274c No custom headers - \u274c No POST requests - \u274c No timeout configuration - \u274c No retry logic for failures - \u2705 Basic caching (local directory)</p> <p>SSL/TLS: - \u2705 HTTPS supported - \u274c Certificate verification can't be disabled - \u274c Client certificates not supported</p> <p>Large files: - Downloads entire file before processing - Not ideal for multi-GB files over HTTP</p>"},{"location":"limitations/#local-files","title":"Local Files","text":"<p>Path issues: - \u2705 Relative paths supported - \u2705 Absolute paths supported - \u274c <code>~</code> (home directory) expansion - not implemented - \u274c Glob patterns (<code>*.csv</code>) - not supported</p> <p>Permissions: - Requires read access - No special handling for permission errors</p>"},{"location":"limitations/#performance-limitations","title":"Performance Limitations","text":""},{"location":"limitations/#memory","title":"Memory","text":"<p>Python backend: - Loads entire result set into memory - No streaming for large result sets - Risk of <code>MemoryError</code> with large data</p> <p>Pandas backend: - Better memory management - Still loads results into RAM - Use LIMIT for very large queries</p>"},{"location":"limitations/#concurrency","title":"Concurrency","text":"<ul> <li>\u274c No parallel query execution</li> <li>\u274c No multi-threading for single query</li> <li>\u274c No connection pooling</li> </ul> <p>Workaround: Run multiple Python processes for parallel queries.</p>"},{"location":"limitations/#caching","title":"Caching","text":"<p>HTTP sources: - Basic file-level caching - No TTL or cache invalidation - Cache directory not configurable via SQL</p> <p>S3 sources: - No caching (downloads each time) - No query result caching</p>"},{"location":"limitations/#platform-limitations","title":"Platform Limitations","text":""},{"location":"limitations/#windows","title":"Windows","text":"<p>Known issues: - Interactive shell may not render perfectly in CMD (use Windows Terminal) - File paths: Use forward slashes or double backslashes</p> <p>Supported shells: - \u2705 PowerShell - \u2705 Windows Terminal - \u26a0\ufe0f CMD (basic support) - \u2705 Git Bash</p>"},{"location":"limitations/#python-versions","title":"Python Versions","text":"<p>Supported: - \u2705 Python 3.8+ - \u2705 Python 3.9 - \u2705 Python 3.10 - \u2705 Python 3.11 - \u2705 Python 3.12</p> <p>Not supported: - \u274c Python 2.7 - \u274c Python 3.7 and older</p>"},{"location":"limitations/#type-system-limitations","title":"Type System Limitations","text":""},{"location":"limitations/#type-inference","title":"Type Inference","text":"<p>CSV type detection: - Samples only first 100 rows - May mis-detect types if early rows not representative - No manual type specification</p> <p>Ambiguous types: - Dates without time may be detected as strings - Large integers may overflow - Mixed types in column default to string</p> <p>NULL handling: - Empty strings vs NULL not distinguished in CSV - Parquet NULL handling depends on file</p>"},{"location":"limitations/#type-coercion","title":"Type Coercion","text":"<ul> <li>\u274c No automatic type coercion in JOINs (int != float)</li> <li>\u274c No CAST function to convert types</li> <li>Limited type checking before operations</li> </ul> <p>Workaround: Preprocess with pandas.</p>"},{"location":"limitations/#known-bugs","title":"Known Bugs","text":""},{"location":"limitations/#critical","title":"Critical","text":"<p>None currently identified.</p>"},{"location":"limitations/#high","title":"High","text":"<ol> <li>JOIN performance on large files: O(n*m) complexity for nested loop joins</li> <li>Workaround: Use pandas backend or filter first</li> <li> <p>Planned fix: Phase 12 - Hash joins</p> </li> <li> <p>Memory leaks with repeated queries in interactive shell</p> </li> <li>Workaround: Restart shell periodically</li> <li>Investigation: Ongoing</li> </ol>"},{"location":"limitations/#medium","title":"Medium","text":"<ol> <li>Tab completion in interactive shell doesn't suggest all table names</li> <li>Workaround: Type full table name</li> <li> <p>Planned fix: Phase 10</p> </li> <li> <p>Error messages can be cryptic for SQL syntax errors</p> </li> <li>Workaround: Check query syntax carefully</li> <li> <p>Planned fix: Phase 10 - Better error messages</p> </li> <li> <p>File browser doesn't show hidden files in interactive shell</p> </li> <li>Workaround: Type path manually</li> <li>Planned fix: Add option to show hidden files</li> </ol>"},{"location":"limitations/#low","title":"Low","text":"<ol> <li>Query history limited to 100 entries</li> <li>Workaround: Export important queries</li> <li> <p>Enhancement: Make configurable</p> </li> <li> <p>No syntax highlighting in non-interactive mode</p> </li> <li>Workaround: Use interactive shell</li> <li>Enhancement: Support --pretty flag</li> </ol>"},{"location":"limitations/#comparison-with-other-tools","title":"Comparison with Other Tools","text":""},{"location":"limitations/#vs-duckdb","title":"vs DuckDB","text":"<p>DuckDB advantages: - 100-1000x faster - Full SQL support - Production ready - ACID transactions - Write operations</p> <p>SQLStream advantages: - Simpler installation - Pure Python (no compilation) - Educational (understand internals) - Lighter weight</p> <p>Recommendation: Use DuckDB for production, SQLStream for learning/light analysis.</p>"},{"location":"limitations/#vs-pandas","title":"vs pandas","text":"<p>pandas advantages: - More data manipulation functions - Better performance for transformations - Wider adoption - Jupyter integration</p> <p>SQLStream advantages: - SQL syntax (if you know SQL) - Lazy evaluation - Join multiple files without loading all</p> <p>Recommendation: Use SQLStream to filter/join files, then pandas for analysis.</p>"},{"location":"limitations/#vs-sqlite","title":"vs sqlite","text":"<p>sqlite advantages: - Full ACID database - Mature and stable - Built into Python - Write operations</p> <p>SQLStream advantages: - No database setup - Query files directly - S3 and HTTP sources - Better for file-based workflows</p> <p>Recommendation: Use sqlite for structured data with updates, SQLStream for read-only file analysis.</p>"},{"location":"limitations/#workarounds-best-practices","title":"Workarounds &amp; Best Practices","text":""},{"location":"limitations/#1-complex-queries","title":"1. Complex Queries","text":"<p>Problem: Need features not supported (CTEs, window functions, etc.)</p> <p>Solutions: 1. Use DuckDB for complex queries 2. Use pandas for transformations 3. Break into multiple queries with temporary files 4. Use Python to combine results</p>"},{"location":"limitations/#2-large-files","title":"2. Large Files","text":"<p>Problem: Files too large for memory</p> <p>Solutions: 1. Use pandas backend: <code>--backend pandas</code> 2. Use LIMIT to sample data 3. Use Parquet instead of CSV 4. Filter with WHERE clause before loading 5. Select only needed columns</p>"},{"location":"limitations/#3-performance","title":"3. Performance","text":"<p>Problem: Queries are slow</p> <p>Solutions: 1. Use pandas backend 2. Convert CSV to Parquet 3. Add WHERE filters 4. Select specific columns (not <code>*</code>) 5. Index on frequently joined columns (Parquet metadata) 6. Use LIMIT for exploration</p>"},{"location":"limitations/#4-type-issues","title":"4. Type Issues","text":"<p>Problem: Wrong types inferred</p> <p>Solutions: 1. Preprocess with pandas 2. Use Parquet with explicit types 3. Ensure first 100 rows are representative 4. Cast in Python after query</p>"},{"location":"limitations/#reporting-issues","title":"Reporting Issues","text":"<p>Found a bug or limitation not listed here?</p> <ol> <li>Check: GitHub Issues</li> <li>Report: Create new issue with:</li> <li>SQLStream version</li> <li>Python version</li> <li>Minimal reproduction code</li> <li> <p>Expected vs actual behavior</p> </li> <li> <p>Discuss: Use GitHub Discussions for feature requests</p> </li> </ol>"},{"location":"limitations/#roadmap","title":"Roadmap","text":"<p>See Project Status for upcoming features.</p> <p>Planned improvements: - Phase 10: Better error messages - Phase 11: Testing &amp; documentation - Phase 12+: Subqueries, CTEs, window functions, hash joins</p>"},{"location":"limitations/#see-also","title":"See Also","text":"<ul> <li>FAQ - Common questions</li> <li>Troubleshooting - Error solutions</li> <li>SQL Support - Supported SQL syntax</li> <li>Performance Guide - Optimization tips (coming soon)</li> </ul>"},{"location":"api/overview/","title":"Python API Overview","text":"<p>Use SQLStream programmatically in your Python code.</p>"},{"location":"api/overview/#basic-usage","title":"Basic Usage","text":"<pre><code>from sqlstream import query\n\n# Execute query with explicit source\nresults = query(\"data.csv\").sql(\"SELECT * FROM data WHERE age &gt; 25\")\n\n# Execute query with inline source (extracted from SQL)\nresults = query().sql(\"SELECT * FROM 'data.csv' WHERE age &gt; 25\")\n\n# Iterate (lazy)\nfor row in results:\n    print(row)\n\n# Or convert to list (eager)\nresults_list = query().sql(\"SELECT * FROM 'data.csv'\").to_list()\n</code></pre>"},{"location":"api/overview/#backend-selection","title":"Backend Selection","text":"<p>SQLStream offers three execution backends with different performance characteristics and SQL support:</p>"},{"location":"api/overview/#auto-backend-default","title":"Auto Backend (Default)","text":"<pre><code>from sqlstream import query\n\n# Automatically selects: pandas &gt; duckdb &gt; python\nresults = query(\"data.csv\").sql(\"SELECT * FROM data\")\n</code></pre> <p>Backend priority: 1. Pandas (if installed) - Fast execution, basic SQL 2. DuckDB (if pandas not available, duckdb installed) - Full SQL, maximum performance 3. Python (fallback) - Educational, Volcano model</p>"},{"location":"api/overview/#explicit-backend","title":"Explicit Backend","text":"<pre><code># Force DuckDB backend (full SQL support)\nresults = query(\"data.csv\").sql(\"\"\"\n    SELECT \n        department,\n        AVG(salary) as avg_salary,\n        ROW_NUMBER() OVER (ORDER BY AVG(salary) DESC) as rank\n    FROM data\n    GROUP BY department\n\"\"\", backend=\"duckdb\")\n\n# Force Pandas backend (10-100x faster than Python)\nresults = query(\"data.csv\").sql(\n    \"SELECT * FROM data WHERE age &gt; 25\", \n    backend=\"pandas\"\n)\n\n# Force Python backend (educational)\nresults = query(\"data.csv\").sql(\n    \"SELECT * FROM data WHERE age &gt; 25\", \n    backend=\"python\"\n)\n</code></pre> <p>When to use each backend: - <code>backend=\"duckdb\"</code> - Complex SQL (CTEs, window functions), production workloads, maximum performance - <code>backend=\"pandas\"</code> - Balance of speed and simplicity, basic queries - <code>backend=\"python\"</code> - Learning query execution, understanding Volcano model - <code>backend=\"auto\"</code> (default) - Let SQLStream choose the best available</p>"},{"location":"api/overview/#working-with-results","title":"Working with Results","text":""},{"location":"api/overview/#lazy-iteration-memory-efficient","title":"Lazy Iteration (Memory-Efficient)","text":"<pre><code># Process rows one at a time (doesn't load all into memory)\nresults = query(\"large_file.csv\").sql(\"SELECT * FROM large_file\")\n\nfor row in results:\n    # Each row is a dictionary\n    print(f\"{row['name']}: {row['value']}\")\n</code></pre>"},{"location":"api/overview/#eager-loading","title":"Eager Loading","text":"<pre><code># Load all results into a list\nresults_list = query(\"data.csv\").sql(\"SELECT * FROM data\").to_list()\n\n# Now you can access by index, slice, etc.\nfirst_row = results_list[0]\ntop_10 = results_list[:10]\n</code></pre>"},{"location":"api/overview/#integration-with-other-libraries","title":"Integration with Other Libraries","text":"<pre><code># Convert to pandas DataFrame\nimport pandas as pd\n\nresults = query(\"data.csv\").sql(\"SELECT * FROM data\")\ndf = pd.DataFrame(results.to_list())\n\n# Convert to JSON\nimport json\n\nresults = query(\"data.csv\").sql(\"SELECT * FROM data\")\njson_str = json.dumps(results.to_list(), indent=2)\n\n# Write to CSV\nimport csv\n\nresults = query(\"data.csv\").sql(\"SELECT * FROM data\")\nwith open(\"output.csv\", \"w\", newline=\"\") as f:\n    if results.to_list():\n        writer = csv.DictWriter(f, fieldnames=results.to_list()[0].keys())\n        writer.writeheader()\n        writer.writerows(results.to_list())\n</code></pre>"},{"location":"api/overview/#multiple-file-queries","title":"Multiple File Queries","text":""},{"location":"api/overview/#inline-file-paths-recommended","title":"Inline File Paths (Recommended)","text":"<pre><code># JOIN multiple files\nresults = query().sql(\"\"\"\n    SELECT \n        e.name, \n        e.salary, \n        d.department_name\n    FROM 'employees.csv' e\n    JOIN 'departments.csv' d ON e.dept_id = d.id\n    WHERE e.salary &gt; 75000\n\"\"\")\n</code></pre>"},{"location":"api/overview/#mixed-sources","title":"Mixed Sources","text":"<pre><code># Query from different sources\nresults = query().sql(\"\"\"\n    SELECT \n        local.*, \n        remote.category\n    FROM 'data/local.csv' local\n    JOIN 's3://bucket/remote.parquet' remote \n        ON local.id = remote.id\n\"\"\", backend=\"duckdb\")\n</code></pre>"},{"location":"api/overview/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/overview/#query-explain-plan","title":"Query Explain Plan","text":"<pre><code># View query execution plan\nresults = query(\"data.csv\").sql(\n    \"SELECT * FROM data WHERE age &gt; 25\", \n    backend=\"python\"\n)\n\n# Get explain plan\nplan = results.explain()\nprint(plan)\n</code></pre> <p>Example output: <pre><code>Filter (predicate: age &gt; 25)\n\u2514\u2500 Scan (source: data.csv, columns: ['name', 'age', 'salary'])\n</code></pre></p>"},{"location":"api/overview/#s3-and-http-sources","title":"S3 and HTTP Sources","text":"<pre><code># Query S3 data\nresults = query(\"s3://my-bucket/data.parquet\").sql(\"\"\"\n    SELECT * FROM data \n    WHERE date &gt; '2024-01-01'\n    LIMIT 100\n\"\"\", backend=\"duckdb\")\n\n# Query HTTP URL\nresults = query(\"https://example.com/data.csv\").sql(\"\"\"\n    SELECT category, COUNT(*) as count\n    FROM data\n    GROUP BY category\n\"\"\")\n</code></pre>"},{"location":"api/overview/#complex-analytics","title":"Complex Analytics","text":"<pre><code># Multi-step analytics with DuckDB\nresults = query().sql(\"\"\"\n    WITH monthly_sales AS (\n        SELECT \n            DATE_TRUNC('month', sale_date) as month,\n            product_id,\n            SUM(amount) as total_sales\n        FROM 's3://bucket/sales.parquet'\n        GROUP BY 1, 2\n    ),\n    ranked AS (\n        SELECT \n            *,\n            ROW_NUMBER() OVER (\n                PARTITION BY month \n                ORDER BY total_sales DESC\n            ) as rank\n        FROM monthly_sales\n    )\n    SELECT * FROM ranked WHERE rank &lt;= 10\n\"\"\", backend=\"duckdb\")\n\nfor row in results:\n    print(f\"{row['month']}: {row['product_id']} - ${row['total_sales']:,.2f}\")\n</code></pre>"},{"location":"api/overview/#error-handling","title":"Error Handling","text":"<pre><code>from sqlstream import query\n\ntry:\n    results = query(\"data.csv\").sql(\n        \"SELECT * FROM data WHERE invalid_column &gt; 10\"\n    )\n    for row in results:\n        print(row)\nexcept FileNotFoundError:\n    print(\"File not found\")\nexcept ValueError as e:\n    print(f\"Query error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"api/overview/#best-practices","title":"Best Practices","text":""},{"location":"api/overview/#1-use-lazy-iteration-for-large-files","title":"1. Use Lazy Iteration for Large Files","text":"<pre><code># \u2705 Good: Memory-efficient\nfor row in query(\"huge.csv\").sql(\"SELECT * FROM huge\"):\n    process(row)\n\n# \u274c Bad: Loads everything into memory\nall_rows = query(\"huge.csv\").sql(\"SELECT * FROM huge\").to_list()\n</code></pre>"},{"location":"api/overview/#2-choose-the-right-backend","title":"2. Choose the Right Backend","text":"<pre><code># \u2705 Good: DuckDB for complex SQL\nresults = query().sql(\"\"\"\n    WITH stats AS (SELECT AVG(value) as avg FROM 'data.csv')\n    SELECT * FROM 'data.csv' WHERE value &gt; (SELECT avg FROM stats)\n\"\"\", backend=\"duckdb\")\n\n# \u274c Bad: Python backend doesn't support CTEs\n# results = query().sql(\"WITH ...\", backend=\"python\")  # Will fail\n</code></pre>"},{"location":"api/overview/#3-use-inline-paths-for-multi-file-queries","title":"3. Use Inline Paths for Multi-File Queries","text":"<pre><code># \u2705 Good: Clear and simple\nresults = query().sql(\"\"\"\n    SELECT * FROM 'file1.csv' a \n    JOIN 'file2.csv' b ON a.id = b.id\n\"\"\")\n\n# \u274c Less convenient: Multiple query objects\n# q1 = query(\"file1.csv\")\n# q2 = query(\"file2.csv\")\n# ...\n</code></pre>"},{"location":"api/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Advanced Python Guides - In-depth Python API usage</li> <li>SQL Support - Supported SQL syntax</li> <li>Backend Comparison - Choose the right backend</li> <li>Examples - Real-world code examples</li> </ul>"},{"location":"api/guides/advanced/","title":"Advanced Usage","text":""},{"location":"api/guides/advanced/#backend-selection","title":"Backend Selection","text":"<p>You can explicitly choose the execution backend to optimize for your specific use case.</p> <pre><code># Force pure Python backend (streaming, low memory)\nquery(\"large_file.csv\").sql(\"SELECT *\", backend=\"python\")\n\n# Force Pandas backend (fast, high memory)\nquery(\"data.csv\").sql(\"SELECT *\", backend=\"pandas\")\n</code></pre>"},{"location":"api/guides/advanced/#s3-configuration","title":"S3 Configuration","text":"<p>To access S3, ensure you have <code>s3fs</code> installed and your AWS credentials configured.</p> <pre><code>pip install sqlstream[s3]\nexport AWS_ACCESS_KEY_ID=...\nexport AWS_SECRET_ACCESS_KEY=...\n</code></pre> <p>Then simply use <code>s3://</code> paths.</p>"},{"location":"api/guides/advanced/#custom-readers","title":"Custom Readers","text":"<p>You can implement custom readers by inheriting from <code>BaseReader</code>. This allows you to support proprietary formats or custom data sources.</p> <pre><code>from sqlstream.readers.base import BaseReader\n\nclass MyCustomReader(BaseReader):\n    def read_lazy(self):\n        yield {\"col\": \"value\"}\n</code></pre>"},{"location":"api/reference/cli/","title":"CLI Reference","text":"<p>CLI API documentation.</p>"},{"location":"api/reference/cli/#baseformatter","title":"BaseFormatter","text":""},{"location":"api/reference/cli/#sqlstream.cli.formatters.base.BaseFormatter","title":"BaseFormatter","text":"<p>Base class for all output formatters</p> Source code in <code>sqlstream/cli/formatters/base.py</code> <pre><code>class BaseFormatter:\n    \"\"\"Base class for all output formatters\"\"\"\n\n    def format(self, results: List[Dict[str, Any]], **kwargs) -&gt; str:\n        \"\"\"\n        Format query results for output\n\n        Args:\n            results: List of result dictionaries\n            **kwargs: Additional formatter-specific options\n\n        Returns:\n            Formatted string ready for output\n        \"\"\"\n        raise NotImplementedError(\"Formatters must implement format() method\")\n\n    def get_name(self) -&gt; str:\n        \"\"\"Get formatter name\"\"\"\n        return self.__class__.__name__.replace(\"Formatter\", \"\").lower()\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.formatters.base.BaseFormatter.format","title":"format","text":"<pre><code>format(results: List[Dict[str, Any]], **kwargs) -&gt; str\n</code></pre> <p>Format query results for output</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Dict[str, Any]]</code> <p>List of result dictionaries</p> required <code>**kwargs</code> <p>Additional formatter-specific options</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string ready for output</p> Source code in <code>sqlstream/cli/formatters/base.py</code> <pre><code>def format(self, results: List[Dict[str, Any]], **kwargs) -&gt; str:\n    \"\"\"\n    Format query results for output\n\n    Args:\n        results: List of result dictionaries\n        **kwargs: Additional formatter-specific options\n\n    Returns:\n        Formatted string ready for output\n    \"\"\"\n    raise NotImplementedError(\"Formatters must implement format() method\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.formatters.base.BaseFormatter.get_name","title":"get_name","text":"<pre><code>get_name() -&gt; str\n</code></pre> <p>Get formatter name</p> Source code in <code>sqlstream/cli/formatters/base.py</code> <pre><code>def get_name(self) -&gt; str:\n    \"\"\"Get formatter name\"\"\"\n    return self.__class__.__name__.replace(\"Formatter\", \"\").lower()\n</code></pre>"},{"location":"api/reference/cli/#csvformatter","title":"CSVFormatter","text":""},{"location":"api/reference/cli/#sqlstream.cli.formatters.csv.CSVFormatter","title":"CSVFormatter","text":"<p>               Bases: <code>BaseFormatter</code></p> <p>Format results as CSV</p> Source code in <code>sqlstream/cli/formatters/csv.py</code> <pre><code>class CSVFormatter(BaseFormatter):\n    \"\"\"Format results as CSV\"\"\"\n\n    def format(self, results: List[Dict[str, Any]], **kwargs) -&gt; str:\n        \"\"\"\n        Format results as CSV\n\n        Args:\n            results: List of result dictionaries\n            **kwargs: Options like 'delimiter', 'quote_all'\n\n        Returns:\n            CSV string\n        \"\"\"\n        if not results:\n            return \"\"\n\n        # Get column names from first row\n        columns = list(results[0].keys())\n\n        # Create CSV in memory\n        output = io.StringIO()\n        writer = csv.DictWriter(\n            output,\n            fieldnames=columns,\n            delimiter=kwargs.get(\"delimiter\", \",\"),\n            quoting=csv.QUOTE_MINIMAL if not kwargs.get(\"quote_all\") else csv.QUOTE_ALL,\n        )\n\n        # Write header and rows\n        writer.writeheader()\n        writer.writerows(results)\n\n        return output.getvalue()\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.formatters.csv.CSVFormatter.format","title":"format","text":"<pre><code>format(results: List[Dict[str, Any]], **kwargs) -&gt; str\n</code></pre> <p>Format results as CSV</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Dict[str, Any]]</code> <p>List of result dictionaries</p> required <code>**kwargs</code> <p>Options like 'delimiter', 'quote_all'</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>CSV string</p> Source code in <code>sqlstream/cli/formatters/csv.py</code> <pre><code>def format(self, results: List[Dict[str, Any]], **kwargs) -&gt; str:\n    \"\"\"\n    Format results as CSV\n\n    Args:\n        results: List of result dictionaries\n        **kwargs: Options like 'delimiter', 'quote_all'\n\n    Returns:\n        CSV string\n    \"\"\"\n    if not results:\n        return \"\"\n\n    # Get column names from first row\n    columns = list(results[0].keys())\n\n    # Create CSV in memory\n    output = io.StringIO()\n    writer = csv.DictWriter(\n        output,\n        fieldnames=columns,\n        delimiter=kwargs.get(\"delimiter\", \",\"),\n        quoting=csv.QUOTE_MINIMAL if not kwargs.get(\"quote_all\") else csv.QUOTE_ALL,\n    )\n\n    # Write header and rows\n    writer.writeheader()\n    writer.writerows(results)\n\n    return output.getvalue()\n</code></pre>"},{"location":"api/reference/cli/#jsonformatter","title":"JSONFormatter","text":""},{"location":"api/reference/cli/#sqlstream.cli.formatters.json.JSONFormatter","title":"JSONFormatter","text":"<p>               Bases: <code>BaseFormatter</code></p> <p>Format results as JSON</p> Source code in <code>sqlstream/cli/formatters/json.py</code> <pre><code>class JSONFormatter(BaseFormatter):\n    \"\"\"Format results as JSON\"\"\"\n\n    def format(self, results: List[Dict[str, Any]], **kwargs) -&gt; str:\n        \"\"\"\n        Format results as JSON\n\n        Args:\n            results: List of result dictionaries\n            **kwargs: Options like 'compact', 'indent'\n\n        Returns:\n            JSON string\n        \"\"\"\n        # Handle NaN and infinity values (convert to null)\n        def clean_value(val):\n            if isinstance(val, float):\n                if math.isnan(val) or math.isinf(val):\n                    return None\n            return val\n\n        # Clean all values\n        cleaned_results = [\n            {k: clean_value(v) for k, v in row.items()}\n            for row in results\n        ]\n\n        # Format as JSON\n        if kwargs.get(\"compact\", False):\n            return json.dumps(cleaned_results, separators=(',', ':'))\n        else:\n            indent = kwargs.get(\"indent\", 2)\n            return json.dumps(cleaned_results, indent=indent)\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.formatters.json.JSONFormatter.format","title":"format","text":"<pre><code>format(results: List[Dict[str, Any]], **kwargs) -&gt; str\n</code></pre> <p>Format results as JSON</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Dict[str, Any]]</code> <p>List of result dictionaries</p> required <code>**kwargs</code> <p>Options like 'compact', 'indent'</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>JSON string</p> Source code in <code>sqlstream/cli/formatters/json.py</code> <pre><code>def format(self, results: List[Dict[str, Any]], **kwargs) -&gt; str:\n    \"\"\"\n    Format results as JSON\n\n    Args:\n        results: List of result dictionaries\n        **kwargs: Options like 'compact', 'indent'\n\n    Returns:\n        JSON string\n    \"\"\"\n    # Handle NaN and infinity values (convert to null)\n    def clean_value(val):\n        if isinstance(val, float):\n            if math.isnan(val) or math.isinf(val):\n                return None\n        return val\n\n    # Clean all values\n    cleaned_results = [\n        {k: clean_value(v) for k, v in row.items()}\n        for row in results\n    ]\n\n    # Format as JSON\n    if kwargs.get(\"compact\", False):\n        return json.dumps(cleaned_results, separators=(',', ':'))\n    else:\n        indent = kwargs.get(\"indent\", 2)\n        return json.dumps(cleaned_results, indent=indent)\n</code></pre>"},{"location":"api/reference/cli/#markdownformatter","title":"MarkdownFormatter","text":""},{"location":"api/reference/cli/#sqlstream.cli.formatters.markdown.MarkdownFormatter","title":"MarkdownFormatter","text":"<p>               Bases: <code>BaseFormatter</code></p> <p>Format results as a Markdown table</p> Source code in <code>sqlstream/cli/formatters/markdown.py</code> <pre><code>class MarkdownFormatter(BaseFormatter):\n    \"\"\"Format results as a Markdown table\"\"\"\n\n    def format(self, results: List[Dict[str, Any]], **kwargs) -&gt; str:\n        \"\"\"\n        Format results as a Markdown table\n\n        Args:\n            results: List of result dictionaries\n            **kwargs: Options like 'show_footer', 'align'\n\n        Returns:\n            Markdown formatted table string\n        \"\"\"\n        if not results:\n            return \"_No results found._\"\n\n        # Get columns from first row\n        columns = list(results[0].keys())\n\n        # Build header row\n        header = \"| \" + \" | \".join(columns) + \" |\"\n\n        # Build separator row with alignment\n        # Default alignment is left, can be 'left', 'center', or 'right'\n        align = kwargs.get(\"align\", \"left\")\n        separators = []\n        for col in columns:\n            col_align = align if isinstance(align, str) else align.get(col, \"left\")\n            if col_align == \"center\":\n                separators.append(\":---:\")\n            elif col_align == \"right\":\n                separators.append(\"---:\")\n            else:  # left or default\n                separators.append(\":---\")\n\n        separator = \"| \" + \" | \".join(separators) + \" |\"\n\n        # Build data rows\n        data_rows = []\n        for row in results:\n            # Format each cell value\n            values = []\n            for col in columns:\n                val = row[col]\n                if val is None:\n                    formatted_val = \"_NULL_\"\n                elif isinstance(val, str):\n                    # Escape pipe characters in strings\n                    formatted_val = str(val).replace(\"|\", \"\\\\|\")\n                else:\n                    formatted_val = str(val)\n                values.append(formatted_val)\n\n            data_rows.append(\"| \" + \" | \".join(values) + \" |\")\n\n        # Combine all parts\n        table_lines = [header, separator] + data_rows\n        output = \"\\n\".join(table_lines)\n\n        # Add footer with row count if requested\n        if kwargs.get(\"show_footer\", True):\n            row_count = len(results)\n            footer = f\"\\n\\n_{row_count} row{'s' if row_count != 1 else ''}_\"\n            output += footer\n\n        return output\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.formatters.markdown.MarkdownFormatter.format","title":"format","text":"<pre><code>format(results: List[Dict[str, Any]], **kwargs) -&gt; str\n</code></pre> <p>Format results as a Markdown table</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Dict[str, Any]]</code> <p>List of result dictionaries</p> required <code>**kwargs</code> <p>Options like 'show_footer', 'align'</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Markdown formatted table string</p> Source code in <code>sqlstream/cli/formatters/markdown.py</code> <pre><code>def format(self, results: List[Dict[str, Any]], **kwargs) -&gt; str:\n    \"\"\"\n    Format results as a Markdown table\n\n    Args:\n        results: List of result dictionaries\n        **kwargs: Options like 'show_footer', 'align'\n\n    Returns:\n        Markdown formatted table string\n    \"\"\"\n    if not results:\n        return \"_No results found._\"\n\n    # Get columns from first row\n    columns = list(results[0].keys())\n\n    # Build header row\n    header = \"| \" + \" | \".join(columns) + \" |\"\n\n    # Build separator row with alignment\n    # Default alignment is left, can be 'left', 'center', or 'right'\n    align = kwargs.get(\"align\", \"left\")\n    separators = []\n    for col in columns:\n        col_align = align if isinstance(align, str) else align.get(col, \"left\")\n        if col_align == \"center\":\n            separators.append(\":---:\")\n        elif col_align == \"right\":\n            separators.append(\"---:\")\n        else:  # left or default\n            separators.append(\":---\")\n\n    separator = \"| \" + \" | \".join(separators) + \" |\"\n\n    # Build data rows\n    data_rows = []\n    for row in results:\n        # Format each cell value\n        values = []\n        for col in columns:\n            val = row[col]\n            if val is None:\n                formatted_val = \"_NULL_\"\n            elif isinstance(val, str):\n                # Escape pipe characters in strings\n                formatted_val = str(val).replace(\"|\", \"\\\\|\")\n            else:\n                formatted_val = str(val)\n            values.append(formatted_val)\n\n        data_rows.append(\"| \" + \" | \".join(values) + \" |\")\n\n    # Combine all parts\n    table_lines = [header, separator] + data_rows\n    output = \"\\n\".join(table_lines)\n\n    # Add footer with row count if requested\n    if kwargs.get(\"show_footer\", True):\n        row_count = len(results)\n        footer = f\"\\n\\n_{row_count} row{'s' if row_count != 1 else ''}_\"\n        output += footer\n\n    return output\n</code></pre>"},{"location":"api/reference/cli/#tableformatter","title":"TableFormatter","text":""},{"location":"api/reference/cli/#sqlstream.cli.formatters.table.TableFormatter","title":"TableFormatter","text":"<p>               Bases: <code>BaseFormatter</code></p> <p>Format results as a beautiful Rich table</p> Source code in <code>sqlstream/cli/formatters/table.py</code> <pre><code>class TableFormatter(BaseFormatter):\n    \"\"\"Format results as a beautiful Rich table\"\"\"\n\n    def format(self, results: List[Dict[str, Any]], **kwargs) -&gt; str:\n        \"\"\"\n        Format results as a Rich table\n\n        Args:\n            results: List of result dictionaries\n            **kwargs: Options like 'no_color', 'show_footer'\n\n        Returns:\n            Formatted table string\n        \"\"\"\n        if not RICH_AVAILABLE:\n            raise ImportError(\n                \"Table formatter requires rich library. \"\n                \"Install `sqlstream[cli]`\"\n            )\n\n        if not results:\n            return \"No results found.\"\n\n        # Get terminal width and column info\n        console = Console(force_terminal=not kwargs.get(\"no_color\", False))\n        terminal_width = console.width\n        columns = list(results[0].keys())\n        num_cols = len(columns)\n\n        # Adaptive column width based on terminal size\n        if terminal_width &lt; 80 or num_cols &gt; 8:\n            # Narrow terminal or many columns: aggressive truncation\n            max_col_width = kwargs.get(\"max_width\", 15)\n            table = Table(show_header=True, header_style=\"bold magenta\", box=box.SIMPLE)\n\n            for col in columns:\n                table.add_column(\n                    col,\n                    style=\"cyan\",\n                    overflow=\"ellipsis\",\n                    max_width=max_col_width,\n                    no_wrap=True,\n                )\n        else:\n            # Normal table with moderate truncation\n            table = Table(show_header=True, header_style=\"bold magenta\")\n\n            for col in columns:\n                table.add_column(\n                    col, style=\"cyan\", overflow=\"ellipsis\", max_width=30, no_wrap=False\n                )\n\n        # Add rows\n        for row in results:\n            # Convert None to \"NULL\" for display\n            values = [\n                str(row[col]) if row[col] is not None else \"[dim]NULL[/dim]\"\n                for col in columns\n            ]\n            table.add_row(*values)\n\n        # Render table to string\n        with console.capture() as capture:\n            console.print(table)\n\n        output = capture.get()\n\n        # Add footer with row count if requested\n        if kwargs.get(\"show_footer\", True):\n            row_count = len(results)\n            footer = f\"\\n[dim]{row_count} row{'s' if row_count != 1 else ''}[/dim]\"\n            with console.capture() as capture:\n                console.print(footer)\n            output += capture.get()\n\n        return output\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.formatters.table.TableFormatter.format","title":"format","text":"<pre><code>format(results: List[Dict[str, Any]], **kwargs) -&gt; str\n</code></pre> <p>Format results as a Rich table</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Dict[str, Any]]</code> <p>List of result dictionaries</p> required <code>**kwargs</code> <p>Options like 'no_color', 'show_footer'</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted table string</p> Source code in <code>sqlstream/cli/formatters/table.py</code> <pre><code>def format(self, results: List[Dict[str, Any]], **kwargs) -&gt; str:\n    \"\"\"\n    Format results as a Rich table\n\n    Args:\n        results: List of result dictionaries\n        **kwargs: Options like 'no_color', 'show_footer'\n\n    Returns:\n        Formatted table string\n    \"\"\"\n    if not RICH_AVAILABLE:\n        raise ImportError(\n            \"Table formatter requires rich library. \"\n            \"Install `sqlstream[cli]`\"\n        )\n\n    if not results:\n        return \"No results found.\"\n\n    # Get terminal width and column info\n    console = Console(force_terminal=not kwargs.get(\"no_color\", False))\n    terminal_width = console.width\n    columns = list(results[0].keys())\n    num_cols = len(columns)\n\n    # Adaptive column width based on terminal size\n    if terminal_width &lt; 80 or num_cols &gt; 8:\n        # Narrow terminal or many columns: aggressive truncation\n        max_col_width = kwargs.get(\"max_width\", 15)\n        table = Table(show_header=True, header_style=\"bold magenta\", box=box.SIMPLE)\n\n        for col in columns:\n            table.add_column(\n                col,\n                style=\"cyan\",\n                overflow=\"ellipsis\",\n                max_width=max_col_width,\n                no_wrap=True,\n            )\n    else:\n        # Normal table with moderate truncation\n        table = Table(show_header=True, header_style=\"bold magenta\")\n\n        for col in columns:\n            table.add_column(\n                col, style=\"cyan\", overflow=\"ellipsis\", max_width=30, no_wrap=False\n            )\n\n    # Add rows\n    for row in results:\n        # Convert None to \"NULL\" for display\n        values = [\n            str(row[col]) if row[col] is not None else \"[dim]NULL[/dim]\"\n            for col in columns\n        ]\n        table.add_row(*values)\n\n    # Render table to string\n    with console.capture() as capture:\n        console.print(table)\n\n    output = capture.get()\n\n    # Add footer with row count if requested\n    if kwargs.get(\"show_footer\", True):\n        row_count = len(results)\n        footer = f\"\\n[dim]{row_count} row{'s' if row_count != 1 else ''}[/dim]\"\n        with console.capture() as capture:\n            console.print(footer)\n        output += capture.get()\n\n    return output\n</code></pre>"},{"location":"api/reference/cli/#launch_interactive","title":"launch_interactive","text":""},{"location":"api/reference/cli/#sqlstream.cli.interactive.launch_interactive","title":"launch_interactive","text":"<pre><code>launch_interactive(results: List[Dict[str, Any]]) -&gt; None\n</code></pre> <p>Launch interactive table viewer.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Dict[str, Any]]</code> <p>Query results to display</p> required <p>Raises:</p> Type Description <code>ImportError</code> <p>If textual library is not installed</p> Source code in <code>sqlstream/cli/interactive.py</code> <pre><code>def launch_interactive(results: List[Dict[str, Any]]) -&gt; None:\n    \"\"\"\n    Launch interactive table viewer.\n\n    Args:\n        results: Query results to display\n\n    Raises:\n        ImportError: If textual library is not installed\n    \"\"\"\n    if not TEXTUAL_AVAILABLE:\n        raise ImportError(\n            \"Interactive mode requires textual library. \"\n            \"Install `sqlstream[cli]`\"\n        )\n\n    app = TableApp(results)\n    app.run()\n</code></pre>"},{"location":"api/reference/cli/#should_use_interactive","title":"should_use_interactive","text":""},{"location":"api/reference/cli/#sqlstream.cli.interactive.should_use_interactive","title":"should_use_interactive","text":"<pre><code>should_use_interactive(results: List[Dict[str, Any]], force: bool = False, no_interactive: bool = False, output_file: str = None, fmt: str = 'table') -&gt; bool\n</code></pre> <p>Determine if interactive mode should be used.</p> <p>Parameters:</p> Name Type Description Default <code>results</code> <code>List[Dict[str, Any]]</code> <p>Query results to display</p> required <code>force</code> <code>bool</code> <p>Force interactive mode (--interactive flag)</p> <code>False</code> <code>no_interactive</code> <code>bool</code> <p>Disable interactive mode (--no-interactive flag)</p> <code>False</code> <code>output_file</code> <code>str</code> <p>Output file path (if set, disable interactive)</p> <code>None</code> <code>fmt</code> <code>str</code> <p>Output format (only use interactive for table format)</p> <code>'table'</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if interactive mode should be used, False otherwise</p> Source code in <code>sqlstream/cli/interactive.py</code> <pre><code>def should_use_interactive(\n    results: List[Dict[str, Any]],\n    force: bool = False,\n    no_interactive: bool = False,\n    output_file: str = None,\n    fmt: str = \"table\",\n) -&gt; bool:\n    \"\"\"\n    Determine if interactive mode should be used.\n\n    Args:\n        results: Query results to display\n        force: Force interactive mode (--interactive flag)\n        no_interactive: Disable interactive mode (--no-interactive flag)\n        output_file: Output file path (if set, disable interactive)\n        fmt: Output format (only use interactive for table format)\n\n    Returns:\n        True if interactive mode should be used, False otherwise\n    \"\"\"\n    # Force flag overrides everything\n    if force:\n        return True\n\n    # Never use interactive if:\n    if no_interactive or output_file or fmt != \"table\":\n        return False\n\n    # Not a TTY (piped output)\n    if not sys.stdout.isatty():\n        return False\n\n    # No results = no need for interactive\n    if not results:\n        return False\n\n    # Auto-detection logic\n    terminal_width = shutil.get_terminal_size().columns\n    columns = list(results[0].keys())\n    num_cols = len(columns)\n\n    # Too many columns\n    if num_cols &gt; 10:\n        return True\n\n    # Terminal too narrow\n    if terminal_width &lt; 80 and num_cols &gt; 5:\n        return True\n\n    # Estimate table width (rough: col_name + avg_value_length + borders)\n    estimated_width = 0\n    for col in columns:\n        col_width = len(col)\n        # Sample first 5 rows to estimate column width\n        for row in results[: min(5, len(results))]:\n            val_len = len(str(row[col]))\n            col_width = max(col_width, val_len)\n        estimated_width += col_width + 3  # +3 for padding/borders\n\n    # Table too wide for terminal\n    if estimated_width &gt; terminal_width * 0.9:\n        return True\n\n    # Check for very long values\n    for row in results[: min(10, len(results))]:  # Sample first 10 rows\n        for val in row.values():\n            if len(str(val)) &gt; 50:\n                return True\n\n    return False\n</code></pre>"},{"location":"api/reference/cli/#cli","title":"cli","text":""},{"location":"api/reference/cli/#sqlstream.cli.main.cli","title":"cli","text":"<pre><code>cli()\n</code></pre> <p>SQLStream - Query CSV/Parquet files with SQL</p> <p>A lightweight SQL query engine for data exploration.</p> Source code in <code>sqlstream/cli/main.py</code> <pre><code>@click.group()\n@click.version_option(version=\"0.1.0\", prog_name=\"sqlstream\")\ndef cli():\n    \"\"\"\n    SQLStream - Query CSV/Parquet files with SQL\n\n    A lightweight SQL query engine for data exploration.\n    \"\"\"\n    if not CLICK_AVAILABLE:\n        print(\"CLI requires click library. Install with: pip install sqlstream[cli]\")\n        sys.exit(1)\n</code></pre>"},{"location":"api/reference/cli/#query","title":"query","text":""},{"location":"api/reference/cli/#sqlstream.cli.main.query","title":"query","text":"<pre><code>query(file_or_sql: str, sql: Optional[str], format: str, backend: str, limit: Optional[int], output: Optional[str], no_color: bool, explain: bool, show_time: bool, interactive: bool, no_interactive: bool)\n</code></pre> <p>Execute SQL query on a data file</p> <p>Examples:</p> <pre><code>\b\n# OLD SYNTAX: Query local CSV file (backward compatible)\n$ sqlstream query data.csv \"SELECT * FROM data WHERE age &gt; 25\"\n\n\b\n# NEW SYNTAX: Inline file paths in SQL\n$ sqlstream query \"SELECT * FROM 'data.csv' WHERE age &gt; 25\"\n\n\b\n# Multi-file JOIN with inline paths\n$ sqlstream query \"SELECT x.*, y.name FROM 'data.csv' x JOIN 'other.csv' y ON x.id = y.id\"\n\n\b\n# Query from URL with JSON output\n$ sqlstream query https://example.com/data.csv \"SELECT name FROM data\" -f json\n\n\b\n# Use pandas backend for performance\n$ sqlstream query data.parquet \"SELECT * FROM data\" --backend pandas\n\n\b\n# Show query plan\n$ sqlstream query data.csv \"SELECT * FROM data WHERE age &gt; 25\" --explain\n\n\b\n# Save results to file\n$ sqlstream query data.csv \"SELECT * FROM data\" -f csv -o results.csv\n</code></pre> Source code in <code>sqlstream/cli/main.py</code> <pre><code>@cli.command()\n@click.argument(\"file_or_sql\", type=str)\n@click.argument(\"sql\", type=str, required=False)\n@click.option(\n    \"--format\",\n    \"-f\",\n    type=click.Choice([\"table\", \"json\", \"csv\", \"markdown\"], case_sensitive=False),\n    default=\"table\",\n    help=\"Output format (default: table)\",\n)\n@click.option(\n    \"--backend\",\n    \"-b\",\n    type=click.Choice([\"auto\", \"pandas\", \"python\", \"duckdb\"], case_sensitive=False),\n    default=\"auto\",\n    help=\"Execution backend (default: auto, tries pandas &gt; duckdb &gt; python)\",\n)\n@click.option(\n    \"--limit\",\n    \"-l\",\n    type=int,\n    default=None,\n    help=\"Limit number of rows displayed\",\n)\n@click.option(\n    \"--output\",\n    \"-o\",\n    type=click.Path(),\n    default=None,\n    help=\"Write output to file instead of stdout\",\n)\n@click.option(\n    \"--no-color\",\n    is_flag=True,\n    help=\"Disable colored output\",\n)\n@click.option(\n    \"--explain\",\n    is_flag=True,\n    help=\"Show query execution plan instead of results\",\n)\n@click.option(\n    \"--time\",\n    \"-t\",\n    \"show_time\",\n    is_flag=True,\n    help=\"Show execution time\",\n)\n@click.option(\n    \"--interactive\",\n    \"-i\",\n    is_flag=True,\n    help=\"Force interactive mode (scrollable table viewer)\",\n)\n@click.option(\n    \"--no-interactive\",\n    is_flag=True,\n    help=\"Disable auto-detection of interactive mode\",\n)\ndef query(\n    file_or_sql: str,\n    sql: Optional[str],\n    format: str,\n    backend: str,\n    limit: Optional[int],\n    output: Optional[str],\n    no_color: bool,\n    explain: bool,\n    show_time: bool,\n    interactive: bool,\n    no_interactive: bool,\n):\n    \"\"\"\n    Execute SQL query on a data file\n\n    Examples:\n\n        \\b\n        # OLD SYNTAX: Query local CSV file (backward compatible)\n        $ sqlstream query data.csv \"SELECT * FROM data WHERE age &gt; 25\"\n\n        \\b\n        # NEW SYNTAX: Inline file paths in SQL\n        $ sqlstream query \"SELECT * FROM 'data.csv' WHERE age &gt; 25\"\n\n        \\b\n        # Multi-file JOIN with inline paths\n        $ sqlstream query \"SELECT x.*, y.name FROM 'data.csv' x JOIN 'other.csv' y ON x.id = y.id\"\n\n        \\b\n        # Query from URL with JSON output\n        $ sqlstream query https://example.com/data.csv \"SELECT name FROM data\" -f json\n\n        \\b\n        # Use pandas backend for performance\n        $ sqlstream query data.parquet \"SELECT * FROM data\" --backend pandas\n\n        \\b\n        # Show query plan\n        $ sqlstream query data.csv \"SELECT * FROM data WHERE age &gt; 25\" --explain\n\n        \\b\n        # Save results to file\n        $ sqlstream query data.csv \"SELECT * FROM data\" -f csv -o results.csv\n    \"\"\"\n    fmt = format\n    del format\n    try:\n        start_time = time.time()\n\n        # Determine if we're using old syntax (file + sql) or new syntax (just sql)\n        if sql is None:\n            # New syntax: file_or_sql is the SQL query with inline file paths\n            sql_query = file_or_sql\n            file = None\n        else:\n            # Old syntax: file_or_sql is the file, sql is the SQL query\n            file = file_or_sql\n            sql_query = sql\n\n        # Create query\n        if file:\n            # Old syntax: single file provided\n            q = query_fn(file)\n        else:\n            # New syntax: sourceless query (file paths in SQL)\n            q = query_fn()\n\n        # Execute query or show explain plan\n        if explain:\n            result = q.sql(sql_query, backend=backend)\n            output_text = result.explain()\n            click.echo(output_text)\n        else:\n            # Execute query\n            result = q.sql(sql_query, backend=backend)\n            results_list = list(result)\n\n            # Auto-detect format from output file extension if not explicitly specified\n            output_format = fmt\n            if output and fmt == \"table\":\n                # User provided -o but not -f, infer format from extension\n                if output.endswith('.json'):\n                    output_format = \"json\"\n                elif output.endswith('.csv'):\n                    output_format = \"csv\"\n                elif output.endswith('.md'):\n                    output_format = \"markdown\"\n                # Otherwise keep as table format\n\n            # Apply display limit if specified (doesn't affect query LIMIT)\n            if limit is not None:\n                results_list = results_list[:limit]\n\n            # Check if interactive mode should be used\n            from sqlstream.cli.interactive import should_use_interactive, launch_interactive\n\n            if should_use_interactive(\n                results_list,\n                force=interactive,\n                no_interactive=no_interactive,\n                output_file=output,\n                fmt=output_format,\n            ):\n                # Launch interactive TUI\n                try:\n                    launch_interactive(results_list)\n                except ImportError as e:\n                    click.echo(f\"Error: {e}\", err=True)\n                    click.echo(\"Install with: pip install sqlstream[cli]\", err=True)\n                    sys.exit(1)\n            else:\n                # Use standard formatter\n                formatter = get_formatter(output_format)\n                output_text = formatter.format(\n                    results_list,\n                    no_color=no_color or (not sys.stdout.isatty()),\n                    show_footer=not output,\n                )\n\n                # Add execution time if requested\n                if show_time:\n                    elapsed = time.time() - start_time\n                    time_text = f\"\\nExecution time: {elapsed:.3f}s\"\n                    if output_format == \"table\" and not no_color:\n                        # Add colored time for table format\n                        try:\n                            from rich.console import Console\n\n                            console = Console()\n                            with console.capture() as capture:\n                                console.print(f\"[dim]{time_text}[/dim]\")\n                            output_text += capture.get()\n                        except ImportError:\n                            output_text += time_text\n                    else:\n                        output_text += time_text\n\n                # Write output\n                if output:\n                    with open(output, \"w\") as f:\n                        f.write(output_text)\n                    click.echo(f\"Results written to {output} ({output_format} format)\", err=True)\n                else:\n                    click.echo(output_text)\n\n    except FileNotFoundError as e:\n        click.echo(f\"Error: File not found - {e}\", err=True)\n        sys.exit(1)\n    except Exception as e:\n        click.echo(f\"Error: {e}\", err=True)\n        if \"--debug\" in sys.argv:\n            raise\n        sys.exit(1)\n</code></pre>"},{"location":"api/reference/cli/#shell","title":"shell","text":""},{"location":"api/reference/cli/#sqlstream.cli.main.shell","title":"shell","text":"<pre><code>shell(file: Optional[str], history_file: Optional[str])\n</code></pre> <p>Launch interactive SQL shell</p> <p>A full-featured SQL REPL with query editing, results viewing, schema browsing, and query history.</p> <p>Examples:</p> <pre><code>\b\n# Launch empty shell\n$ sqlstream shell\n\n\b\n# Launch with initial file loaded\n$ sqlstream shell employees.csv\n\n\b\n# Use custom history file\n$ sqlstream shell --history-file ~/.my_history\n</code></pre> Source code in <code>sqlstream/cli/main.py</code> <pre><code>@cli.command()\n@click.argument(\"file\", type=str, required=False)\n@click.option(\n    \"--history-file\",\n    type=click.Path(),\n    default=None,\n    help=\"Path to query history file (default: ~/.sqlstream_history)\",\n)\ndef shell(file: Optional[str], history_file: Optional[str]):\n    \"\"\"\n    Launch interactive SQL shell\n\n    A full-featured SQL REPL with query editing, results viewing,\n    schema browsing, and query history.\n\n    Examples:\n\n        \\b\n        # Launch empty shell\n        $ sqlstream shell\n\n        \\b\n        # Launch with initial file loaded\n        $ sqlstream shell employees.csv\n\n        \\b\n        # Use custom history file\n        $ sqlstream shell --history-file ~/.my_history\n    \"\"\"\n    try:\n        from sqlstream.cli.shell import launch_shell\n\n        launch_shell(initial_file=file, history_file=history_file)\n    except ImportError as e:\n        click.echo(\n            f\"Error: {e}\\n\"\n            \"Interactive shell requires textual library.\\n\"\n            \"Install with: pip install sqlstream[cli]\",\n            err=True,\n        )\n        sys.exit(1)\n</code></pre>"},{"location":"api/reference/cli/#configsidebar","title":"ConfigSidebar","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.ConfigSidebar","title":"ConfigSidebar","text":"<p>               Bases: <code>Container</code></p> <p>Sidebar tab for application configuration.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class ConfigSidebar(Container):\n    \"\"\"Sidebar tab for application configuration.\"\"\"\n\n    def compose(self) -&gt; ComposeResult:\n        with VerticalScroll(id=\"config-form\"):\n            yield Label(\"System Configuration\", classes=\"filter-label\")\n\n            # --- SECTION 1: BEHAVIOR (Functional) ---\n            with Vertical(classes=\"config-group\"):\n                yield Label(\"Behavior\", classes=\"config-label\")\n\n                # Confirm Exit\n                with Horizontal(classes=\"switch-row\"):\n                    yield Label(\"Confirm on Exit:\")\n                    yield Switch(value=False, id=\"cfg-confirm-exit\")\n\n                # History Limit\n                yield Label(\"History Size (Queries):\", classes=\"config-sublabel\")\n                yield Input(value=\"100\", type=\"integer\", id=\"cfg-history-size\")\n\n                # Default Export Format\n                yield Label(\"Default Export Format:\", classes=\"config-sublabel\")\n                yield Select(\n                    [(\"CSV\", \"csv\"), (\"JSON\", \"json\"), (\"Parquet\", \"parquet\")],\n                    value=\"csv\",\n                    id=\"cfg-export-fmt\",\n                    allow_blank=False\n                )\n\n            # --- SECTION 2: EXECUTION ---\n            with Vertical(classes=\"config-group\"):\n                yield Label(\"Execution\", classes=\"config-label\")\n\n                yield Label(\"Default Backend:\", classes=\"config-sublabel\")\n                yield Select(\n                    [(\"Auto\", \"auto\"), (\"DuckDB\", \"duckdb\"), (\"Pandas\", \"pandas\")],\n                    value=\"auto\",\n                    id=\"cfg-backend\",\n                    allow_blank=False\n                )\n\n                yield Label(\"Page Size (Rows):\", classes=\"config-sublabel\")\n                yield Input(value=\"100\", type=\"integer\", id=\"cfg-pagesize\")\n\n            # --- SECTION 3: APPEARANCE ---\n            with Vertical(classes=\"config-group\"):\n                yield Label(\"Appearance\", classes=\"config-label\")\n\n                yield Label(\"UI Theme:\", classes=\"config-sublabel\")\n                yield Select(APP_THEMES, id=\"cfg-app-theme\", allow_blank=False)\n\n                yield Label(\"SQL Syntax Theme:\", classes=\"config-sublabel\")\n                yield Select(TEXT_AREA_THEMES, id=\"cfg-editor-theme\", allow_blank=False)\n\n            # --- SECTION 4: DISPLAY SETTINGS ---\n            with Vertical(classes=\"config-group\"):\n                yield Label(\"Display Options\", classes=\"config-label\")\n\n                with Horizontal(classes=\"switch-row\"):\n                    yield Label(\"Line Numbers:\")\n                    yield Switch(value=True, id=\"cfg-linenums\")\n\n                with Horizontal(classes=\"switch-row\"):\n                    yield Label(\"Soft Wrap:\")\n                    yield Switch(value=False, id=\"cfg-softwrap\")\n\n                with Horizontal(classes=\"switch-row\"):\n                    yield Label(\"Zebra Stripes:\")\n                    yield Switch(value=True, id=\"cfg-zebra\")\n\n                with Horizontal(classes=\"switch-row\"):\n                    yield Label(\"Compact Results:\")\n                    yield Switch(value=False, id=\"cfg-compact\")\n\n            yield Button(\"Save &amp; Apply\", variant=\"primary\", id=\"config-save-btn\")\n\n    def on_mount(self) -&gt; None:\n        \"\"\"Load current values from App.\"\"\"\n        app = self.app\n\n        # Behavior\n        self.query_one(\"#cfg-confirm-exit\", Switch).value = app.confirm_exit\n        self.query_one(\"#cfg-history-size\", Input).value = str(app.max_history)\n        self.query_one(\"#cfg-export-fmt\", Select).value = app.default_export_fmt\n\n        # Execution\n        self.query_one(\"#cfg-backend\", Select).value = app.backend\n        self.query_one(\"#cfg-pagesize\", Input).value = str(app.page_size)\n\n        # Appearance\n        self.query_one(\"#cfg-app-theme\", Select).value = app.theme\n        self.query_one(\"#cfg-editor-theme\", Select).value = app.editor_theme\n\n        # Display\n        self.query_one(\"#cfg-linenums\", Switch).value = app.editor_linenums\n        self.query_one(\"#cfg-softwrap\", Switch).value = app.editor_soft_wrap\n        self.query_one(\"#cfg-zebra\", Switch).value = app.results_zebra\n        self.query_one(\"#cfg-compact\", Switch).value = app.results_compact\n\n    def on_button_pressed(self, event: Button.Pressed) -&gt; None:\n        if event.button.id == \"config-save-btn\":\n            self._save_settings()\n\n    def _save_settings(self) -&gt; None:\n        \"\"\"Apply settings to App and save to disk.\"\"\"\n        app = self.app\n\n        # 1. Gather Values\n        confirm_exit = self.query_one(\"#cfg-confirm-exit\", Switch).value\n        try:\n            history_size = int(self.query_one(\"#cfg-history-size\", Input).value)\n        except ValueError:\n            history_size = 100\n        export_fmt = self.query_one(\"#cfg-export-fmt\", Select).value\n\n        backend = self.query_one(\"#cfg-backend\", Select).value\n        try:\n            page_size = int(self.query_one(\"#cfg-pagesize\", Input).value)\n        except ValueError:\n            page_size = 100\n\n        app_theme = self.query_one(\"#cfg-app-theme\", Select).value\n        editor_theme = self.query_one(\"#cfg-editor-theme\", Select).value\n\n        linenums = self.query_one(\"#cfg-linenums\", Switch).value\n        softwrap = self.query_one(\"#cfg-softwrap\", Switch).value\n        zebra = self.query_one(\"#cfg-zebra\", Switch).value\n        compact = self.query_one(\"#cfg-compact\", Switch).value\n\n        # 2. Apply to App State\n        app.confirm_exit = confirm_exit\n        app.max_history = history_size\n        app.default_export_fmt = export_fmt\n\n        app.backend = backend\n        app.page_size = page_size\n        app.theme = app_theme\n\n        app.editor_theme = editor_theme\n        app.editor_linenums = linenums\n        app.editor_soft_wrap = softwrap\n\n        app.results_zebra = zebra\n        app.results_compact = compact\n\n        # 3. Apply to Active Widgets\n        for editor in app.query(QueryEditor):\n            editor.theme = editor_theme\n            editor.show_line_numbers = linenums\n            editor.soft_wrap = softwrap\n\n        results = app.query_one(ResultsViewer)\n        results.zebra_stripes = zebra\n        if compact:\n            results.add_class(\"compact\")\n        else:\n            results.remove_class(\"compact\")\n\n        if app.last_results:\n            app._refresh_displayed_results()\n\n        # 4. Persist\n        app.save_config_file()\n        app.notify(\"Configuration Saved &amp; Applied!\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.ConfigSidebar.on_mount","title":"on_mount","text":"<pre><code>on_mount() -&gt; None\n</code></pre> <p>Load current values from App.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def on_mount(self) -&gt; None:\n    \"\"\"Load current values from App.\"\"\"\n    app = self.app\n\n    # Behavior\n    self.query_one(\"#cfg-confirm-exit\", Switch).value = app.confirm_exit\n    self.query_one(\"#cfg-history-size\", Input).value = str(app.max_history)\n    self.query_one(\"#cfg-export-fmt\", Select).value = app.default_export_fmt\n\n    # Execution\n    self.query_one(\"#cfg-backend\", Select).value = app.backend\n    self.query_one(\"#cfg-pagesize\", Input).value = str(app.page_size)\n\n    # Appearance\n    self.query_one(\"#cfg-app-theme\", Select).value = app.theme\n    self.query_one(\"#cfg-editor-theme\", Select).value = app.editor_theme\n\n    # Display\n    self.query_one(\"#cfg-linenums\", Switch).value = app.editor_linenums\n    self.query_one(\"#cfg-softwrap\", Switch).value = app.editor_soft_wrap\n    self.query_one(\"#cfg-zebra\", Switch).value = app.results_zebra\n    self.query_one(\"#cfg-compact\", Switch).value = app.results_compact\n</code></pre>"},{"location":"api/reference/cli/#confirmexitdialog","title":"ConfirmExitDialog","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.ConfirmExitDialog","title":"ConfirmExitDialog","text":"<p>               Bases: <code>ModalScreen[bool]</code></p> <p>Modal to confirm application exit.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class ConfirmExitDialog(ModalScreen[bool]):\n    \"\"\"Modal to confirm application exit.\"\"\"\n\n    def compose(self) -&gt; ComposeResult:\n        with Container(classes=\"confirm-dialog\"):\n            yield Label(\"Confirm Exit\", classes=\"confirm-header\")\n            with Container(classes=\"confirm-body\"):\n                yield Label(\"Are you sure you want to quit?\")\n            with Horizontal(classes=\"confirm-footer\"):\n                yield Button(\"Cancel\", variant=\"default\", id=\"cancel-btn\")\n                yield Button(\"Exit\", variant=\"error\", id=\"exit-btn\")\n\n    def on_button_pressed(self, event: Button.Pressed) -&gt; None:\n        if event.button.id == \"exit-btn\":\n            self.dismiss(True)\n        else:\n            self.dismiss(False)\n</code></pre>"},{"location":"api/reference/cli/#explaindialog","title":"ExplainDialog","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.ExplainDialog","title":"ExplainDialog","text":"<p>               Bases: <code>ModalScreen</code></p> <p>Modal dialog for showing query execution plan.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class ExplainDialog(ModalScreen):\n    \"\"\"Modal dialog for showing query execution plan.\"\"\"\n\n    def __init__(self, plan: str, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self.plan = plan\n\n    def compose(self) -&gt; ComposeResult:\n        with Container(id=\"explain-dialog\"):\n            yield Label(\"Query Execution Plan\", id=\"explain-title\")\n            with VerticalScroll(id=\"explain-content\"):\n                yield Static(self.plan, id=\"explain-text\")\n            yield Button(\"Close\", variant=\"primary\", id=\"close-btn\")\n\n    def on_button_pressed(self, event: Button.Pressed) -&gt; None:\n        self.dismiss()\n</code></pre>"},{"location":"api/reference/cli/#exportsidebar","title":"ExportSidebar","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.ExportSidebar","title":"ExportSidebar","text":"<p>               Bases: <code>Container</code></p> <p>Right sidebar for exporting data.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class ExportSidebar(Container):\n    \"\"\"Right sidebar for exporting data.\"\"\"\n\n    def compose(self) -&gt; ComposeResult:\n        with Vertical(classes=\"sidebar-pane\"):\n            yield Label(\"Export Data\", classes=\"filter-label\")\n            yield Label(\"Ready to export 0 rows\", id=\"export-info\")\n\n            # 1. Format Selection\n            yield Label(\"Format:\")\n            format_options = [\n                (\"CSV\", \"csv\"),\n                (\"JSON\", \"json\"),\n                (\"Parquet\", \"parquet\")\n            ]\n            yield Select(format_options, allow_blank=False, value=\"csv\", id=\"ex-format\")\n\n            # 2. Directory Browser\n            yield Label(\"Save Location:\")\n            yield DirectoryTree(\"./\", id=\"export-tree\")\n\n            # 3. Filename Input\n            yield Label(\"Filename:\")\n            yield Input(placeholder=\"filename.csv\", id=\"ex-filename\")\n\n            # 4. Actions\n            with Horizontal(id=\"export-actions\"):\n                yield Button(\"Export\", variant=\"primary\", id=\"ex-btn\")\n\n    def on_mount(self) -&gt; None:\n        # Generate default filename\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        # Use configured default format\n        fmt = self.app.default_export_fmt\n        self.query_one(\"#ex-format\", Select).value = fmt\n        self.query_one(\"#ex-filename\").value = f\"results_{timestamp}.{fmt}\"\n\n    def update_info(self, row_count: int) -&gt; None:\n        \"\"\"Update the row count label.\"\"\"\n        self.query_one(\"#export-info\").update(f\"Ready to export {row_count:,} rows\")\n\n    def on_directory_tree_file_selected(self, event: DirectoryTree.FileSelected) -&gt; None:\n        \"\"\"When user clicks a file, adopt its name but keep our format extension.\"\"\"\n        selected_path = Path(event.path)\n\n        # Get current selected format\n        fmt = self.query_one(\"#ex-format\", Select).value\n\n        # Get the stem (filename without extension)\n        stem = selected_path.stem\n\n        # Set the input value\n        self.query_one(\"#ex-filename\").value = f\"{stem}.{fmt}\"\n\n    def on_select_changed(self, event: Select.Changed) -&gt; None:\n        \"\"\"Update extension if format changes.\"\"\"\n        if event.select.id == \"ex-format\":\n            inp = self.query_one(\"#ex-filename\", Input)\n            current = inp.value\n            if current and \".\" in current:\n                stem = current.rsplit(\".\", 1)[0]\n                inp.value = f\"{stem}.{event.value}\"\n\n    def on_button_pressed(self, event: Button.Pressed) -&gt; None:\n        if event.button.id == \"ex-btn\":\n            self._initiate_export()\n\n    def _initiate_export(self) -&gt; None:\n        \"\"\"Validate and trigger export process.\"\"\"\n        filename = self.query_one(\"#ex-filename\", Input).value\n        fmt = self.query_one(\"#ex-format\", Select).value\n\n        if not filename:\n            self.app.notify(\"Please enter a filename\", severity=\"error\")\n            return\n\n        # Determine directory:\n        # If a node is selected in tree, use its parent (if file) or itself (if dir)\n        # Fallback to current working directory of the tree\n        tree = self.query_one(\"#export-tree\", DirectoryTree)\n        cursor_node = tree.cursor_node\n\n        target_dir = Path(tree.path) # Default to root of tree\n\n        if cursor_node and cursor_node.data:\n            node_path = cursor_node.data.path\n            if node_path.is_dir():\n                target_dir = node_path\n            else:\n                target_dir = node_path.parent\n\n        full_path = target_dir / filename\n\n        # Check overwrite\n        if full_path.exists():\n            # PASS filename.name HERE\n            self.app.push_screen(\n                OverwriteConfirmDialog(filename),\n                lambda should_overwrite: self._finish_export(full_path, fmt) if should_overwrite else None\n            )\n        else:\n            self._finish_export(full_path, fmt)\n\n    def _finish_export(self, path: Path, fmt: str) -&gt; None:\n        \"\"\"Call the main app to perform the write.\"\"\"\n        self.app.perform_export(path, fmt)\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.ExportSidebar.update_info","title":"update_info","text":"<pre><code>update_info(row_count: int) -&gt; None\n</code></pre> <p>Update the row count label.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def update_info(self, row_count: int) -&gt; None:\n    \"\"\"Update the row count label.\"\"\"\n    self.query_one(\"#export-info\").update(f\"Ready to export {row_count:,} rows\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.ExportSidebar.on_directory_tree_file_selected","title":"on_directory_tree_file_selected","text":"<pre><code>on_directory_tree_file_selected(event: FileSelected) -&gt; None\n</code></pre> <p>When user clicks a file, adopt its name but keep our format extension.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def on_directory_tree_file_selected(self, event: DirectoryTree.FileSelected) -&gt; None:\n    \"\"\"When user clicks a file, adopt its name but keep our format extension.\"\"\"\n    selected_path = Path(event.path)\n\n    # Get current selected format\n    fmt = self.query_one(\"#ex-format\", Select).value\n\n    # Get the stem (filename without extension)\n    stem = selected_path.stem\n\n    # Set the input value\n    self.query_one(\"#ex-filename\").value = f\"{stem}.{fmt}\"\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.ExportSidebar.on_select_changed","title":"on_select_changed","text":"<pre><code>on_select_changed(event: Changed) -&gt; None\n</code></pre> <p>Update extension if format changes.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def on_select_changed(self, event: Select.Changed) -&gt; None:\n    \"\"\"Update extension if format changes.\"\"\"\n    if event.select.id == \"ex-format\":\n        inp = self.query_one(\"#ex-filename\", Input)\n        current = inp.value\n        if current and \".\" in current:\n            stem = current.rsplit(\".\", 1)[0]\n            inp.value = f\"{stem}.{event.value}\"\n</code></pre>"},{"location":"api/reference/cli/#filebrowser","title":"FileBrowser","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.FileBrowser","title":"FileBrowser","text":"<p>               Bases: <code>DirectoryTree</code></p> <p>Side panel for browsing files.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class FileBrowser(DirectoryTree):\n    \"\"\"Side panel for browsing files.\"\"\"\n\n    def __init__(self, path: str, **kwargs) -&gt; None:\n        super().__init__(path, **kwargs)\n        self.border_title = \"Files\"\n</code></pre>"},{"location":"api/reference/cli/#filtersidebar","title":"FilterSidebar","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.FilterSidebar","title":"FilterSidebar","text":"<p>               Bases: <code>Container</code></p> <p>Right sidebar for context-aware filtering.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class FilterSidebar(Container):\n    \"\"\"Right sidebar for context-aware filtering.\"\"\"\n\n    def compose(self) -&gt; ComposeResult:\n        yield Label(\"Filter Data\", classes=\"filter-label\")\n\n        # 1. Column Selector\n        yield Label(\"Column:\")\n        yield Select([], prompt=\"Select Column\", id=\"fs-column\")\n\n        # 2. Operator Selector (Populated dynamically)\n        yield Label(\"Operator:\")\n        yield Select([], prompt=\"Select Operator\", id=\"fs-operator\", disabled=True)\n\n        # 3. Value Inputs (Swapped dynamically)\n        yield Label(\"Value:\")\n        with Container(id=\"fs-input-container\"):\n            yield Input(placeholder=\"Select a column first...\", id=\"fs-value-1\", disabled=True)\n            # Secondary input for \"Between\" operations (hidden by default)\n            yield Input(placeholder=\"And...\", id=\"fs-value-2\", classes=\"hidden\")\n\n        # 4. Actions\n        with Horizontal(id=\"filter-actions\"):\n            yield Button(\"Clear\", variant=\"error\", id=\"fs-clear\")\n            yield Button(\"Apply\", variant=\"primary\", id=\"fs-apply\")\n\n    def update_columns(self, columns: List[str]) -&gt; None:\n        \"\"\"Called by App when results change.\"\"\"\n        select = self.query_one(\"#fs-column\", Select)\n        # Preserve \"Global Search\" as first option\n        options = [(\"Global Search (All)\", \"global\")] + [(c, c) for c in columns]\n        select.set_options(options)\n        select.value = \"global\"\n\n    def on_select_changed(self, event: Select.Changed) -&gt; None:\n        \"\"\"Handle column or operator changes.\"\"\"\n        if event.select.id == \"fs-column\":\n            self._on_column_changed(event.value)\n        elif event.select.id == \"fs-operator\":\n            self._on_operator_changed(event.value)\n\n    def _on_column_changed(self, column: str) -&gt; None:\n        \"\"\"Update operators based on column type.\"\"\"\n        op_select = self.query_one(\"#fs-operator\", Select)\n        val_input = self.query_one(\"#fs-value-1\", Input)\n        val_input_2 = self.query_one(\"#fs-value-2\", Input)\n\n        # Reset inputs\n        val_input.value = \"\"\n        val_input_2.value = \"\"\n        val_input_2.add_class(\"hidden\")\n        val_input.disabled = False\n        op_select.disabled = False\n\n        if column == \"global\":\n            op_select.set_options([(\"Contains\", \"contains\")])\n            op_select.value = \"contains\"\n            op_select.disabled = True\n            return\n\n        # Infer Data Type from App's last results\n        # We peek at the first row of data\n        col_type = str\n        if self.app.last_results:\n            first_val = self.app.last_results[0].get(column)\n            if isinstance(first_val, (int, float)):\n                col_type = float\n            elif isinstance(first_val, bool):\n                col_type = bool\n            # Simple date detection could go here if data is Python datetime objects\n\n        # Populate Operators based on Type\n        if col_type == float: # Numbers\n            ops = [\n                (\"Equals (=)\", \"eq\"),\n                (\"Greater Than (&gt;)\", \"gt\"),\n                (\"Less Than (&lt;)\", \"lt\"),\n                (\"Between (Range)\", \"between\")\n            ]\n            op_select.set_options(ops)\n            op_select.value = \"eq\"\n\n        elif col_type == bool: # Booleans\n            ops = [(\"Is\", \"is\")]\n            op_select.set_options(ops)\n            op_select.value = \"is\"\n\n        else: # Strings (Default)\n            ops = [\n                (\"Contains\", \"contains\"),\n                (\"Equals\", \"eq\"),\n                (\"Starts With\", \"startswith\"),\n                (\"Ends With\", \"endswith\"),\n                (\"Regex\", \"regex\")\n            ]\n            op_select.set_options(ops)\n            op_select.value = \"contains\"\n\n    def _on_operator_changed(self, operator: str) -&gt; None:\n        \"\"\"Show/Hide secondary input for 'between' operator.\"\"\"\n        val_2 = self.query_one(\"#fs-value-2\", Input)\n        if operator == \"between\":\n            val_2.remove_class(\"hidden\")\n        else:\n            val_2.add_class(\"hidden\")\n\n    def on_button_pressed(self, event: Button.Pressed) -&gt; None:\n        if event.button.id == \"fs-apply\":\n            self._trigger_filter()\n        elif event.button.id == \"fs-clear\":\n            self.app.action_clear_filter()\n\n    def on_input_submitted(self, event: Input.Submitted) -&gt; None:\n        self._trigger_filter()\n\n    def _trigger_filter(self) -&gt; None:\n        \"\"\"Gather values and tell App to filter.\"\"\"\n        col = self.query_one(\"#fs-column\", Select).value\n        op = self.query_one(\"#fs-operator\", Select).value\n        val1 = self.query_one(\"#fs-value-1\", Input).value\n        val2 = self.query_one(\"#fs-value-2\", Input).value\n\n        self.app.apply_advanced_filter(col, op, val1, val2)\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.FilterSidebar.update_columns","title":"update_columns","text":"<pre><code>update_columns(columns: List[str]) -&gt; None\n</code></pre> <p>Called by App when results change.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def update_columns(self, columns: List[str]) -&gt; None:\n    \"\"\"Called by App when results change.\"\"\"\n    select = self.query_one(\"#fs-column\", Select)\n    # Preserve \"Global Search\" as first option\n    options = [(\"Global Search (All)\", \"global\")] + [(c, c) for c in columns]\n    select.set_options(options)\n    select.value = \"global\"\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.FilterSidebar.on_select_changed","title":"on_select_changed","text":"<pre><code>on_select_changed(event: Changed) -&gt; None\n</code></pre> <p>Handle column or operator changes.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def on_select_changed(self, event: Select.Changed) -&gt; None:\n    \"\"\"Handle column or operator changes.\"\"\"\n    if event.select.id == \"fs-column\":\n        self._on_column_changed(event.value)\n    elif event.select.id == \"fs-operator\":\n        self._on_operator_changed(event.value)\n</code></pre>"},{"location":"api/reference/cli/#overwriteconfirmdialog","title":"OverwriteConfirmDialog","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.OverwriteConfirmDialog","title":"OverwriteConfirmDialog","text":"<p>               Bases: <code>ModalScreen[bool]</code></p> <p>Modal to confirm file overwrite.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class OverwriteConfirmDialog(ModalScreen[bool]):\n    \"\"\"Modal to confirm file overwrite.\"\"\"\n\n    def __init__(self, filename: str, **kwargs) -&gt; None:\n        super().__init__(**kwargs)\n        self.filename = filename\n\n    def compose(self) -&gt; ComposeResult:\n        with Container(classes=\"confirm-dialog\"):\n            yield Label(\"\u26a0\ufe0f File Already Exists\", classes=\"confirm-header\")\n\n            with Container(classes=\"confirm-body\"):\n                yield Label(\"The file below already exists:\")\n                yield Label(self.filename, classes=\"confirm-filename\")\n                yield Label(\"\\nDo you want to overwrite it?\")\n\n            with Horizontal(classes=\"confirm-footer\"):\n                yield Button(\"Cancel\", variant=\"default\", id=\"no-btn\")\n                yield Button(\"Overwrite\", variant=\"error\", id=\"yes-btn\")\n\n    def on_button_pressed(self, event: Button.Pressed) -&gt; None:\n        if event.button.id == \"yes-btn\":\n            self.dismiss(True)\n        else:\n            self.dismiss(False)\n</code></pre>"},{"location":"api/reference/cli/#queryeditor","title":"QueryEditor","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.QueryEditor","title":"QueryEditor","text":"<p>               Bases: <code>TextArea</code></p> <p>Multi-line SQL query editor with syntax highlighting and auto-completion.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class QueryEditor(TextArea):\n    \"\"\"Multi-line SQL query editor with syntax highlighting and auto-completion.\"\"\"\n\n    BINDINGS = [\n        Binding(\"ctrl+enter\", \"execute_query\", \"Execute\", priority=True),\n        Binding(\"ctrl+e\", \"execute_query\", \"Execute (Alt)\", priority=True),\n        Binding(\"ctrl+l\", \"clear_editor\", \"Clear\", priority=True),\n        Binding(\"ctrl+up\", \"history_prev\", \"Prev Query\", priority=True),\n        Binding(\"ctrl+down\", \"history_next\", \"Next Query\", priority=True),\n        Binding(\"ctrl+d\", \"app.quit\", \"Exit\", priority=True),\n        # Word deletion shortcuts\n        Binding(\"ctrl+delete\", \"delete_word_forward\", \"Delete Word \u2192\", show=False),\n        Binding(\"ctrl+backspace\", \"delete_word_backward\", \"Delete Word \u2190\", show=False),\n    ]\n\n    # SQL Keywords to suggest\n    KEYWORDS = [\n        \"SELECT\", \"FROM\", \"WHERE\", \"GROUP BY\", \"ORDER BY\", \"LIMIT\",\n        \"JOIN\", \"LEFT JOIN\", \"RIGHT JOIN\", \"INNER JOIN\", \"AND\", \"OR\",\n        \"NOT\", \"NULL\", \"IS\", \"IN\", \"VALUES\", \"INSERT\", \"UPDATE\",\n        \"DELETE\", \"CREATE\", \"TABLE\", \"DROP\", \"ALTER\", \"HAVING\", \"AS\"\n    ]\n\n    autocomplete_popup: SQLAutoComplete | None = None\n\n    def _get_current_word(self) -&gt; str:\n        \"\"\"Get the word under the cursor.\"\"\"\n        # Get current line text\n        line = self.document.get_line(self.cursor_location[0])\n        col = self.cursor_location[1]\n\n        # Find start of word\n        start = col\n        while start &gt; 0 and (line[start-1].isalnum() or line[start-1] == \"_\"):\n            start -= 1\n\n        return line[start:col]\n\n    def _get_schema_suggestions(self) -&gt; list[str]:\n        \"\"\"Get column names and table names from the app's schema.\"\"\"\n        suggestions = []\n        try:\n            # Access parent app's schema\n            if hasattr(self.app, 'schemas') and self.app.schemas:\n                for source_name, schema in self.app.schemas.items():\n                    # Add table/source name\n                    suggestions.append(source_name)\n                    # Add column names\n                    for col in schema.columns:\n                        suggestions.append(col.name)\n        except Exception:\n            pass  # Silently fail if schema not available\n        return suggestions\n\n    def _show_suggestions(self, word: str):\n        \"\"\"Show the autocomplete popup if matches found.\"\"\"\n        # Combine keywords with schema suggestions\n        all_suggestions = self.KEYWORDS + self._get_schema_suggestions()\n        matches = [s for s in all_suggestions if s.upper().startswith(word.upper())]\n\n        # Remove duplicates while preserving order\n        seen = set()\n        unique_matches = []\n        for m in matches:\n            if m.upper() not in seen:\n                seen.add(m.upper())\n                unique_matches.append(m)\n\n        # Remove existing popup if it exists\n        if self.autocomplete_popup:\n            self.autocomplete_popup.remove()\n            self.autocomplete_popup = None\n\n        if not unique_matches or not word:\n            return\n\n        # Create and mount the popup\n        self.autocomplete_popup = SQLAutoComplete(unique_matches[:10])  # Limit to 10 suggestions\n        self.screen.mount(self.autocomplete_popup)\n\n        # Position the popup near the cursor\n        x, y = self.cursor_screen_offset\n\n        # FIXED LINE: Use x, y directly as they are already screen coordinates\n        popup_offset = Offset(x, y + 1)\n\n        self.autocomplete_popup.styles.offset = (popup_offset.x, popup_offset.y)\n        self.autocomplete_popup.styles.width = 25  # Increased width for column names\n        self.autocomplete_popup.styles.height = min(len(unique_matches) + 2, 10)\n\n    def on_text_area_changed(self) -&gt; None:\n        \"\"\"Called when text changes.\"\"\"\n        word = self._get_current_word()\n        self._show_suggestions(word)\n\n    def on_key(self, event: Key) -&gt; None:\n        \"\"\"Handle key presses for selecting suggestions.\"\"\"\n        if self.autocomplete_popup:\n            if event.key == \"down\":\n                self.autocomplete_popup.action_cursor_down()\n                event.prevent_default()\n                return\n            elif event.key == \"up\":\n                self.autocomplete_popup.action_cursor_up()\n                event.prevent_default()\n                return\n            elif event.key in (\"enter\", \"tab\"):\n                # Complete the word\n                selected = self.autocomplete_popup.get_option_at_index(self.autocomplete_popup.highlighted).prompt\n                self._insert_completion(str(selected))\n                self._close_popup()\n                event.prevent_default()\n                return\n            elif event.key == \"escape\":\n                self._close_popup()\n                event.prevent_default()\n                return\n\n    def _insert_completion(self, completion: str):\n        \"\"\"Replace the current partial word with the completion.\"\"\"\n        current_word = self._get_current_word()\n        # Delete the partial word\n        self.delete(\n            start=(self.cursor_location[0], self.cursor_location[1] - len(current_word)),\n            end=self.cursor_location\n        )\n        # Insert the full keyword\n        self.insert(completion)\n\n    def _close_popup(self):\n        if self.autocomplete_popup:\n            self.autocomplete_popup.remove()\n            self.autocomplete_popup = None\n\n    def action_execute_query(self) -&gt; None:\n        \"\"\"Execute the current query.\"\"\"\n        self.post_message(self.ExecuteQuery(self.text))\n\n    def action_clear_editor(self) -&gt; None:\n        \"\"\"Clear the query editor.\"\"\"\n        self.clear()\n        self.focus()\n\n    def action_history_prev(self) -&gt; None:\n        \"\"\"Show previous query from history.\"\"\"\n        self.app.action_history_prev()\n\n    def action_history_next(self) -&gt; None:\n        \"\"\"Show next query from history.\"\"\"\n        self.app.action_history_next()\n\n    def action_delete_word_backward(self) -&gt; None:\n        \"\"\"Delete word to the left of cursor (Ctrl+Backspace).\"\"\"\n        row, col = self.cursor_location\n\n        if col == 0:\n            # At start of line, behave like normal backspace (join lines)\n            self.action_delete_left()\n            return\n\n        line = self.document.get_line(row)\n\n        # Scan backwards\n        i = col - 1\n\n        # 1. Consume whitespace if any\n        while i &gt;= 0 and line[i].isspace():\n            i -= 1\n\n        # 2. Consume word characters OR symbols (but not mixed)\n        if i &gt;= 0:\n            if line[i].isalnum() or line[i] == '_':\n                # Word characters\n                while i &gt;= 0 and (line[i].isalnum() or line[i] == '_'):\n                    i -= 1\n            else:\n                # Symbols\n                while i &gt;= 0 and not (line[i].isalnum() or line[i] == '_' or line[i].isspace()):\n                    i -= 1\n\n        target_col = i + 1\n        self.delete(start=(row, target_col), end=(row, col))\n\n    def action_delete_word_forward(self) -&gt; None:\n        \"\"\"Delete word to the right of cursor (Ctrl+Delete).\"\"\"\n        row, col = self.cursor_location\n        line = self.document.get_line(row)\n\n        if col &gt;= len(line):\n            # At end of line, behave like normal delete (join next line)\n            self.action_delete_right()\n            return\n\n        # Scan forwards\n        i = col\n\n        # 1. Consume whitespace if any\n        while i &lt; len(line) and line[i].isspace():\n            i += 1\n\n        # 2. Consume word characters OR symbols\n        if i &lt; len(line):\n            if line[i].isalnum() or line[i] == '_':\n                # Word characters\n                while i &lt; len(line) and (line[i].isalnum() or line[i] == '_'):\n                    i += 1\n            else:\n                # Symbols\n                while i &lt; len(line) and not (line[i].isalnum() or line[i] == '_' or line[i].isspace()):\n                    i += 1\n\n        target_col = i\n        self.delete(start=(row, col), end=(row, target_col))\n\n    class ExecuteQuery(TextArea.Changed):\n        \"\"\"Message sent when user wants to execute a query.\"\"\"\n\n        def __init__(self, query_text: str) -&gt; None:\n            super().__init__(query_text)\n            self.query_text = query_text\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.QueryEditor.ExecuteQuery","title":"ExecuteQuery","text":"<p>               Bases: <code>Changed</code></p> <p>Message sent when user wants to execute a query.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class ExecuteQuery(TextArea.Changed):\n    \"\"\"Message sent when user wants to execute a query.\"\"\"\n\n    def __init__(self, query_text: str) -&gt; None:\n        super().__init__(query_text)\n        self.query_text = query_text\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.QueryEditor.on_text_area_changed","title":"on_text_area_changed","text":"<pre><code>on_text_area_changed() -&gt; None\n</code></pre> <p>Called when text changes.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def on_text_area_changed(self) -&gt; None:\n    \"\"\"Called when text changes.\"\"\"\n    word = self._get_current_word()\n    self._show_suggestions(word)\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.QueryEditor.on_key","title":"on_key","text":"<pre><code>on_key(event: Key) -&gt; None\n</code></pre> <p>Handle key presses for selecting suggestions.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def on_key(self, event: Key) -&gt; None:\n    \"\"\"Handle key presses for selecting suggestions.\"\"\"\n    if self.autocomplete_popup:\n        if event.key == \"down\":\n            self.autocomplete_popup.action_cursor_down()\n            event.prevent_default()\n            return\n        elif event.key == \"up\":\n            self.autocomplete_popup.action_cursor_up()\n            event.prevent_default()\n            return\n        elif event.key in (\"enter\", \"tab\"):\n            # Complete the word\n            selected = self.autocomplete_popup.get_option_at_index(self.autocomplete_popup.highlighted).prompt\n            self._insert_completion(str(selected))\n            self._close_popup()\n            event.prevent_default()\n            return\n        elif event.key == \"escape\":\n            self._close_popup()\n            event.prevent_default()\n            return\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.QueryEditor.action_execute_query","title":"action_execute_query","text":"<pre><code>action_execute_query() -&gt; None\n</code></pre> <p>Execute the current query.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_execute_query(self) -&gt; None:\n    \"\"\"Execute the current query.\"\"\"\n    self.post_message(self.ExecuteQuery(self.text))\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.QueryEditor.action_clear_editor","title":"action_clear_editor","text":"<pre><code>action_clear_editor() -&gt; None\n</code></pre> <p>Clear the query editor.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_clear_editor(self) -&gt; None:\n    \"\"\"Clear the query editor.\"\"\"\n    self.clear()\n    self.focus()\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.QueryEditor.action_history_prev","title":"action_history_prev","text":"<pre><code>action_history_prev() -&gt; None\n</code></pre> <p>Show previous query from history.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_history_prev(self) -&gt; None:\n    \"\"\"Show previous query from history.\"\"\"\n    self.app.action_history_prev()\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.QueryEditor.action_history_next","title":"action_history_next","text":"<pre><code>action_history_next() -&gt; None\n</code></pre> <p>Show next query from history.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_history_next(self) -&gt; None:\n    \"\"\"Show next query from history.\"\"\"\n    self.app.action_history_next()\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.QueryEditor.action_delete_word_backward","title":"action_delete_word_backward","text":"<pre><code>action_delete_word_backward() -&gt; None\n</code></pre> <p>Delete word to the left of cursor (Ctrl+Backspace).</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_delete_word_backward(self) -&gt; None:\n    \"\"\"Delete word to the left of cursor (Ctrl+Backspace).\"\"\"\n    row, col = self.cursor_location\n\n    if col == 0:\n        # At start of line, behave like normal backspace (join lines)\n        self.action_delete_left()\n        return\n\n    line = self.document.get_line(row)\n\n    # Scan backwards\n    i = col - 1\n\n    # 1. Consume whitespace if any\n    while i &gt;= 0 and line[i].isspace():\n        i -= 1\n\n    # 2. Consume word characters OR symbols (but not mixed)\n    if i &gt;= 0:\n        if line[i].isalnum() or line[i] == '_':\n            # Word characters\n            while i &gt;= 0 and (line[i].isalnum() or line[i] == '_'):\n                i -= 1\n        else:\n            # Symbols\n            while i &gt;= 0 and not (line[i].isalnum() or line[i] == '_' or line[i].isspace()):\n                i -= 1\n\n    target_col = i + 1\n    self.delete(start=(row, target_col), end=(row, col))\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.QueryEditor.action_delete_word_forward","title":"action_delete_word_forward","text":"<pre><code>action_delete_word_forward() -&gt; None\n</code></pre> <p>Delete word to the right of cursor (Ctrl+Delete).</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_delete_word_forward(self) -&gt; None:\n    \"\"\"Delete word to the right of cursor (Ctrl+Delete).\"\"\"\n    row, col = self.cursor_location\n    line = self.document.get_line(row)\n\n    if col &gt;= len(line):\n        # At end of line, behave like normal delete (join next line)\n        self.action_delete_right()\n        return\n\n    # Scan forwards\n    i = col\n\n    # 1. Consume whitespace if any\n    while i &lt; len(line) and line[i].isspace():\n        i += 1\n\n    # 2. Consume word characters OR symbols\n    if i &lt; len(line):\n        if line[i].isalnum() or line[i] == '_':\n            # Word characters\n            while i &lt; len(line) and (line[i].isalnum() or line[i] == '_'):\n                i += 1\n        else:\n            # Symbols\n            while i &lt; len(line) and not (line[i].isalnum() or line[i] == '_' or line[i].isspace()):\n                i += 1\n\n    target_col = i\n    self.delete(start=(row, col), end=(row, target_col))\n</code></pre>"},{"location":"api/reference/cli/#resultsviewer","title":"ResultsViewer","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.ResultsViewer","title":"ResultsViewer","text":"<p>               Bases: <code>DataTable</code></p> <p>Interactive results viewer with scrolling.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class ResultsViewer(DataTable):\n    \"\"\"Interactive results viewer with scrolling.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        super().__init__(zebra_stripes=True, cursor_type=\"row\", **kwargs)\n        self.border_title = \"Results\"\n</code></pre>"},{"location":"api/reference/cli/#sqlautocomplete","title":"SQLAutoComplete","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLAutoComplete","title":"SQLAutoComplete","text":"<p>               Bases: <code>OptionList</code></p> <p>A popup widget that shows autocomplete suggestions.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class SQLAutoComplete(OptionList):\n    \"\"\"A popup widget that shows autocomplete suggestions.\"\"\"\n    def __init__(self, suggestions: list[str], **kwargs):\n        super().__init__(*suggestions, **kwargs)\n        self.add_class(\"autocomplete-popup\")\n</code></pre>"},{"location":"api/reference/cli/#sqlshellapp","title":"SQLShellApp","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp","title":"SQLShellApp","text":"<p>               Bases: <code>App</code></p> <p>SQLStream Interactive Shell Application.</p> <p>A full-featured SQL REPL with query editing, results viewing, schema browsing, and query history.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class SQLShellApp(App):\n    \"\"\"\n    SQLStream Interactive Shell Application.\n\n    A full-featured SQL REPL with query editing, results viewing,\n    schema browsing, and query history.\n    \"\"\"\n\n    CSS = \"\"\"\n    Screen {\n        background: $surface;\n    }\n\n    /* --- Main Layout --- */\n    #main-container {\n        width: 100%;\n        height: 100%;\n    }\n\n    /* Left Sidebar (Files/Schema) */\n    #sidebar-container {\n        width: 20%;\n        height: 100%;\n        border-right: solid $primary;\n        background: $panel;\n        display: none;\n    }\n\n    #sidebar-container.visible {\n        display: block;\n    }\n\n    /* Center Panel */\n    #center-panel {\n        width: 1fr;\n        height: 100%;\n        layout: vertical;\n    }\n\n    /* Right Sidebar (Tools) */\n    #tools-sidebar {\n        width: 20%;\n        height: 100%;\n        border-left: solid $primary;\n        background: $surface;\n        display: none;\n    }\n\n    #tools-sidebar.visible {\n        display: block;\n    }\n\n    /* --- Sidebar Internals --- */\n    #tools-tabs {\n        height: 100%;\n    }\n\n    .sidebar-pane {\n        padding: 1;\n        height: 100%;\n    }\n\n    /* Filter Styles */\n    .filter-label {\n        color: $accent;\n        text-style: bold;\n        margin-bottom: 1;\n        margin-top: 1;\n    }\n\n    #filter-actions, #export-actions {\n        height: auto;\n        margin-top: 2;\n        align: center middle;\n    }\n\n    #filter-actions Button, #export-actions Button {\n        width: 1fr;\n        margin: 0 1;\n    }\n\n    /* Export Styles */\n    #export-tree {\n        height: 1fr; /* Takes available space */\n        border: solid $primary;\n        margin-bottom: 1;\n        background: $panel;\n    }\n\n    #export-info {\n        text-align: center;\n        color: $accent;\n        margin-bottom: 1;\n    }\n\n    .confirm-dialog {\n        width: 50;\n        height: auto;\n        background: $surface;\n        border: thick $error;\n        padding: 0;\n    }\n\n    .confirm-header {\n        width: 100%;\n        background: $error;\n        color: white;\n        text-align: center;\n        text-style: bold;\n        padding: 1;\n    }\n\n    .confirm-body {\n        width: 100%;\n        padding: 2;\n        text-align: center;\n    }\n\n    .confirm-filename {\n        color: $accent;\n        text-style: bold;\n    }\n\n    .confirm-footer {\n        width: 100%;\n        padding: 1;\n        align: center middle;\n        background: $surface-darken-1;\n    }\n\n    .confirm-footer Button {\n        margin: 0 1;\n    }\n\n    #query-container {\n        border-bottom: solid $primary;\n        background: $surface;\n        height: 12; /* Default height */\n    }\n\n    #query-editor {\n        height: 100%;\n        border: none;\n        background: $surface;\n    }\n\n    #results-container {\n        height: 1fr;\n        background: $surface;\n    }\n\n    #results-viewer {\n        height: 100%;\n    }\n\n    #file-browser {\n        height: 1fr;\n        border: none;\n    }\n\n    #status-bar {\n        height: 1;\n        background: $primary;\n        color: $text;\n        content-align: center middle;\n    }\n\n    /* --- Filter Sidebar Specifics --- */\n    .filter-group {\n        margin-bottom: 2;\n        background: $panel;\n        padding: 1;\n        border: solid $primary;\n    }\n\n    .filter-label {\n        color: $accent;\n        text-style: bold;\n        margin-bottom: 1;\n    }\n\n    #filter-sidebar Select, #filter-sidebar Input {\n        margin-bottom: 1;\n    }\n\n    #filter-actions {\n        height: auto;\n        margin-top: 2;\n        align: center middle;\n    }\n\n    #filter-actions Button {\n        width: 1fr;\n        margin: 0 1;\n    }\n\n    /* --- Utility Classes --- */\n    .hidden { display: none !important; }\n    .maximized { height: 1fr !important; }\n\n    /* RESTORED: Status Bar Colors */\n    .error {\n        background: $error;\n        color: $text;\n    }\n\n    .success {\n        background: $success;\n        color: $text;\n    }\n\n    /* --- Dialog / Modal Styling (The Overhaul) --- */\n    ModalScreen {\n        align: center middle;\n        background: rgba(0,0,0,0.5); /* Dim background */\n    }\n\n    .dialog-container {\n        width: 60;\n        height: auto;\n        background: $surface;\n        border: thick $primary;\n        padding: 0;\n    }\n\n    .dialog-header {\n        width: 100%;\n        height: 3;\n        background: $primary;\n        color: $text;\n        content-align: center middle;\n        text-style: bold;\n        border-bottom: solid $surface-lighten-1;\n    }\n\n    .dialog-body {\n        width: 100%;\n        height: auto;\n        padding: 1 2;\n        layout: vertical;\n    }\n\n    .dialog-footer {\n        width: 100%;\n        height: auto;\n        padding: 1 2;\n        align: right middle;\n        background: $surface-darken-1;\n    }\n\n    .dialog-label {\n        margin-top: 1;\n        margin-bottom: 0;\n        color: $text-muted;\n    }\n\n    .dialog-info {\n        margin: 1 0;\n        color: $accent;\n        text-align: center;\n    }\n\n    /* Input/Select styling within dialogs */\n    .dialog-body Input, .dialog-body Select {\n        margin-bottom: 1;\n    }\n\n    /* Button Styling */\n    Button {\n        margin-left: 1;\n    }\n\n    .btn-primary {\n        background: $primary;\n        color: $text;\n    }\n\n    .btn-default {\n        background: $surface-lighten-1;\n    }\n\n    /* RESTORED: Explain Dialog Styles (Legacy ID support) */\n    #explain-dialog {\n        width: 80;\n        height: 30;\n        background: $surface;\n        border: thick $primary;\n    }\n\n    #explain-title {\n        text-style: bold;\n        text-align: center;\n        margin-bottom: 1;\n        background: $primary;\n        color: $text;\n        padding: 1;\n    }\n\n    #explain-content {\n        height: 1fr;\n        border: solid $accent;\n        margin-bottom: 1;\n        margin: 1;\n    }\n\n    #explain-text {\n        padding: 1;\n    }\n\n    /* --- Autocomplete Popup --- */\n    .autocomplete-popup {\n        layer: overlay;\n        background: $surface-lighten-1;\n        border: solid $accent;\n        display: block;\n        position: absolute;\n    }\n\n    /* --- Config Sidebar Styles --- */\n    #config-form {\n        height: 1fr;\n        padding: 1;\n        scrollbar-gutter: stable; /* Prevent layout shift */\n    }\n\n    .config-group {\n        height: auto;\n        margin-bottom: 2;\n        background: $panel;\n        padding: 1;\n        border: solid $primary;\n        layout: vertical; /* CRITICAL: Ensures children stack */\n    }\n\n    .config-label {\n        color: $accent;\n        text-style: bold;\n        margin-top: 1;\n        margin-bottom: 0; /* Tighten up label to input */\n    }\n\n    .config-sublabel {\n        color: $text;\n        height: 1;\n        margin-top: 1;\n    }\n\n    .config-description {\n        color: $text-muted;\n        text-style: italic;\n        margin-bottom: 1;\n        height: auto;\n    }\n\n    /* Ensure inputs/selects have space */\n    #config-form Select, #config-form Input {\n        margin-bottom: 1;\n        height: auto;\n    }\n\n    /* Switch rows */\n    .switch-row {\n        height: auto;\n        margin-bottom: 1;\n        align: left middle;\n    }\n\n    .switch-row Label {\n        width: 1fr;\n    }\n\n    #config-save-btn {\n        width: 100%;\n        margin-top: 1;\n        margin-bottom: 2;\n    }\n\n    /* --- Compact Mode for DataTable --- */\n    DataTable.compact .datatable--header {\n        height: 1;\n        padding: 0 1;\n    }\n\n    DataTable.compact .datatable--cursor {\n        background: $accent 20%;\n    }\n\n    /* Reduce padding in cells for compact mode */\n    DataTable.compact &gt; .datatable--header-hover,\n    DataTable.compact &gt; .datatable--header {\n        padding: 0 1;\n    }\n    \"\"\"\n\n    BINDINGS = [\n        Binding(\"f1\", \"show_help\", \"Help\"),\n        Binding(\"f2\", \"toggle_sidebar\", \"Sidebar\"),\n        Binding(\"f3\", \"toggle_history\", \"History\", show=False),\n        Binding(\"f4\", \"toggle_explain\", \"Explain\", show=False),\n        Binding(\"f5\", \"cycle_backend\", \"Backend\", show=False),\n\n        # --- NEW BINDINGS ---\n        Binding(\"f6\", \"cycle_layout\", \"Layout\"),\n        Binding(\"alt+up\", \"resize_query(-1)\", \"Shrink Edit\", show=False),\n        Binding(\"alt+down\", \"resize_query(1)\", \"Grow Edit\", show=False),\n        # --------------------\n\n        Binding(\"ctrl+b\", \"cycle_backend\", \"Backend\", show=True),\n        Binding(\"ctrl+o\", \"open_file\", \"Files\", priority=True),\n        Binding(\"ctrl+f\", \"toggle_tools('filter')\", \"Filter\", priority=True),\n        Binding(\"ctrl+x\", \"toggle_tools('export')\", \"Export\", priority=True),\n        Binding(\"ctrl+q\", \"quit\", \"Exit\", priority=True),\n        Binding(\"ctrl+d\", \"quit\", \"Exit\", priority=True),\n        Binding(\"ctrl+s\", \"save_state\", \"Save State\"),\n        Binding(\"ctrl+t\", \"new_tab\", \"New Tab\", priority=True),\n        Binding(\"ctrl+w\", \"close_tab\", \"Close Tab\", priority=True),\n        Binding(\"[\", \"prev_page\", \"\u25c0 Prev\", show=True, priority=True),\n        Binding(\"]\", \"next_page\", \"Next \u25b6\", show=True, priority=True),\n    ]\n\n    def __init__(\n        self,\n        initial_file: Optional[str] = None,\n        history_file: Optional[str] = None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.initial_file = initial_file\n        self.history_file = history_file or str(Path.home() / \".sqlstream_history\")\n        self.query_engine = Query()\n        self.backend = \"auto\"\n        self.query_history: List[str] = []\n        self.history_index = -1\n        self.last_results: List[Dict[str, Any]] = []\n        self.last_query = \"\"\n        self.loaded_files: List[str] = []\n        self.config_file = str(Path.home() / \".sqlstream_config\")\n\n        # Configuration Defaults\n        self.confirm_exit = False\n        self.max_history = 100\n        self.default_export_fmt = \"csv\"\n\n        self.editor_theme = \"dracula\"\n        self.editor_linenums = True\n        self.editor_soft_wrap = False\n        self.results_zebra = True\n        self.results_compact = False\n\n        # Pagination state\n        self.page_size = 100\n        self.current_page = 0\n\n        # Filter and sort state\n        self.filter_text = \"\"\n        self.filter_column = None\n        self.filter_mode = \"contains\"\n        self.filter_active = False\n        self.filtered_results: List[Dict[str, Any]] = []\n        self.sort_column = None\n        self.sort_reverse = False\n\n        self.state_file = str(Path.home() / \".sqlstream_state\")\n        self.tab_counter = 0\n\n        # --- Layout State ---\n        self.layout_mode = 0  # 0: Split, 1: Max Editor, 2: Max Results\n        self.query_height = 12 # Default height\n        # --------------------\n\n        if initial_file:\n            self.loaded_files.append(initial_file)\n\n    def compose(self) -&gt; ComposeResult:\n        yield Header(show_clock=True)\n\n        with Horizontal(id=\"main-container\"):\n            # Left Sidebar\n            with Container(id=\"sidebar-container\"):\n                with TabbedContent(id=\"sidebar-tabs\"):\n                    with TabPane(\"Schema\", id=\"tab-schema\"):\n                        yield SchemaBrowser(id=\"schema-browser\")\n                    with TabPane(\"Files\", id=\"tab-files\"):\n                        yield FileBrowser(\"./\", id=\"file-browser\")\n                    with TabPane(\"Config\", id=\"tab-config\"):\n                        yield ConfigSidebar(id=\"config-sidebar\")\n\n            # Center Panel\n            with Vertical(id=\"center-panel\"):\n                with Container(id=\"query-container\"):\n                    with TabbedContent(id=\"query-tabs\"):\n                        pass\n\n                with Container(id=\"results-container\"):\n                    yield ResultsViewer(id=\"results-viewer\")\n\n                yield StatusBar(id=\"status-bar\")\n\n            # Right Sidebar (Tools)\n            with Container(id=\"tools-sidebar\"):\n                with TabbedContent(id=\"tools-tabs\"):\n                    with TabPane(\"Filter\", id=\"tab-filter\"):\n                        yield FilterSidebar(id=\"filter-sidebar\")\n                    with TabPane(\"Export\", id=\"tab-export\"):\n                        yield ExportSidebar(id=\"export-sidebar\")\n\n        yield Footer()\n\n    async def on_mount(self) -&gt; None:\n        \"\"\"Initialize the shell on mount.\"\"\"\n        self.title = \"SQLStream Interactive Shell\"\n        self.sub_title = \"Ctrl+Enter: Run | F6: Layout | Alt+Up/Down: Resize\"\n\n        # Apply initial height\n        self.query_one(\"#query-container\").styles.height = self.query_height\n\n        # 1. Load Config\n        self.load_config_file()\n\n        # 2. Apply Config to Static Widgets (ResultsViewer)\n        results = self.query_one(ResultsViewer)\n        results.zebra_stripes = self.results_zebra\n        if self.results_compact:\n            results.add_class(\"compact\")\n\n        # 3. Load State\n        await self._load_state()\n\n        status_bar = self.query_one(StatusBar)\n        status_bar.update_status(\n            \"Welcome! Type SQL and press Ctrl+Enter. Use F6 to toggle layout.\"\n        )\n\n        self._get_active_editor().focus()\n\n        if self.initial_file:\n            self._load_initial_file()\n            self._update_schema_browser()\n\n    def action_cycle_layout(self) -&gt; None:\n        \"\"\"Cycle between Split, Max Editor, and Max Results views.\"\"\"\n        self.layout_mode = (self.layout_mode + 1) % 3\n\n        query_container = self.query_one(\"#query-container\")\n        results_container = self.query_one(\"#results-container\")\n\n        # Reset classes\n        query_container.remove_class(\"hidden\", \"maximized\")\n        results_container.remove_class(\"hidden\", \"maximized\")\n\n        if self.layout_mode == 0:\n            # Split View (Default)\n            # Restore the fixed height\n            query_container.styles.height = self.query_height\n            self._show_status(\"Layout: Split View\")\n\n        elif self.layout_mode == 1:\n            # Maximize Editor\n            # IMPORTANT: Clear the fixed height so the CSS '1fr' takes effect\n            query_container.styles.height = None\n            query_container.add_class(\"maximized\")\n            results_container.add_class(\"hidden\")\n            self._show_status(\"Layout: Editor Fullscreen\")\n\n        elif self.layout_mode == 2:\n            # Maximize Results\n            # Clear height here as well to ensure it hides cleanly\n            query_container.styles.height = None\n            query_container.add_class(\"hidden\")\n            results_container.add_class(\"maximized\")\n            self._show_status(\"Layout: Results Fullscreen\")\n\n        # Force a refresh of the active editor to handle resize\n        try:\n            self._get_active_editor().refresh()\n        except Exception:\n            pass\n\n    def action_resize_query(self, amount: int) -&gt; None:\n        \"\"\"Resize the query editor height (only in Split View).\"\"\"\n        if self.layout_mode != 0:\n            self._show_status(\"Cannot resize in fullscreen mode\", error=True)\n            return\n\n        # Update height\n        new_height = self.query_height + amount\n\n        # Enforce limits (min 3 lines, max 80% of screen approx)\n        if 3 &lt;= new_height &lt;= 50:\n            self.query_height = new_height\n            self.query_one(\"#query-container\").styles.height = self.query_height\n            self._show_status(f\"Editor Height: {self.query_height}\")\n\n    # --------------------------\n\n    # ... (Keep all other existing methods: on_data_table_header_selected, action_history_prev, etc.) ...\n    # ... (Copy the rest of the methods from your previous code here) ...\n\n    def on_data_table_header_selected(self, event: DataTable.HeaderSelected) -&gt; None:\n        \"\"\"Handle column header clicks for sorting.\"\"\"\n        # Get the column key - might be ColumnKey object\n        column_key = event.column_key\n        if hasattr(column_key, 'value'):\n            column_key = column_key.value\n        else:\n            column_key = str(column_key)\n\n        if not self.last_results:\n            return\n\n        # Toggle sort direction if clicking same column\n        if self.sort_column == column_key:\n            self.sort_reverse = not self.sort_reverse\n        else:\n            self.sort_column = column_key\n            self.sort_reverse = False\n\n        # Reapply filter and sort\n        self.filtered_results = self._apply_filter(self.last_results)\n        self.filtered_results = self._apply_sort(self.filtered_results)\n        self.current_page = 0\n        self._refresh_displayed_results()\n\n        direction = \"\u2193\" if self.sort_reverse else \"\u2191\"\n        self._show_status(f\"Sorted by {column_key} {direction}\")\n\n    def action_history_prev(self) -&gt; None:\n        \"\"\"Navigate to previous query in history.\"\"\"\n        if not self.query_history:\n            return\n\n        if self.history_index == -1:\n            # Start browsing from end\n            self.history_index = len(self.query_history) - 1\n        elif self.history_index &gt; 0:\n            self.history_index -= 1\n\n        self._set_editor_text(self.query_history[self.history_index])\n\n    def action_history_next(self) -&gt; None:\n        \"\"\"Navigate to next query in history.\"\"\"\n        if not self.query_history or self.history_index == -1:\n            return\n\n        if self.history_index &lt; len(self.query_history) - 1:\n            self.history_index += 1\n            self._set_editor_text(self.query_history[self.history_index])\n        else:\n            # Reset to empty/current\n            self.history_index = -1\n            self._set_editor_text(\"\")\n\n    def _get_active_editor(self) -&gt; QueryEditor:\n        \"\"\"Get the currently active query editor.\"\"\"\n        tabs = self.query_one(\"#query-tabs\", TabbedContent)\n        if not tabs.active:\n            # Fallback if no tabs exist yet or logic fails\n            try:\n                return self.query_one(QueryEditor)\n            except Exception:\n                pass\n        active_pane = self.query_one(f\"#{tabs.active}\", TabPane)\n        return active_pane.query_one(QueryEditor)\n\n    def _set_editor_text(self, text: str) -&gt; None:\n        \"\"\"Set text in active query editor.\"\"\"\n        editor = self._get_active_editor()\n        editor.text = text\n        if text:\n            editor.cursor_location = (len(text.splitlines()) - 1, len(text.splitlines()[-1]))\n        else:\n            editor.cursor_location = (0, 0)\n\n    def on_query_editor_execute_query(self, message: QueryEditor.ExecuteQuery) -&gt; None:\n        \"\"\"Handle query execution request.\"\"\"\n        query_text = message.query_text.strip()\n\n        if not query_text:\n            self._show_status(\"No query to execute\", error=True)\n            return\n\n        # Add to history if new\n        if not self.query_history or self.query_history[-1] != query_text:\n            self.query_history.append(query_text)\n            self._save_history()\n\n        # Reset history index\n        self.history_index = -1\n\n        # Execute query\n        self._execute_query(query_text)\n\n    def _execute_query(self, query_text: str) -&gt; None:\n        \"\"\"Execute a SQL query and display results.\"\"\"\n        status_bar = self.query_one(StatusBar)\n        results_viewer = self.query_one(ResultsViewer)\n\n        try:\n            # Clear previous results\n            results_viewer.clear(columns=True)\n\n            # Show loading status\n            status_bar.update_status(\"Executing query...\")\n\n            # Execute query\n            start_time = datetime.now()\n            result = self.query_engine.sql(query_text, backend=self.backend)\n\n            # Safe source discovery\n            try:\n                _sources = result._discover_sources()\n                self.loaded_files.extend([f for f in _sources.values() if f and f not in self.loaded_files])\n            except Exception:\n                pass\n\n            # Update schema browser\n            self._update_schema_browser()\n\n            # Get results\n            results = result.to_list()\n            execution_time = (datetime.now() - start_time).total_seconds()\n\n            # Store results and query\n            self.last_results = results\n            self.last_query = query_text  # Store for explain mode\n\n            # Display results\n            if results:\n                self._display_results(results, execution_time)\n            else:\n                results_viewer.clear(columns=True)\n                status_bar.update_status(\"Query executed successfully (no results)\", execution_time=execution_time, row_count=0)\n\n        except Exception as e:\n            self._show_error(str(e))\n\n    def _display_results(self, results: List[Dict[str, Any]], execution_time: float) -&gt; None:\n        \"\"\"Display query results in the DataTable with pagination.\"\"\"\n        # Get column names from first row\n        if not results:\n            return\n\n        # Apply filter if set\n        self.filtered_results = self._apply_filter(results)\n\n        # Apply sort if set\n        if self.sort_column:\n            self.filtered_results = self._apply_sort(self.filtered_results)\n\n        # Reset to first page\n        self.current_page = 0\n\n        # Display current page\n        self._refresh_displayed_results(execution_time)\n\n    def _apply_filter(self, results: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Apply filter text to results with specific modes.\"\"\"\n        if not self.filter_text:\n            return results\n\n        filtered = []\n        filter_lower = self.filter_text.lower()\n        mode = getattr(self, \"filter_mode\", \"contains\")\n\n        for row in results:\n            # Determine values to check\n            if self.filter_column:\n                values_to_check = [str(row.get(self.filter_column, \"\"))]\n            else:\n                values_to_check = [str(v) for v in row.values()]\n\n            match_found = False\n            for val in values_to_check:\n                val_str = val.lower()\n\n                if mode == \"exact\":\n                    if val_str == filter_lower:\n                        match_found = True\n                elif mode == \"startswith\":\n                    if val_str.startswith(filter_lower):\n                        match_found = True\n                elif mode == \"endswith\":\n                    if val_str.endswith(filter_lower):\n                        match_found = True\n                else: # contains\n                    if filter_lower in val_str:\n                        match_found = True\n\n                if match_found:\n                    break\n\n            if match_found:\n                filtered.append(row)\n\n        return filtered\n\n    def _apply_sort(self, results: List[Dict[str, Any]]) -&gt; List[Dict[str, Any]]:\n        \"\"\"Sort results by column.\"\"\"\n        if not self.sort_column or not results:\n            return results\n\n        try:\n            return sorted(\n                results,\n                key=lambda x: x.get(self.sort_column, \"\"),\n                reverse=self.sort_reverse\n            )\n        except Exception:\n            return results\n\n    def _format_value(self, value: Any) -&gt; str:\n        \"\"\"Format a value for display, handling scientific notation.\"\"\"\n        if value is None:\n            return \"NULL\"\n        elif isinstance(value, float):\n            # Format floats nicely - avoid scientific notation for small numbers\n            if abs(value) &lt; 1e-10 and value != 0:\n                return \"0.0\"\n            elif abs(value) &lt; 0.01 or abs(value) &gt; 1e6:\n                # Use scientific notation for very small or very large\n                return f\"{value:.6g}\"\n            else:\n                # Regular decimal notation\n                return f\"{value:.6f}\".rstrip('0').rstrip('.')\n        else:\n            return str(value)\n\n    def _prepare_value_for_export(self, value: Any) -&gt; Any:\n        \"\"\"Prepare a value for export, preserving proper data types.\"\"\"\n        if value is None:\n            return None\n        elif isinstance(value, float):\n            # Convert near-zero values to 0.0 to avoid scientific notation in exports\n            if abs(value) &lt; 1e-10 and value != 0:\n                return 0.0\n            return value\n        else:\n            return value\n\n    def _refresh_displayed_results(self, execution_time: Optional[float] = None) -&gt; None:\n        \"\"\"Refresh the displayed results with current page.\"\"\"\n        results_viewer = self.query_one(ResultsViewer)\n        status_bar = self.query_one(StatusBar)\n\n        # Clear existing\n        results_viewer.clear(columns=True)\n\n        if not self.filtered_results:\n            status_bar.update_status(\"No results to display\")\n            return\n\n        # Calculate pagination\n        total_rows = len(self.filtered_results)\n        start_idx = self.current_page * self.page_size\n        end_idx = min(start_idx + self.page_size, total_rows)\n        page_results = self.filtered_results[start_idx:end_idx]\n\n        columns = list(self.filtered_results[0].keys())\n\n        # Add columns\n        for col in columns:\n            results_viewer.add_column(col, key=col)\n\n        # Add rows (current page only)\n        for row in page_results:\n            values = [self._format_value(row.get(col)) for col in columns]\n            results_viewer.add_row(*values)\n\n        # Update status with pagination info\n        total_pages = (total_rows + self.page_size - 1) // self.page_size\n        page_info = f\"Page {self.current_page + 1}/{total_pages}\"\n\n        # FIX: Check filter_active flag\n        filter_info = f\" (filtered from {len(self.last_results)})\" if self.filter_active else \"\"\n\n        message = f\"Showing {start_idx + 1}-{end_idx} of {total_rows} rows{filter_info} | {page_info}\"\n        status_bar.update_status(message, execution_time=execution_time, row_count=total_rows)\n        status_bar.remove_class(\"error\")\n        status_bar.add_class(\"success\")\n\n\n    def _show_error(self, error_message: str) -&gt; None:\n        \"\"\"Show an error message.\"\"\"\n        status_bar = self.query_one(StatusBar)\n        status_bar.update_status(f\"Error: {error_message}\")\n        status_bar.remove_class(\"success\")\n        status_bar.add_class(\"error\")\n\n    def _show_status(self, message: str, error: bool = False) -&gt; None:\n        \"\"\"Show a status message.\"\"\"\n        status_bar = self.query_one(StatusBar)\n\n        # Construct filter info if active\n        filter_info = \"\"\n        # FIX: Check filter_active flag\n        if self.filter_active:\n            filter_mode = \"equals\" if self.filter_mode == \"exact\" else self.filter_mode\n            if self.filter_column:\n                filter_info = f\"`{self.filter_column}` {filter_mode} \"\n            filter_info += f\"'{self.filter_text}'\"\n\n        status_bar.update_status(message, filter_info=filter_info, backend_info=self.backend.upper())\n\n        if error:\n            status_bar.remove_class(\"success\")\n            status_bar.add_class(\"error\")\n        else:\n            status_bar.remove_class(\"error\")\n\n    def _load_initial_file(self) -&gt; None:\n        \"\"\"Load the initial file if provided.\"\"\"\n        if not self.initial_file:\n            return\n\n        try:\n            # Pre-populate query editor\n            editor = self._get_active_editor()\n            editor.text = f\"SELECT * FROM '{self.initial_file}' LIMIT 10\"\n\n            self._show_status(f\"Loaded {self.initial_file}. Press Ctrl+Enter to execute.\")\n        except Exception as e:\n            self._show_error(f\"Could not load {self.initial_file}: {e}\")\n\n    def _load_history(self) -&gt; None:\n        \"\"\"Load query history from file.\"\"\"\n        history_path = Path(self.history_file)\n        if history_path.exists():\n            try:\n                content = history_path.read_text()\n                # Use special delimiter to separate queries (supports multiline)\n                if content:\n                    self.query_history = content.split(\"\\n===\\n\")\n                    self.query_history = sorted(set(self.query_history), key=self.query_history.index, reverse=True)\n                else:\n                    self.query_history = []\n            except Exception:\n                pass  # Silently ignore history loading errors\n\n    def _save_history(self) -&gt; None:\n        try:\n            history_path = Path(self.history_file)\n            history_path.parent.mkdir(parents=True, exist_ok=True)\n            # Use configurable limit\n            history_to_save = self.query_history[-self.max_history:]\n            history_path.write_text(\"\\n===\\n\".join(history_to_save))\n        except Exception:\n            pass\n\n    def save_config_file(self) -&gt; None:\n        config = {\n            \"confirm_exit\": self.confirm_exit,\n            \"max_history\": self.max_history,\n            \"default_export_fmt\": self.default_export_fmt,\n            \"app_theme\": self.theme,\n            \"backend\": self.backend,\n            \"page_size\": self.page_size,\n            \"editor_theme\": self.editor_theme,\n            \"editor_linenums\": self.editor_linenums,\n            \"editor_soft_wrap\": self.editor_soft_wrap,\n            \"results_zebra\": self.results_zebra,\n            \"results_compact\": self.results_compact\n        }\n        try:\n            Path(self.config_file).write_text(json.dumps(config, indent=2))\n        except Exception as e:\n            self.notify(f\"Failed to save config: {e}\", severity=\"error\")\n\n    def load_config_file(self) -&gt; None:\n        try:\n            path = Path(self.config_file)\n            if not path.exists():\n                return\n\n            config = json.loads(path.read_text())\n\n            # Functional\n            self.confirm_exit = config.get(\"confirm_exit\", False)\n            self.max_history = int(config.get(\"max_history\", 100))\n            self.default_export_fmt = config.get(\"default_export_fmt\", \"csv\")\n\n            # Appearance &amp; Execution\n            if \"app_theme\" in config:\n                self.theme = config[\"app_theme\"]\n            self.backend = config.get(\"backend\", \"auto\")\n            self.page_size = int(config.get(\"page_size\", 100))\n            self.editor_theme = config.get(\"editor_theme\", \"dracula\")\n            self.editor_linenums = config.get(\"editor_linenums\", True)\n            self.editor_soft_wrap = config.get(\"editor_soft_wrap\", False)\n            self.results_zebra = config.get(\"results_zebra\", True)\n            self.results_compact = config.get(\"results_compact\", False)\n\n        except Exception as e:\n            self.notify(f\"Failed to load config: {e}\", severity=\"error\")\n\n    @work(thread=True)\n    def _update_schema_browser(self) -&gt; None:\n        \"\"\"Update the schema browser with loaded files.\"\"\"\n        schemas = {}\n        for file in self.loaded_files:\n            if not file:\n                continue\n            try:\n                # Use query() to get schema\n                q = query(file)\n                schemas[file] = q.schema().to_dict()\n            except Exception as e:\n                schemas[file] = {\"Error\": str(e)}\n\n        self.call_from_thread(self.query_one(SchemaBrowser).show_schemas, schemas)\n\n    def action_show_help(self) -&gt; None:\n        \"\"\"Show help dialog.\"\"\"\n        self._show_status(\"F6=Layout | Alt+Up/Down=Resize | Ctrl+Enter=Run | F2=Schema\")\n\n    def action_toggle_sidebar(self) -&gt; None:\n        \"\"\"Toggle sidebar panel.\"\"\"\n        container = self.query_one(\"#sidebar-container\")\n        if container.has_class(\"visible\"):\n            container.remove_class(\"visible\")\n            self._show_status(\"Sidebar hidden\")\n        else:\n            container.add_class(\"visible\")\n            self._show_status(\"Sidebar visible\")\n\n        # Force layout refresh to prevent text overflow\n        self.refresh(layout=True)\n        # Also refresh the active editor to reflow text\n        try:\n            editor = self._get_active_editor()\n            editor.refresh()\n        except Exception:\n            pass  # Editor might not be ready yet\n\n    def action_toggle_tools(self, tab: str = \"filter\") -&gt; None:\n        \"\"\"Toggle the right tools sidebar and select specific tab.\"\"\"\n        sidebar = self.query_one(\"#tools-sidebar\")\n        tabs = self.query_one(\"#tools-tabs\", TabbedContent)\n\n        # If sidebar is hidden, show it and select tab\n        if not sidebar.has_class(\"visible\"):\n            sidebar.add_class(\"visible\")\n            tabs.active = f\"tab-{tab}\"\n            self._refresh_tools_data()\n        else:\n            # If sidebar is visible...\n            if tabs.active == f\"tab-{tab}\":\n                # And we clicked the same key, hide it\n                sidebar.remove_class(\"visible\")\n            else:\n                # If we clicked a different key, just switch tab\n                tabs.active = f\"tab-{tab}\"\n                self._refresh_tools_data()\n\n    def _refresh_tools_data(self) -&gt; None:\n        \"\"\"Update filter columns and export counts based on current results.\"\"\"\n        if not self.last_results:\n            return\n\n        # Update Filter Sidebar\n        try:\n            cols = list(self.last_results[0].keys())\n            self.query_one(FilterSidebar).update_columns(cols)\n        except: pass\n\n        # Update Export Sidebar\n        try:\n            # FIX: Check filter_active flag\n            count = len(self.filtered_results) if self.filter_active else len(self.last_results)\n            self.query_one(ExportSidebar).update_info(count)\n        except: pass\n\n    # Renamed from action_export_results to perform_export (called by sidebar)\n    def perform_export(self, path: Path, fmt: str) -&gt; None:\n        \"\"\"Execute the file write.\"\"\"\n        if not self.last_results:\n            self._show_status(\"No results to export\", error=True)\n            return\n\n        # FIX: Check filter_active flag explicitly\n        results_to_export = self.filtered_results if self.filter_active else self.last_results\n\n        try:\n            row_count = len(results_to_export)\n            filename = str(path)\n\n            if fmt == 'csv':\n                import csv\n                with open(filename, \"w\", newline=\"\") as f:\n                    writer = csv.DictWriter(f, fieldnames=results_to_export[0].keys())\n                    writer.writeheader()\n                    export_rows = []\n                    for row in results_to_export:\n                        export_row = {k: self._prepare_value_for_export(v) for k, v in row.items()}\n                        export_rows.append(export_row)\n                    writer.writerows(export_rows)\n                self._show_status(f\"\u2713 Exported {row_count} rows to CSV: {filename}\")\n\n            elif fmt == 'json':\n                import json\n                export_rows = []\n                for row in results_to_export:\n                    export_row = {k: self._prepare_value_for_export(v) for k, v in row.items()}\n                    export_rows.append(export_row)\n                with open(filename, \"w\") as f:\n                    json.dump(export_rows, f, indent=2)\n                self._show_status(f\"\u2713 Exported {row_count} rows to JSON: {filename}\")\n\n            elif fmt == 'parquet':\n                try:\n                    import pyarrow as pa\n                    import pyarrow.parquet as pq\n                    table = pa.Table.from_pylist(results_to_export)\n                    pq.write_table(table, filename)\n                    self._show_status(f\"\u2713 Exported {row_count} rows to Parquet: {filename}\")\n                except ImportError:\n                    self._show_status(\"pyarrow not installed\", error=True)\n\n            # Close sidebar on success\n            self.query_one(\"#tools-sidebar\").remove_class(\"visible\")\n\n        except Exception as e:\n            self._show_status(f\"Export failed: {e}\", error=True)\n\n    def action_clear_filter(self) -&gt; None:\n        \"\"\"Clear active filters.\"\"\"\n        self.filter_active = False\n        self.filtered_results = self.last_results.copy()\n        self.current_page = 0\n        self._refresh_displayed_results()\n        self._show_status(\"Filter cleared\")\n\n    def action_toggle_history(self) -&gt; None:\n        \"\"\"Toggle query history panel.\"\"\"\n        self._show_status(\"Query history - Coming soon!\")\n\n    def action_cycle_backend(self) -&gt; None:\n        \"\"\"Cycle through available execution backends.\"\"\"\n        backends = [\"auto\", \"duckdb\", \"pandas\", \"python\"]\n        try:\n            current_idx = backends.index(self.backend)\n            next_idx = (current_idx + 1) % len(backends)\n        except ValueError:\n            next_idx = 0\n\n        self.backend = backends[next_idx]\n        self._show_status(f\"Switched backend to: {self.backend.upper()}\")\n\n    @work\n    async def action_toggle_explain(self) -&gt; None:\n        \"\"\"Toggle explain mode - shows query execution plan.\"\"\"\n        if not self.last_query:\n            self._show_status(\"Execute a query first to see explain plan\", error=True)\n            return\n\n        # Generate query plan\n        try:\n            parsed = parse(self.last_query)\n\n            # Build explain plan text\n            plan_lines = []\n            plan_lines.append(\"=\" * 60)\n            plan_lines.append(\"QUERY EXECUTION PLAN\")\n            plan_lines.append(\"=\" * 60)\n            plan_lines.append(\"\")\n            plan_lines.append(f\"Query: {self.last_query}\")\n            plan_lines.append(\"\")\n            plan_lines.append(\"--- PLAN STEPS ---\")\n            plan_lines.append(\"\")\n\n            step = 1\n            # Source scan\n            plan_lines.append(f\"{step}. TABLE SCAN\")\n            plan_lines.append(f\"   Source: {parsed.source}\")\n            step += 1\n\n            # JOIN if present\n            if parsed.join and parsed.join.right_source:\n                plan_lines.append(\"\")\n                plan_lines.append(f\"{step}. JOIN\")\n                plan_lines.append(f\"   Type: {parsed.join.join_type.upper()}\")\n                plan_lines.append(f\"   Right Source: {parsed.join.right_source}\")\n                plan_lines.append(f\"   Condition: {parsed.join.on_left} = {parsed.join.on_right}\")\n                step += 1\n\n            # WHERE clause\n            if parsed.where:\n                plan_lines.append(\"\")\n                plan_lines.append(f\"{step}. FILTER\")\n                plan_lines.append(f\"   Condition: {parsed.where}\")\n                step += 1\n\n            # GROUP BY\n            if parsed.group_by:\n                plan_lines.append(\"\")\n                plan_lines.append(f\"{step}. GROUP BY\")\n                plan_lines.append(f\"   Columns: {', '.join(parsed.group_by)}\")\n                step += 1\n\n            # ORDER BY\n            if parsed.order_by:\n                plan_lines.append(\"\")\n                plan_lines.append(f\"{step}. SORT\")\n                order_strs = []\n                for col, direction in parsed.order_by:\n                    order_strs.append(f\"{col} {'DESC' if direction else 'ASC'}\")\n                plan_lines.append(f\"   Order: {', '.join(order_strs)}\")\n                step += 1\n\n            # LIMIT\n            if parsed.limit is not None:\n                plan_lines.append(\"\")\n                plan_lines.append(f\"{step}. LIMIT\")\n                plan_lines.append(f\"   Rows: {parsed.limit}\")\n                step += 1\n\n            # Projection\n            plan_lines.append(\"\")\n            plan_lines.append(f\"{step}. PROJECTION\")\n            if parsed.columns:\n                plan_lines.append(f\"   Columns: {', '.join(parsed.columns)}\")\n            else:\n                plan_lines.append(\"   Columns: * (all)\")\n\n            plan_lines.append(\"\")\n            plan_lines.append(\"=\" * 60)\n            plan_lines.append(f\"Estimated rows returned: {len(self.last_results)}\")\n            plan_lines.append(\"=\" * 60)\n\n            plan_text = \"\\n\".join(plan_lines)\n\n            # Show explain dialog\n            await self.push_screen_wait(ExplainDialog(plan_text))\n\n        except Exception as e:\n            self._show_status(f\"Could not generate explain plan: {e}\", error=True)\n\n    def action_prev_page(self) -&gt; None:\n        \"\"\"Go to previous page of results.\"\"\"\n        if not self.filtered_results:\n            return\n\n        if self.current_page &gt; 0:\n            self.current_page -= 1\n            self._refresh_displayed_results()\n        else:\n            self._show_status(\"Already on first page\")\n\n    def action_next_page(self) -&gt; None:\n        \"\"\"Go to next page of results.\"\"\"\n        if not self.filtered_results:\n            return\n\n        total_pages = (len(self.filtered_results) + self.page_size - 1) // self.page_size\n        if self.current_page &lt; total_pages - 1:\n            self.current_page += 1\n            self._refresh_displayed_results()\n        else:\n            self._show_status(\"Already on last page\")\n\n    def apply_advanced_filter(self, col: str, op: str, val1: str, val2: str) -&gt; None:\n        \"\"\"Apply filter logic based on sidebar inputs.\"\"\"\n        if not self.last_results:\n            return\n\n        self.filtered_results = []\n\n        # Helper to safely cast types\n        def safe_cast(val, target_type):\n            try:\n                if target_type is float:\n                    return float(val)\n                if target_type is bool:\n                    return val.lower() == \"true\"\n                return str(val).lower()\n            except Exception:\n                return None\n\n        for row in self.last_results:\n            # 1. Determine Row Value\n            if col == \"global\":\n                # Global search is always string match\n                row_vals = [str(v).lower() for v in row.values()]\n                if any(val1.lower() in rv for rv in row_vals):\n                    self.filtered_results.append(row)\n                continue\n\n            # Specific Column Search\n            raw_val = row.get(col)\n\n            # Determine type based on the raw value in the row\n            target_type = str\n            if isinstance(raw_val, (int, float)):\n                target_type = float\n            elif isinstance(raw_val, bool):\n                target_type = bool\n\n            # Cast row value and input value\n            row_val = safe_cast(raw_val, target_type)\n            input_val = safe_cast(val1, target_type)\n\n            if row_val is None or input_val is None:\n                continue # Skip invalid data\n\n            match = False\n\n            # 2. Apply Operator Logic\n            if op == \"eq\":\n                match = row_val == input_val\n            elif op == \"contains\":\n                match = str(input_val) in str(row_val)\n            elif op == \"startswith\":\n                match = str(row_val).startswith(str(input_val))\n            elif op == \"endswith\":\n                match = str(row_val).endswith(str(input_val))\n            elif op == \"gt\":\n                match = row_val &gt; input_val\n            elif op == \"lt\":\n                match = row_val &lt; input_val\n            elif op == \"between\":\n                input_val_2 = safe_cast(val2, target_type)\n                if input_val_2 is not None:\n                    match = input_val &lt;= row_val &lt;= input_val_2\n            elif op == \"is\":\n                # For booleans, input_val is already cast to bool\n                match = row_val is input_val\n            elif op == \"regex\":\n                import re\n                try:\n                    if re.search(str(input_val), str(row_val), re.IGNORECASE):\n                        match = True\n                except Exception:\n                    pass\n\n            if match:\n                self.filtered_results.append(row)\n\n         # Update state so Export and Status Bar know a filter is active\n        self.filter_active = True\n        self.filter_column = col\n        self.filter_mode = op\n        self.filter_text = str(val1) # Store value for display/logic\n        # ------------------------------------------------\n\n        self.current_page = 0\n        self._refresh_displayed_results()\n        self._refresh_tools_data() # Update the export sidebar count immediately\n        self._show_status(f\"Filtered: {col} {op} {val1}\")\n\n\n    def action_open_file(self) -&gt; None:\n        \"\"\"Switch to file browser tab and show sidebar.\"\"\"\n        # Show sidebar if hidden\n        container = self.query_one(\"#sidebar-container\")\n        if not container.has_class(\"visible\"):\n            container.add_class(\"visible\")\n\n        # Switch to Files tab\n        self.query_one(\"#sidebar-tabs\", TabbedContent).active = \"tab-files\"\n\n        # Focus file browser\n        self.query_one(\"#file-browser\", FileBrowser).focus()\n        self._show_status(\"Select a file from the sidebar\")\n\n    def on_directory_tree_file_selected(self, event: DirectoryTree.FileSelected) -&gt; None:\n        \"\"\"Handle file selection from sidebar.\"\"\"\n        # FIX: Ignore events from the export tree to prevent overwriting the query\n        if event.control.id == \"export-tree\":\n            return\n\n        file_path = str(event.path)\n\n        # Get active editor\n        editor = self._get_active_editor()\n        current_text = editor.text.strip()\n\n        # Build query text with selected file\n        if not current_text:\n            # Empty editor - create simple SELECT query\n            new_text = f\"SELECT * FROM '{file_path}'\"\n        elif \"FROM\" in current_text.upper():\n            # Already has FROM clause - add the current text to history and replace with simple select statement\n            # Add to history if new\n            if not self.query_history or self.query_history[-1] != current_text:\n                self.query_history.append(current_text)\n                self._save_history()\n\n            new_text = f\"SELECT * FROM '{file_path}'\"\n            self._show_status(\"Last query stored in history\")\n        else:\n            # Append FROM clause\n            new_text = f\"{current_text}\\nFROM '{file_path}'\"\n\n        # Update editor\n        editor.text = new_text\n        # Move cursor to end\n        lines = new_text.splitlines()\n        editor.cursor_location = (len(lines) - 1, len(lines[-1]))\n\n        # Focus editor\n        editor.focus()\n\n        self._show_status(f\"Added file to query: {file_path}\")\n\n\n    async def action_new_tab(self, content: str = \"\", title: str = None) -&gt; None:\n        \"\"\"Create a new query tab.\"\"\"\n        self.tab_counter += 1\n        if not title:\n            title = f\"Query {self.tab_counter}\"\n\n        tab_id = f\"tab-query-{self.tab_counter}\"\n        editor_id = f\"query-editor-{self.tab_counter}\"\n\n        pane = TabPane(title, id=tab_id)\n        editor = QueryEditor(\n            id=editor_id,\n            language=\"sql\",\n            theme=self.editor_theme,\n            show_line_numbers=self.editor_linenums,\n            soft_wrap=self.editor_soft_wrap,\n            text=content\n        )\n\n        tabs = self.query_one(\"#query-tabs\", TabbedContent)\n        await tabs.add_pane(pane)\n        await pane.mount(editor)\n        tabs.active = tab_id\n        editor.focus()\n\n    async def action_close_tab(self) -&gt; None:\n        \"\"\"Close the current query tab.\"\"\"\n        tabs = self.query_one(\"#query-tabs\", TabbedContent)\n        active_tab = tabs.active\n        if not active_tab:\n            return\n\n        await tabs.remove_pane(active_tab)\n\n        # If no tabs left, create a new one\n        if not tabs.query(TabPane):\n             await self.action_new_tab()\n\n    def action_quit(self) -&gt; None:\n        \"\"\"Save state and exit, optionally confirming.\"\"\"\n        if self.confirm_exit:\n            self.push_screen(ConfirmExitDialog(), self._finish_quit)\n        else:\n            self._finish_quit(True)\n\n    def _finish_quit(self, should_quit: bool) -&gt; None:\n        if should_quit:\n            self._save_state()\n            self.exit()\n\n    def action_save_state(self) -&gt; None:\n        \"\"\"Manual save state action.\"\"\"\n        self._save_state()\n        self._show_status(\"State saved!\")\n\n    def _save_state(self) -&gt; None:\n        \"\"\"Save editor state to file.\"\"\"\n        try:\n            tabs = self.query_one(\"#query-tabs\", TabbedContent)\n            state = []\n\n            # Strategy 1: ContentSwitcher children\n            try:\n                switcher = tabs.query_one(ContentSwitcher)\n\n                for child in switcher.children:\n                    if isinstance(child, TabPane):\n                        editors = list(child.query(QueryEditor))\n\n                        if editors:\n                            editor = editors[0]\n                            state.append({\n                                \"title\": str(child._title),\n                                \"content\": editor.text\n                            })\n            except Exception:\n                pass\n\n            # Strategy 2: Direct query if Strategy 1 found nothing\n            if not state:\n                for pane in tabs.query(TabPane):\n                    editors = list(pane.query(QueryEditor))\n                    if editors:\n                        state.append({\n                            \"title\": str(pane._title),\n                            \"content\": editors[0].text\n                        })\n\n            # Write to file\n            path = Path(self.state_file)\n            path.write_text(json.dumps(state))\n            self.notify(f\"Saved {len(state)} tabs\", timeout=3)\n\n        except Exception as e:\n            self.notify(f\"Failed to save state: {e}\", severity=\"error\")\n\n    async def _load_state(self) -&gt; None:\n        \"\"\"Load editor state from file.\"\"\"\n        # Load history first\n        self._load_history()\n\n        state_path = Path(self.state_file)\n        loaded = False\n\n        if state_path.exists():\n            try:\n                state = json.loads(state_path.read_text())\n                if state and isinstance(state, list):\n                    for tab_data in state:\n                        await self.action_new_tab(\n                            content=tab_data.get(\"content\", \"\"),\n                            title=tab_data.get(\"title\")\n                        )\n                    self.notify(f\"Loaded {len(state)} tabs\", timeout=3)\n                    loaded = True\n            except Exception as e:\n                self.notify(f\"Failed to load state: {e}\", severity=\"error\")\n\n        if not loaded:\n            # Create default tab\n            await self.action_new_tab()\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.on_mount","title":"on_mount  <code>async</code>","text":"<pre><code>on_mount() -&gt; None\n</code></pre> <p>Initialize the shell on mount.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>async def on_mount(self) -&gt; None:\n    \"\"\"Initialize the shell on mount.\"\"\"\n    self.title = \"SQLStream Interactive Shell\"\n    self.sub_title = \"Ctrl+Enter: Run | F6: Layout | Alt+Up/Down: Resize\"\n\n    # Apply initial height\n    self.query_one(\"#query-container\").styles.height = self.query_height\n\n    # 1. Load Config\n    self.load_config_file()\n\n    # 2. Apply Config to Static Widgets (ResultsViewer)\n    results = self.query_one(ResultsViewer)\n    results.zebra_stripes = self.results_zebra\n    if self.results_compact:\n        results.add_class(\"compact\")\n\n    # 3. Load State\n    await self._load_state()\n\n    status_bar = self.query_one(StatusBar)\n    status_bar.update_status(\n        \"Welcome! Type SQL and press Ctrl+Enter. Use F6 to toggle layout.\"\n    )\n\n    self._get_active_editor().focus()\n\n    if self.initial_file:\n        self._load_initial_file()\n        self._update_schema_browser()\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_cycle_layout","title":"action_cycle_layout","text":"<pre><code>action_cycle_layout() -&gt; None\n</code></pre> <p>Cycle between Split, Max Editor, and Max Results views.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_cycle_layout(self) -&gt; None:\n    \"\"\"Cycle between Split, Max Editor, and Max Results views.\"\"\"\n    self.layout_mode = (self.layout_mode + 1) % 3\n\n    query_container = self.query_one(\"#query-container\")\n    results_container = self.query_one(\"#results-container\")\n\n    # Reset classes\n    query_container.remove_class(\"hidden\", \"maximized\")\n    results_container.remove_class(\"hidden\", \"maximized\")\n\n    if self.layout_mode == 0:\n        # Split View (Default)\n        # Restore the fixed height\n        query_container.styles.height = self.query_height\n        self._show_status(\"Layout: Split View\")\n\n    elif self.layout_mode == 1:\n        # Maximize Editor\n        # IMPORTANT: Clear the fixed height so the CSS '1fr' takes effect\n        query_container.styles.height = None\n        query_container.add_class(\"maximized\")\n        results_container.add_class(\"hidden\")\n        self._show_status(\"Layout: Editor Fullscreen\")\n\n    elif self.layout_mode == 2:\n        # Maximize Results\n        # Clear height here as well to ensure it hides cleanly\n        query_container.styles.height = None\n        query_container.add_class(\"hidden\")\n        results_container.add_class(\"maximized\")\n        self._show_status(\"Layout: Results Fullscreen\")\n\n    # Force a refresh of the active editor to handle resize\n    try:\n        self._get_active_editor().refresh()\n    except Exception:\n        pass\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_resize_query","title":"action_resize_query","text":"<pre><code>action_resize_query(amount: int) -&gt; None\n</code></pre> <p>Resize the query editor height (only in Split View).</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_resize_query(self, amount: int) -&gt; None:\n    \"\"\"Resize the query editor height (only in Split View).\"\"\"\n    if self.layout_mode != 0:\n        self._show_status(\"Cannot resize in fullscreen mode\", error=True)\n        return\n\n    # Update height\n    new_height = self.query_height + amount\n\n    # Enforce limits (min 3 lines, max 80% of screen approx)\n    if 3 &lt;= new_height &lt;= 50:\n        self.query_height = new_height\n        self.query_one(\"#query-container\").styles.height = self.query_height\n        self._show_status(f\"Editor Height: {self.query_height}\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.on_data_table_header_selected","title":"on_data_table_header_selected","text":"<pre><code>on_data_table_header_selected(event: HeaderSelected) -&gt; None\n</code></pre> <p>Handle column header clicks for sorting.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def on_data_table_header_selected(self, event: DataTable.HeaderSelected) -&gt; None:\n    \"\"\"Handle column header clicks for sorting.\"\"\"\n    # Get the column key - might be ColumnKey object\n    column_key = event.column_key\n    if hasattr(column_key, 'value'):\n        column_key = column_key.value\n    else:\n        column_key = str(column_key)\n\n    if not self.last_results:\n        return\n\n    # Toggle sort direction if clicking same column\n    if self.sort_column == column_key:\n        self.sort_reverse = not self.sort_reverse\n    else:\n        self.sort_column = column_key\n        self.sort_reverse = False\n\n    # Reapply filter and sort\n    self.filtered_results = self._apply_filter(self.last_results)\n    self.filtered_results = self._apply_sort(self.filtered_results)\n    self.current_page = 0\n    self._refresh_displayed_results()\n\n    direction = \"\u2193\" if self.sort_reverse else \"\u2191\"\n    self._show_status(f\"Sorted by {column_key} {direction}\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_history_prev","title":"action_history_prev","text":"<pre><code>action_history_prev() -&gt; None\n</code></pre> <p>Navigate to previous query in history.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_history_prev(self) -&gt; None:\n    \"\"\"Navigate to previous query in history.\"\"\"\n    if not self.query_history:\n        return\n\n    if self.history_index == -1:\n        # Start browsing from end\n        self.history_index = len(self.query_history) - 1\n    elif self.history_index &gt; 0:\n        self.history_index -= 1\n\n    self._set_editor_text(self.query_history[self.history_index])\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_history_next","title":"action_history_next","text":"<pre><code>action_history_next() -&gt; None\n</code></pre> <p>Navigate to next query in history.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_history_next(self) -&gt; None:\n    \"\"\"Navigate to next query in history.\"\"\"\n    if not self.query_history or self.history_index == -1:\n        return\n\n    if self.history_index &lt; len(self.query_history) - 1:\n        self.history_index += 1\n        self._set_editor_text(self.query_history[self.history_index])\n    else:\n        # Reset to empty/current\n        self.history_index = -1\n        self._set_editor_text(\"\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.on_query_editor_execute_query","title":"on_query_editor_execute_query","text":"<pre><code>on_query_editor_execute_query(message: ExecuteQuery) -&gt; None\n</code></pre> <p>Handle query execution request.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def on_query_editor_execute_query(self, message: QueryEditor.ExecuteQuery) -&gt; None:\n    \"\"\"Handle query execution request.\"\"\"\n    query_text = message.query_text.strip()\n\n    if not query_text:\n        self._show_status(\"No query to execute\", error=True)\n        return\n\n    # Add to history if new\n    if not self.query_history or self.query_history[-1] != query_text:\n        self.query_history.append(query_text)\n        self._save_history()\n\n    # Reset history index\n    self.history_index = -1\n\n    # Execute query\n    self._execute_query(query_text)\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_show_help","title":"action_show_help","text":"<pre><code>action_show_help() -&gt; None\n</code></pre> <p>Show help dialog.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_show_help(self) -&gt; None:\n    \"\"\"Show help dialog.\"\"\"\n    self._show_status(\"F6=Layout | Alt+Up/Down=Resize | Ctrl+Enter=Run | F2=Schema\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_toggle_sidebar","title":"action_toggle_sidebar","text":"<pre><code>action_toggle_sidebar() -&gt; None\n</code></pre> <p>Toggle sidebar panel.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_toggle_sidebar(self) -&gt; None:\n    \"\"\"Toggle sidebar panel.\"\"\"\n    container = self.query_one(\"#sidebar-container\")\n    if container.has_class(\"visible\"):\n        container.remove_class(\"visible\")\n        self._show_status(\"Sidebar hidden\")\n    else:\n        container.add_class(\"visible\")\n        self._show_status(\"Sidebar visible\")\n\n    # Force layout refresh to prevent text overflow\n    self.refresh(layout=True)\n    # Also refresh the active editor to reflow text\n    try:\n        editor = self._get_active_editor()\n        editor.refresh()\n    except Exception:\n        pass  # Editor might not be ready yet\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_toggle_tools","title":"action_toggle_tools","text":"<pre><code>action_toggle_tools(tab: str = 'filter') -&gt; None\n</code></pre> <p>Toggle the right tools sidebar and select specific tab.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_toggle_tools(self, tab: str = \"filter\") -&gt; None:\n    \"\"\"Toggle the right tools sidebar and select specific tab.\"\"\"\n    sidebar = self.query_one(\"#tools-sidebar\")\n    tabs = self.query_one(\"#tools-tabs\", TabbedContent)\n\n    # If sidebar is hidden, show it and select tab\n    if not sidebar.has_class(\"visible\"):\n        sidebar.add_class(\"visible\")\n        tabs.active = f\"tab-{tab}\"\n        self._refresh_tools_data()\n    else:\n        # If sidebar is visible...\n        if tabs.active == f\"tab-{tab}\":\n            # And we clicked the same key, hide it\n            sidebar.remove_class(\"visible\")\n        else:\n            # If we clicked a different key, just switch tab\n            tabs.active = f\"tab-{tab}\"\n            self._refresh_tools_data()\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.perform_export","title":"perform_export","text":"<pre><code>perform_export(path: Path, fmt: str) -&gt; None\n</code></pre> <p>Execute the file write.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def perform_export(self, path: Path, fmt: str) -&gt; None:\n    \"\"\"Execute the file write.\"\"\"\n    if not self.last_results:\n        self._show_status(\"No results to export\", error=True)\n        return\n\n    # FIX: Check filter_active flag explicitly\n    results_to_export = self.filtered_results if self.filter_active else self.last_results\n\n    try:\n        row_count = len(results_to_export)\n        filename = str(path)\n\n        if fmt == 'csv':\n            import csv\n            with open(filename, \"w\", newline=\"\") as f:\n                writer = csv.DictWriter(f, fieldnames=results_to_export[0].keys())\n                writer.writeheader()\n                export_rows = []\n                for row in results_to_export:\n                    export_row = {k: self._prepare_value_for_export(v) for k, v in row.items()}\n                    export_rows.append(export_row)\n                writer.writerows(export_rows)\n            self._show_status(f\"\u2713 Exported {row_count} rows to CSV: {filename}\")\n\n        elif fmt == 'json':\n            import json\n            export_rows = []\n            for row in results_to_export:\n                export_row = {k: self._prepare_value_for_export(v) for k, v in row.items()}\n                export_rows.append(export_row)\n            with open(filename, \"w\") as f:\n                json.dump(export_rows, f, indent=2)\n            self._show_status(f\"\u2713 Exported {row_count} rows to JSON: {filename}\")\n\n        elif fmt == 'parquet':\n            try:\n                import pyarrow as pa\n                import pyarrow.parquet as pq\n                table = pa.Table.from_pylist(results_to_export)\n                pq.write_table(table, filename)\n                self._show_status(f\"\u2713 Exported {row_count} rows to Parquet: {filename}\")\n            except ImportError:\n                self._show_status(\"pyarrow not installed\", error=True)\n\n        # Close sidebar on success\n        self.query_one(\"#tools-sidebar\").remove_class(\"visible\")\n\n    except Exception as e:\n        self._show_status(f\"Export failed: {e}\", error=True)\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_clear_filter","title":"action_clear_filter","text":"<pre><code>action_clear_filter() -&gt; None\n</code></pre> <p>Clear active filters.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_clear_filter(self) -&gt; None:\n    \"\"\"Clear active filters.\"\"\"\n    self.filter_active = False\n    self.filtered_results = self.last_results.copy()\n    self.current_page = 0\n    self._refresh_displayed_results()\n    self._show_status(\"Filter cleared\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_toggle_history","title":"action_toggle_history","text":"<pre><code>action_toggle_history() -&gt; None\n</code></pre> <p>Toggle query history panel.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_toggle_history(self) -&gt; None:\n    \"\"\"Toggle query history panel.\"\"\"\n    self._show_status(\"Query history - Coming soon!\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_cycle_backend","title":"action_cycle_backend","text":"<pre><code>action_cycle_backend() -&gt; None\n</code></pre> <p>Cycle through available execution backends.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_cycle_backend(self) -&gt; None:\n    \"\"\"Cycle through available execution backends.\"\"\"\n    backends = [\"auto\", \"duckdb\", \"pandas\", \"python\"]\n    try:\n        current_idx = backends.index(self.backend)\n        next_idx = (current_idx + 1) % len(backends)\n    except ValueError:\n        next_idx = 0\n\n    self.backend = backends[next_idx]\n    self._show_status(f\"Switched backend to: {self.backend.upper()}\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_toggle_explain","title":"action_toggle_explain  <code>async</code>","text":"<pre><code>action_toggle_explain() -&gt; None\n</code></pre> <p>Toggle explain mode - shows query execution plan.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>@work\nasync def action_toggle_explain(self) -&gt; None:\n    \"\"\"Toggle explain mode - shows query execution plan.\"\"\"\n    if not self.last_query:\n        self._show_status(\"Execute a query first to see explain plan\", error=True)\n        return\n\n    # Generate query plan\n    try:\n        parsed = parse(self.last_query)\n\n        # Build explain plan text\n        plan_lines = []\n        plan_lines.append(\"=\" * 60)\n        plan_lines.append(\"QUERY EXECUTION PLAN\")\n        plan_lines.append(\"=\" * 60)\n        plan_lines.append(\"\")\n        plan_lines.append(f\"Query: {self.last_query}\")\n        plan_lines.append(\"\")\n        plan_lines.append(\"--- PLAN STEPS ---\")\n        plan_lines.append(\"\")\n\n        step = 1\n        # Source scan\n        plan_lines.append(f\"{step}. TABLE SCAN\")\n        plan_lines.append(f\"   Source: {parsed.source}\")\n        step += 1\n\n        # JOIN if present\n        if parsed.join and parsed.join.right_source:\n            plan_lines.append(\"\")\n            plan_lines.append(f\"{step}. JOIN\")\n            plan_lines.append(f\"   Type: {parsed.join.join_type.upper()}\")\n            plan_lines.append(f\"   Right Source: {parsed.join.right_source}\")\n            plan_lines.append(f\"   Condition: {parsed.join.on_left} = {parsed.join.on_right}\")\n            step += 1\n\n        # WHERE clause\n        if parsed.where:\n            plan_lines.append(\"\")\n            plan_lines.append(f\"{step}. FILTER\")\n            plan_lines.append(f\"   Condition: {parsed.where}\")\n            step += 1\n\n        # GROUP BY\n        if parsed.group_by:\n            plan_lines.append(\"\")\n            plan_lines.append(f\"{step}. GROUP BY\")\n            plan_lines.append(f\"   Columns: {', '.join(parsed.group_by)}\")\n            step += 1\n\n        # ORDER BY\n        if parsed.order_by:\n            plan_lines.append(\"\")\n            plan_lines.append(f\"{step}. SORT\")\n            order_strs = []\n            for col, direction in parsed.order_by:\n                order_strs.append(f\"{col} {'DESC' if direction else 'ASC'}\")\n            plan_lines.append(f\"   Order: {', '.join(order_strs)}\")\n            step += 1\n\n        # LIMIT\n        if parsed.limit is not None:\n            plan_lines.append(\"\")\n            plan_lines.append(f\"{step}. LIMIT\")\n            plan_lines.append(f\"   Rows: {parsed.limit}\")\n            step += 1\n\n        # Projection\n        plan_lines.append(\"\")\n        plan_lines.append(f\"{step}. PROJECTION\")\n        if parsed.columns:\n            plan_lines.append(f\"   Columns: {', '.join(parsed.columns)}\")\n        else:\n            plan_lines.append(\"   Columns: * (all)\")\n\n        plan_lines.append(\"\")\n        plan_lines.append(\"=\" * 60)\n        plan_lines.append(f\"Estimated rows returned: {len(self.last_results)}\")\n        plan_lines.append(\"=\" * 60)\n\n        plan_text = \"\\n\".join(plan_lines)\n\n        # Show explain dialog\n        await self.push_screen_wait(ExplainDialog(plan_text))\n\n    except Exception as e:\n        self._show_status(f\"Could not generate explain plan: {e}\", error=True)\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_prev_page","title":"action_prev_page","text":"<pre><code>action_prev_page() -&gt; None\n</code></pre> <p>Go to previous page of results.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_prev_page(self) -&gt; None:\n    \"\"\"Go to previous page of results.\"\"\"\n    if not self.filtered_results:\n        return\n\n    if self.current_page &gt; 0:\n        self.current_page -= 1\n        self._refresh_displayed_results()\n    else:\n        self._show_status(\"Already on first page\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_next_page","title":"action_next_page","text":"<pre><code>action_next_page() -&gt; None\n</code></pre> <p>Go to next page of results.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_next_page(self) -&gt; None:\n    \"\"\"Go to next page of results.\"\"\"\n    if not self.filtered_results:\n        return\n\n    total_pages = (len(self.filtered_results) + self.page_size - 1) // self.page_size\n    if self.current_page &lt; total_pages - 1:\n        self.current_page += 1\n        self._refresh_displayed_results()\n    else:\n        self._show_status(\"Already on last page\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.apply_advanced_filter","title":"apply_advanced_filter","text":"<pre><code>apply_advanced_filter(col: str, op: str, val1: str, val2: str) -&gt; None\n</code></pre> <p>Apply filter logic based on sidebar inputs.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def apply_advanced_filter(self, col: str, op: str, val1: str, val2: str) -&gt; None:\n    \"\"\"Apply filter logic based on sidebar inputs.\"\"\"\n    if not self.last_results:\n        return\n\n    self.filtered_results = []\n\n    # Helper to safely cast types\n    def safe_cast(val, target_type):\n        try:\n            if target_type is float:\n                return float(val)\n            if target_type is bool:\n                return val.lower() == \"true\"\n            return str(val).lower()\n        except Exception:\n            return None\n\n    for row in self.last_results:\n        # 1. Determine Row Value\n        if col == \"global\":\n            # Global search is always string match\n            row_vals = [str(v).lower() for v in row.values()]\n            if any(val1.lower() in rv for rv in row_vals):\n                self.filtered_results.append(row)\n            continue\n\n        # Specific Column Search\n        raw_val = row.get(col)\n\n        # Determine type based on the raw value in the row\n        target_type = str\n        if isinstance(raw_val, (int, float)):\n            target_type = float\n        elif isinstance(raw_val, bool):\n            target_type = bool\n\n        # Cast row value and input value\n        row_val = safe_cast(raw_val, target_type)\n        input_val = safe_cast(val1, target_type)\n\n        if row_val is None or input_val is None:\n            continue # Skip invalid data\n\n        match = False\n\n        # 2. Apply Operator Logic\n        if op == \"eq\":\n            match = row_val == input_val\n        elif op == \"contains\":\n            match = str(input_val) in str(row_val)\n        elif op == \"startswith\":\n            match = str(row_val).startswith(str(input_val))\n        elif op == \"endswith\":\n            match = str(row_val).endswith(str(input_val))\n        elif op == \"gt\":\n            match = row_val &gt; input_val\n        elif op == \"lt\":\n            match = row_val &lt; input_val\n        elif op == \"between\":\n            input_val_2 = safe_cast(val2, target_type)\n            if input_val_2 is not None:\n                match = input_val &lt;= row_val &lt;= input_val_2\n        elif op == \"is\":\n            # For booleans, input_val is already cast to bool\n            match = row_val is input_val\n        elif op == \"regex\":\n            import re\n            try:\n                if re.search(str(input_val), str(row_val), re.IGNORECASE):\n                    match = True\n            except Exception:\n                pass\n\n        if match:\n            self.filtered_results.append(row)\n\n     # Update state so Export and Status Bar know a filter is active\n    self.filter_active = True\n    self.filter_column = col\n    self.filter_mode = op\n    self.filter_text = str(val1) # Store value for display/logic\n    # ------------------------------------------------\n\n    self.current_page = 0\n    self._refresh_displayed_results()\n    self._refresh_tools_data() # Update the export sidebar count immediately\n    self._show_status(f\"Filtered: {col} {op} {val1}\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_open_file","title":"action_open_file","text":"<pre><code>action_open_file() -&gt; None\n</code></pre> <p>Switch to file browser tab and show sidebar.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_open_file(self) -&gt; None:\n    \"\"\"Switch to file browser tab and show sidebar.\"\"\"\n    # Show sidebar if hidden\n    container = self.query_one(\"#sidebar-container\")\n    if not container.has_class(\"visible\"):\n        container.add_class(\"visible\")\n\n    # Switch to Files tab\n    self.query_one(\"#sidebar-tabs\", TabbedContent).active = \"tab-files\"\n\n    # Focus file browser\n    self.query_one(\"#file-browser\", FileBrowser).focus()\n    self._show_status(\"Select a file from the sidebar\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.on_directory_tree_file_selected","title":"on_directory_tree_file_selected","text":"<pre><code>on_directory_tree_file_selected(event: FileSelected) -&gt; None\n</code></pre> <p>Handle file selection from sidebar.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def on_directory_tree_file_selected(self, event: DirectoryTree.FileSelected) -&gt; None:\n    \"\"\"Handle file selection from sidebar.\"\"\"\n    # FIX: Ignore events from the export tree to prevent overwriting the query\n    if event.control.id == \"export-tree\":\n        return\n\n    file_path = str(event.path)\n\n    # Get active editor\n    editor = self._get_active_editor()\n    current_text = editor.text.strip()\n\n    # Build query text with selected file\n    if not current_text:\n        # Empty editor - create simple SELECT query\n        new_text = f\"SELECT * FROM '{file_path}'\"\n    elif \"FROM\" in current_text.upper():\n        # Already has FROM clause - add the current text to history and replace with simple select statement\n        # Add to history if new\n        if not self.query_history or self.query_history[-1] != current_text:\n            self.query_history.append(current_text)\n            self._save_history()\n\n        new_text = f\"SELECT * FROM '{file_path}'\"\n        self._show_status(\"Last query stored in history\")\n    else:\n        # Append FROM clause\n        new_text = f\"{current_text}\\nFROM '{file_path}'\"\n\n    # Update editor\n    editor.text = new_text\n    # Move cursor to end\n    lines = new_text.splitlines()\n    editor.cursor_location = (len(lines) - 1, len(lines[-1]))\n\n    # Focus editor\n    editor.focus()\n\n    self._show_status(f\"Added file to query: {file_path}\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_new_tab","title":"action_new_tab  <code>async</code>","text":"<pre><code>action_new_tab(content: str = '', title: str = None) -&gt; None\n</code></pre> <p>Create a new query tab.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>async def action_new_tab(self, content: str = \"\", title: str = None) -&gt; None:\n    \"\"\"Create a new query tab.\"\"\"\n    self.tab_counter += 1\n    if not title:\n        title = f\"Query {self.tab_counter}\"\n\n    tab_id = f\"tab-query-{self.tab_counter}\"\n    editor_id = f\"query-editor-{self.tab_counter}\"\n\n    pane = TabPane(title, id=tab_id)\n    editor = QueryEditor(\n        id=editor_id,\n        language=\"sql\",\n        theme=self.editor_theme,\n        show_line_numbers=self.editor_linenums,\n        soft_wrap=self.editor_soft_wrap,\n        text=content\n    )\n\n    tabs = self.query_one(\"#query-tabs\", TabbedContent)\n    await tabs.add_pane(pane)\n    await pane.mount(editor)\n    tabs.active = tab_id\n    editor.focus()\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_close_tab","title":"action_close_tab  <code>async</code>","text":"<pre><code>action_close_tab() -&gt; None\n</code></pre> <p>Close the current query tab.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>async def action_close_tab(self) -&gt; None:\n    \"\"\"Close the current query tab.\"\"\"\n    tabs = self.query_one(\"#query-tabs\", TabbedContent)\n    active_tab = tabs.active\n    if not active_tab:\n        return\n\n    await tabs.remove_pane(active_tab)\n\n    # If no tabs left, create a new one\n    if not tabs.query(TabPane):\n         await self.action_new_tab()\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_quit","title":"action_quit","text":"<pre><code>action_quit() -&gt; None\n</code></pre> <p>Save state and exit, optionally confirming.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_quit(self) -&gt; None:\n    \"\"\"Save state and exit, optionally confirming.\"\"\"\n    if self.confirm_exit:\n        self.push_screen(ConfirmExitDialog(), self._finish_quit)\n    else:\n        self._finish_quit(True)\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SQLShellApp.action_save_state","title":"action_save_state","text":"<pre><code>action_save_state() -&gt; None\n</code></pre> <p>Manual save state action.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def action_save_state(self) -&gt; None:\n    \"\"\"Manual save state action.\"\"\"\n    self._save_state()\n    self._show_status(\"State saved!\")\n</code></pre>"},{"location":"api/reference/cli/#schemabrowser","title":"SchemaBrowser","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.SchemaBrowser","title":"SchemaBrowser","text":"<p>               Bases: <code>Tree</code></p> <p>Side panel for browsing files and schemas.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class SchemaBrowser(Tree):\n    \"\"\"Side panel for browsing files and schemas.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        super().__init__(\"Data Sources\", **kwargs)\n        self.border_title = \"Schema\"\n        self.show_root = False\n\n    def show_schemas(self, schemas: Dict[str, Dict[str, str]]) -&gt; None:\n        \"\"\"Update the schema tree with files and columns.\"\"\"\n        self.clear()\n        self.root.expand()\n\n        if not schemas:\n            self.root.add(\"No files loaded\")\n            return\n\n        for filename, schema in schemas.items():\n            # Add file node\n            file_node = self.root.add(Path(filename).name, expand=True)\n\n            # Add columns\n            for col, dtype in schema.items():\n                if col == \"Error\":\n                    file_node.add(f\"[red]Error: {dtype}[/red]\")\n                else:\n                    file_node.add(f\"[green]{col}[/green]: [dim]{dtype}[/dim]\")\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.SchemaBrowser.show_schemas","title":"show_schemas","text":"<pre><code>show_schemas(schemas: Dict[str, Dict[str, str]]) -&gt; None\n</code></pre> <p>Update the schema tree with files and columns.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def show_schemas(self, schemas: Dict[str, Dict[str, str]]) -&gt; None:\n    \"\"\"Update the schema tree with files and columns.\"\"\"\n    self.clear()\n    self.root.expand()\n\n    if not schemas:\n        self.root.add(\"No files loaded\")\n        return\n\n    for filename, schema in schemas.items():\n        # Add file node\n        file_node = self.root.add(Path(filename).name, expand=True)\n\n        # Add columns\n        for col, dtype in schema.items():\n            if col == \"Error\":\n                file_node.add(f\"[red]Error: {dtype}[/red]\")\n            else:\n                file_node.add(f\"[green]{col}[/green]: [dim]{dtype}[/dim]\")\n</code></pre>"},{"location":"api/reference/cli/#statusbar","title":"StatusBar","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.StatusBar","title":"StatusBar","text":"<p>               Bases: <code>Static</code></p> <p>Status bar showing messages and execution info.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>class StatusBar(Static):\n    \"\"\"Status bar showing messages and execution info.\"\"\"\n\n    def __init__(self, **kwargs) -&gt; None:\n        super().__init__(\"\", **kwargs)\n        self.last_execution_time: Optional[float] = None\n        self.row_count: Optional[int] = None\n\n    def update_status(\n        self,\n        message: str,\n        execution_time: Optional[float] = None,\n        row_count: Optional[int] = None,\n        filter_info: str = \"\",\n        backend_info: str = \"\"\n    ) -&gt; None:\n        \"\"\"Update status bar with execution info.\"\"\"\n        if execution_time is not None:\n            self.last_execution_time = execution_time\n        if row_count is not None:\n            self.row_count = row_count\n\n        status_parts = [message]\n        if filter_info:\n             status_parts.append(f\"\ud83d\udd0d {filter_info}\")\n        if backend_info:\n            status_parts.append(f\"\u2699\ufe0f {backend_info}\")\n        if self.row_count is not None:\n            status_parts.append(f\"{self.row_count} rows\")\n        if self.last_execution_time is not None:\n            status_parts.append(f\"{self.last_execution_time:.3f}s\")\n\n        self.update(\" | \".join(status_parts))\n</code></pre>"},{"location":"api/reference/cli/#sqlstream.cli.shell.StatusBar.update_status","title":"update_status","text":"<pre><code>update_status(message: str, execution_time: Optional[float] = None, row_count: Optional[int] = None, filter_info: str = '', backend_info: str = '') -&gt; None\n</code></pre> <p>Update status bar with execution info.</p> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def update_status(\n    self,\n    message: str,\n    execution_time: Optional[float] = None,\n    row_count: Optional[int] = None,\n    filter_info: str = \"\",\n    backend_info: str = \"\"\n) -&gt; None:\n    \"\"\"Update status bar with execution info.\"\"\"\n    if execution_time is not None:\n        self.last_execution_time = execution_time\n    if row_count is not None:\n        self.row_count = row_count\n\n    status_parts = [message]\n    if filter_info:\n         status_parts.append(f\"\ud83d\udd0d {filter_info}\")\n    if backend_info:\n        status_parts.append(f\"\u2699\ufe0f {backend_info}\")\n    if self.row_count is not None:\n        status_parts.append(f\"{self.row_count} rows\")\n    if self.last_execution_time is not None:\n        status_parts.append(f\"{self.last_execution_time:.3f}s\")\n\n    self.update(\" | \".join(status_parts))\n</code></pre>"},{"location":"api/reference/cli/#launch_shell","title":"launch_shell","text":""},{"location":"api/reference/cli/#sqlstream.cli.shell.launch_shell","title":"launch_shell","text":"<pre><code>launch_shell(initial_file: Optional[str] = None, history_file: Optional[str] = None) -&gt; None\n</code></pre> <p>Launch the interactive SQL shell.</p> <p>Parameters:</p> Name Type Description Default <code>initial_file</code> <code>Optional[str]</code> <p>Optional file to load on startup</p> <code>None</code> <code>history_file</code> <code>Optional[str]</code> <p>Path to query history file</p> <code>None</code> Source code in <code>sqlstream/cli/shell.py</code> <pre><code>def launch_shell(initial_file: Optional[str] = None, history_file: Optional[str] = None) -&gt; None:\n    \"\"\"\n    Launch the interactive SQL shell.\n\n    Args:\n        initial_file: Optional file to load on startup\n        history_file: Path to query history file\n    \"\"\"\n    app = SQLShellApp(initial_file=initial_file, history_file=history_file)\n    app.run()\n</code></pre>"},{"location":"api/reference/operators/","title":"Operators Reference","text":"<p>Query execution operators.</p>"},{"location":"api/reference/operators/#operator","title":"Operator","text":""},{"location":"api/reference/operators/#sqlstream.operators.base.Operator","title":"Operator","text":"<p>Base class for all query operators</p> <p>Operators form a tree where: - Leaf operators (e.g., Scan) read from data sources - Internal operators (e.g., Filter, Project) transform data - Root operator is pulled by the executor to get results</p> <p>The pull-based execution model means: - Operators are lazy (generators) - Data flows through the tree on-demand - Memory usage is O(pipeline depth), not O(data size)</p> Source code in <code>sqlstream/operators/base.py</code> <pre><code>class Operator:\n    \"\"\"\n    Base class for all query operators\n\n    Operators form a tree where:\n    - Leaf operators (e.g., Scan) read from data sources\n    - Internal operators (e.g., Filter, Project) transform data\n    - Root operator is pulled by the executor to get results\n\n    The pull-based execution model means:\n    - Operators are lazy (generators)\n    - Data flows through the tree on-demand\n    - Memory usage is O(pipeline depth), not O(data size)\n    \"\"\"\n\n    def __init__(self, child: Optional[\"Operator\"] = None):\n        \"\"\"\n        Initialize operator\n\n        Args:\n            child: Child operator to pull data from (None for leaf operators)\n        \"\"\"\n        self.child = child\n\n    def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Execute operator and yield results\n\n        This is the core method that defines operator behavior.\n        Subclasses must implement this to define how they process data.\n\n        Yields:\n            Rows as dictionaries\n        \"\"\"\n        raise NotImplementedError(f\"{self.__class__.__name__} must implement __iter__()\")\n\n    def __repr__(self) -&gt; str:\n        \"\"\"String representation for debugging\"\"\"\n        return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.base.Operator.__init__","title":"__init__","text":"<pre><code>__init__(child: Optional[Operator] = None)\n</code></pre> <p>Initialize operator</p> <p>Parameters:</p> Name Type Description Default <code>child</code> <code>Optional[Operator]</code> <p>Child operator to pull data from (None for leaf operators)</p> <code>None</code> Source code in <code>sqlstream/operators/base.py</code> <pre><code>def __init__(self, child: Optional[\"Operator\"] = None):\n    \"\"\"\n    Initialize operator\n\n    Args:\n        child: Child operator to pull data from (None for leaf operators)\n    \"\"\"\n    self.child = child\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.base.Operator.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Execute operator and yield results</p> <p>This is the core method that defines operator behavior. Subclasses must implement this to define how they process data.</p> <p>Yields:</p> Type Description <code>Dict[str, Any]</code> <p>Rows as dictionaries</p> Source code in <code>sqlstream/operators/base.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Execute operator and yield results\n\n    This is the core method that defines operator behavior.\n    Subclasses must implement this to define how they process data.\n\n    Yields:\n        Rows as dictionaries\n    \"\"\"\n    raise NotImplementedError(f\"{self.__class__.__name__} must implement __iter__()\")\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.base.Operator.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>String representation for debugging</p> Source code in <code>sqlstream/operators/base.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation for debugging\"\"\"\n    return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"api/reference/operators/#filter","title":"Filter","text":""},{"location":"api/reference/operators/#sqlstream.operators.filter.Filter","title":"Filter","text":"<p>               Bases: <code>Operator</code></p> <p>Filter operator - evaluates WHERE conditions</p> <p>Pulls rows from child and only yields those that satisfy all conditions (AND logic).</p> Source code in <code>sqlstream/operators/filter.py</code> <pre><code>class Filter(Operator):\n    \"\"\"\n    Filter operator - evaluates WHERE conditions\n\n    Pulls rows from child and only yields those that satisfy\n    all conditions (AND logic).\n    \"\"\"\n\n    def __init__(self, child: Operator, conditions: List[Condition]):\n        \"\"\"\n        Initialize filter operator\n\n        Args:\n            child: Child operator to pull rows from\n            conditions: List of conditions (AND'd together)\n        \"\"\"\n        super().__init__(child)\n        self.conditions = conditions\n\n    def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Yield only rows that match all conditions\n\n        For each row from child:\n        1. Evaluate all conditions\n        2. If all are True, yield the row\n        3. Otherwise, skip it\n        \"\"\"\n        for row in self.child:\n            if self._matches(row):\n                yield row\n\n    def _matches(self, row: Dict[str, Any]) -&gt; bool:\n        \"\"\"\n        Check if row matches all conditions\n\n        Args:\n            row: Row to check\n\n        Returns:\n            True if all conditions are satisfied (AND logic)\n        \"\"\"\n        for condition in self.conditions:\n            if not self._evaluate_condition(row, condition):\n                return False\n        return True\n\n    def _evaluate_condition(self, row: Dict[str, Any], condition: Condition) -&gt; bool:\n        \"\"\"\n        Evaluate a single condition against a row\n\n        Args:\n            row: Row to check\n            condition: Condition to evaluate\n\n        Returns:\n            True if condition is satisfied\n        \"\"\"\n        # Get column value\n        if condition.column not in row:\n            return False\n\n        value = row[condition.column]\n\n        # Handle NULL values\n        if value is None:\n            return False\n\n        # Get expected value\n        expected = condition.value\n\n        # Evaluate operator\n        op = condition.operator\n\n        try:\n            if op == \"=\":\n                return value == expected\n            elif op == \"&gt;\":\n                return value &gt; expected\n            elif op == \"&lt;\":\n                return value &lt; expected\n            elif op == \"&gt;=\":\n                return value &gt;= expected\n            elif op == \"&lt;=\":\n                return value &lt;= expected\n            elif op == \"!=\":\n                return value != expected\n            else:\n                # Unknown operator - default to True to avoid filtering\n                return True\n\n        except TypeError:\n            # Type mismatch (e.g., comparing string to int)\n            return False\n\n    def __repr__(self) -&gt; str:\n        cond_str = \" AND \".join(str(c) for c in self.conditions)\n        return f\"Filter({cond_str})\"\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.filter.Filter.__init__","title":"__init__","text":"<pre><code>__init__(child: Operator, conditions: List[Condition])\n</code></pre> <p>Initialize filter operator</p> <p>Parameters:</p> Name Type Description Default <code>child</code> <code>Operator</code> <p>Child operator to pull rows from</p> required <code>conditions</code> <code>List[Condition]</code> <p>List of conditions (AND'd together)</p> required Source code in <code>sqlstream/operators/filter.py</code> <pre><code>def __init__(self, child: Operator, conditions: List[Condition]):\n    \"\"\"\n    Initialize filter operator\n\n    Args:\n        child: Child operator to pull rows from\n        conditions: List of conditions (AND'd together)\n    \"\"\"\n    super().__init__(child)\n    self.conditions = conditions\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.filter.Filter.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Yield only rows that match all conditions</p> <p>For each row from child: 1. Evaluate all conditions 2. If all are True, yield the row 3. Otherwise, skip it</p> Source code in <code>sqlstream/operators/filter.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Yield only rows that match all conditions\n\n    For each row from child:\n    1. Evaluate all conditions\n    2. If all are True, yield the row\n    3. Otherwise, skip it\n    \"\"\"\n    for row in self.child:\n        if self._matches(row):\n            yield row\n</code></pre>"},{"location":"api/reference/operators/#groupbyoperator","title":"GroupByOperator","text":""},{"location":"api/reference/operators/#sqlstream.operators.groupby.GroupByOperator","title":"GroupByOperator","text":"<p>               Bases: <code>Operator</code></p> <p>GROUP BY operator with aggregation</p> <p>Uses hash-based aggregation: 1. Scan all input rows 2. Group by key columns 3. Maintain aggregators for each group 4. Yield one row per group</p> <p>Note: This operator materializes all data in memory (not lazy). For large datasets, consider external sorting/grouping.</p> Source code in <code>sqlstream/operators/groupby.py</code> <pre><code>class GroupByOperator(Operator):\n    \"\"\"\n    GROUP BY operator with aggregation\n\n    Uses hash-based aggregation:\n    1. Scan all input rows\n    2. Group by key columns\n    3. Maintain aggregators for each group\n    4. Yield one row per group\n\n    Note: This operator materializes all data in memory (not lazy).\n    For large datasets, consider external sorting/grouping.\n    \"\"\"\n\n    def __init__(\n        self,\n        source: Operator,\n        group_by_columns: List[str],\n        aggregates: List[AggregateFunction],\n        select_columns: List[str],\n    ):\n        \"\"\"\n        Initialize GroupBy operator\n\n        Args:\n            source: Source operator\n            group_by_columns: List of columns to group by\n            aggregates: List of aggregate functions to compute\n            select_columns: List of columns in SELECT clause (for output order)\n        \"\"\"\n        super().__init__(source)\n        self.group_by_columns = group_by_columns\n        self.aggregates = aggregates\n        self.select_columns = select_columns\n\n    def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Execute GROUP BY aggregation\n\n        Yields:\n            One row per group with group columns and aggregated values\n        \"\"\"\n        # Hash map: group_key -&gt; aggregators\n        groups: Dict[Tuple, List] = {}\n\n        # Scan all input rows and build groups\n        for row in self.child:\n            # Extract group key\n            group_key = self._extract_group_key(row)\n\n            # Initialize aggregators for new group\n            if group_key not in groups:\n                groups[group_key] = self._create_aggregators()\n\n            # Update aggregators\n            aggregators = groups[group_key]\n            for i, agg_func in enumerate(self.aggregates):\n                value = row.get(agg_func.column) if agg_func.column != \"*\" else None\n                aggregators[i].update(value)\n\n        # Yield one row per group\n        for group_key, aggregators in groups.items():\n            row = self._build_output_row(group_key, aggregators)\n            yield row\n\n    def _extract_group_key(self, row: Dict[str, Any]) -&gt; Tuple:\n        \"\"\"\n        Extract group key from row\n\n        Args:\n            row: Input row\n\n        Returns:\n            Tuple of group key values\n        \"\"\"\n        key_values = []\n        for col in self.group_by_columns:\n            value = row.get(col)\n            # Handle unhashable types (e.g., lists, dicts)\n            # Convert to string representation for hashing\n            if isinstance(value, (list, dict)):\n                value = str(value)\n            key_values.append(value)\n\n        return tuple(key_values)\n\n    def _create_aggregators(self) -&gt; List:\n        \"\"\"\n        Create fresh aggregators for a new group\n\n        Returns:\n            List of aggregator instances\n        \"\"\"\n        aggregators = []\n        for agg_func in self.aggregates:\n            aggregator = create_aggregator(agg_func.function, agg_func.column)\n            aggregators.append(aggregator)\n        return aggregators\n\n    def _build_output_row(self, group_key: Tuple, aggregators: List) -&gt; Dict[str, Any]:\n        \"\"\"\n        Build output row from group key and aggregated values\n\n        Args:\n            group_key: Tuple of group key values\n            aggregators: List of aggregators with final values\n\n        Returns:\n            Output row dictionary\n        \"\"\"\n        row = {}\n\n        # Add group key columns\n        for i, col_name in enumerate(self.group_by_columns):\n            row[col_name] = group_key[i]\n\n        # Add aggregated columns\n        for i, agg_func in enumerate(self.aggregates):\n            # Use alias if provided, otherwise generate name\n            col_name = (\n                agg_func.alias\n                if agg_func.alias\n                else f\"{agg_func.function.lower()}_{agg_func.column}\"\n            )\n            row[col_name] = aggregators[i].result()\n\n        return row\n\n    def explain(self, indent: int = 0) -&gt; List[str]:\n        \"\"\"Generate execution plan explanation\"\"\"\n        lines = [\" \" * indent + f\"GroupBy(keys={self.group_by_columns})\"]\n\n        # Show aggregate functions\n        for agg in self.aggregates:\n            lines.append(\" \" * (indent + 2) + f\"\u2192 {agg}\")\n\n        # Add source explanation\n        lines.extend(self.child.explain(indent + 2))\n\n        return lines\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.groupby.GroupByOperator.__init__","title":"__init__","text":"<pre><code>__init__(source: Operator, group_by_columns: List[str], aggregates: List[AggregateFunction], select_columns: List[str])\n</code></pre> <p>Initialize GroupBy operator</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Operator</code> <p>Source operator</p> required <code>group_by_columns</code> <code>List[str]</code> <p>List of columns to group by</p> required <code>aggregates</code> <code>List[AggregateFunction]</code> <p>List of aggregate functions to compute</p> required <code>select_columns</code> <code>List[str]</code> <p>List of columns in SELECT clause (for output order)</p> required Source code in <code>sqlstream/operators/groupby.py</code> <pre><code>def __init__(\n    self,\n    source: Operator,\n    group_by_columns: List[str],\n    aggregates: List[AggregateFunction],\n    select_columns: List[str],\n):\n    \"\"\"\n    Initialize GroupBy operator\n\n    Args:\n        source: Source operator\n        group_by_columns: List of columns to group by\n        aggregates: List of aggregate functions to compute\n        select_columns: List of columns in SELECT clause (for output order)\n    \"\"\"\n    super().__init__(source)\n    self.group_by_columns = group_by_columns\n    self.aggregates = aggregates\n    self.select_columns = select_columns\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.groupby.GroupByOperator.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Execute GROUP BY aggregation</p> <p>Yields:</p> Type Description <code>Dict[str, Any]</code> <p>One row per group with group columns and aggregated values</p> Source code in <code>sqlstream/operators/groupby.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Execute GROUP BY aggregation\n\n    Yields:\n        One row per group with group columns and aggregated values\n    \"\"\"\n    # Hash map: group_key -&gt; aggregators\n    groups: Dict[Tuple, List] = {}\n\n    # Scan all input rows and build groups\n    for row in self.child:\n        # Extract group key\n        group_key = self._extract_group_key(row)\n\n        # Initialize aggregators for new group\n        if group_key not in groups:\n            groups[group_key] = self._create_aggregators()\n\n        # Update aggregators\n        aggregators = groups[group_key]\n        for i, agg_func in enumerate(self.aggregates):\n            value = row.get(agg_func.column) if agg_func.column != \"*\" else None\n            aggregators[i].update(value)\n\n    # Yield one row per group\n    for group_key, aggregators in groups.items():\n        row = self._build_output_row(group_key, aggregators)\n        yield row\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.groupby.GroupByOperator.explain","title":"explain","text":"<pre><code>explain(indent: int = 0) -&gt; List[str]\n</code></pre> <p>Generate execution plan explanation</p> Source code in <code>sqlstream/operators/groupby.py</code> <pre><code>def explain(self, indent: int = 0) -&gt; List[str]:\n    \"\"\"Generate execution plan explanation\"\"\"\n    lines = [\" \" * indent + f\"GroupBy(keys={self.group_by_columns})\"]\n\n    # Show aggregate functions\n    for agg in self.aggregates:\n        lines.append(\" \" * (indent + 2) + f\"\u2192 {agg}\")\n\n    # Add source explanation\n    lines.extend(self.child.explain(indent + 2))\n\n    return lines\n</code></pre>"},{"location":"api/reference/operators/#hashjoinoperator","title":"HashJoinOperator","text":""},{"location":"api/reference/operators/#sqlstream.operators.join.HashJoinOperator","title":"HashJoinOperator","text":"<p>               Bases: <code>Operator</code></p> <p>Hash Join operator for equi-joins</p> <p>Supports: - INNER JOIN: Only matching rows from both tables - LEFT JOIN: All rows from left, matched rows from right (NULL if no match) - RIGHT JOIN: All rows from right, matched rows from left (NULL if no match)</p> <p>Algorithm: 1. Build hash table from right table (keyed by join column) 2. Probe hash table with rows from left table 3. Output joined rows based on join type</p> <p>Note: This operator materializes the right table in memory. For very large right tables, consider external hash join or other algorithms.</p> Source code in <code>sqlstream/operators/join.py</code> <pre><code>class HashJoinOperator(Operator):\n    \"\"\"\n    Hash Join operator for equi-joins\n\n    Supports:\n    - INNER JOIN: Only matching rows from both tables\n    - LEFT JOIN: All rows from left, matched rows from right (NULL if no match)\n    - RIGHT JOIN: All rows from right, matched rows from left (NULL if no match)\n\n    Algorithm:\n    1. Build hash table from right table (keyed by join column)\n    2. Probe hash table with rows from left table\n    3. Output joined rows based on join type\n\n    Note: This operator materializes the right table in memory.\n    For very large right tables, consider external hash join or other algorithms.\n    \"\"\"\n\n    def __init__(\n        self,\n        left: Operator,\n        right: Operator,\n        join_type: str,\n        left_key: str,\n        right_key: str,\n    ):\n        \"\"\"\n        Initialize Hash Join operator\n\n        Args:\n            left: Left table operator\n            right: Right table operator\n            join_type: 'INNER', 'LEFT', or 'RIGHT'\n            left_key: Column name in left table for join condition\n            right_key: Column name in right table for join condition\n        \"\"\"\n        # Store both children (join has two inputs)\n        super().__init__(left)\n        self.left = left\n        self.right = right\n        self.join_type = join_type.upper()\n        self.left_key = left_key\n        self.right_key = right_key\n\n        # Validate join type\n        if self.join_type not in (\"INNER\", \"LEFT\", \"RIGHT\"):\n            raise ValueError(f\"Unsupported join type: {join_type}\")\n\n    def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Execute hash join\n\n        Yields:\n            Joined rows with columns from both tables\n        \"\"\"\n        if self.join_type == \"INNER\":\n            yield from self._inner_join()\n        elif self.join_type == \"LEFT\":\n            yield from self._left_join()\n        elif self.join_type == \"RIGHT\":\n            yield from self._right_join()\n\n    def _inner_join(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Execute INNER JOIN\n\n        Returns only rows that have matching join keys in both tables.\n        \"\"\"\n        # Build phase: Create hash table from right table\n        hash_table = self._build_hash_table()\n\n        # Probe phase: Scan left table and find matches\n        for left_row in self.left:\n            join_key = left_row.get(self.left_key)\n\n            # Skip rows with NULL join key (standard SQL behavior)\n            if join_key is None:\n                continue\n\n            # Probe hash table\n            if join_key in hash_table:\n                # Found match(es) - join with all matching right rows\n                for right_row in hash_table[join_key]:\n                    yield self._merge_rows(left_row, right_row)\n\n    def _left_join(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Execute LEFT JOIN\n\n        Returns all rows from left table. If there's a match in right table,\n        include right columns. If no match, right columns are NULL.\n        \"\"\"\n        # Build phase: Create hash table from right table\n        hash_table = self._build_hash_table()\n\n        # Probe phase: Scan left table\n        for left_row in self.left:\n            join_key = left_row.get(self.left_key)\n\n            # Check for match\n            if join_key is not None and join_key in hash_table:\n                # Found match(es) - join with all matching right rows\n                for right_row in hash_table[join_key]:\n                    yield self._merge_rows(left_row, right_row)\n            else:\n                # No match - output left row with NULL for right columns\n                yield self._merge_rows(left_row, None)\n\n    def _right_join(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Execute RIGHT JOIN\n\n        Returns all rows from right table. If there's a match in left table,\n        include left columns. If no match, left columns are NULL.\n        \"\"\"\n        # Build phase: Create hash table from right table\n        # Also track which right rows were matched\n        hash_table = self._build_hash_table()\n        matched_right_rows = set()  # Track (join_key, row_index) tuples\n\n        # Probe phase: Scan left table and output matches\n        for left_row in self.left:\n            join_key = left_row.get(self.left_key)\n\n            if join_key is not None and join_key in hash_table:\n                # Found match(es) - join with all matching right rows\n                for idx, right_row in enumerate(hash_table[join_key]):\n                    yield self._merge_rows(left_row, right_row)\n                    # Mark this right row as matched\n                    matched_right_rows.add((join_key, idx))\n\n        # Output unmatched right rows with NULL for left columns\n        for join_key, right_rows in hash_table.items():\n            for idx, right_row in enumerate(right_rows):\n                if (join_key, idx) not in matched_right_rows:\n                    yield self._merge_rows(None, right_row)\n\n    def _build_hash_table(self) -&gt; Dict[Any, List[Dict[str, Any]]]:\n        \"\"\"\n        Build hash table from right table\n\n        Returns:\n            Hash table mapping join key values to lists of matching rows\n        \"\"\"\n        hash_table: Dict[Any, List[Dict[str, Any]]] = {}\n\n        for row in self.right:\n            join_key = row.get(self.right_key)\n\n            # Skip rows with NULL join key (they can never match)\n            if join_key is None:\n                continue\n\n            # Handle unhashable types (e.g., lists, dicts)\n            if isinstance(join_key, (list, dict)):\n                join_key = str(join_key)\n\n            # Add row to hash table\n            if join_key not in hash_table:\n                hash_table[join_key] = []\n            hash_table[join_key].append(row)\n\n        return hash_table\n\n    def _merge_rows(\n        self, left_row: Dict[str, Any] | None, right_row: Dict[str, Any] | None\n    ) -&gt; Dict[str, Any]:\n        \"\"\"\n        Merge left and right rows into a single output row\n\n        Handles column name conflicts by prefixing with table names if needed.\n\n        Args:\n            left_row: Row from left table (None for RIGHT JOIN with no match)\n            right_row: Row from right table (None for LEFT JOIN with no match)\n\n        Returns:\n            Merged row dictionary\n        \"\"\"\n        result = {}\n\n        # Add left columns\n        if left_row is not None:\n            result.update(left_row)\n        elif right_row is not None:\n            # For RIGHT JOIN with no match, add NULL for all left columns\n            # We don't know the left schema, so we just don't add anything\n            # The columns will be added on first matched row\n            pass\n\n        # Add right columns\n        if right_row is not None:\n            for key, value in right_row.items():\n                # Handle column name conflicts\n                if key in result and left_row is not None:\n                    # Column exists in both tables - this shouldn't happen with\n                    # proper column qualification in SELECT, but handle it\n                    # by keeping the left value and prefixing right with \"right_\"\n                    result[f\"right_{key}\"] = value\n                else:\n                    result[key] = value\n\n        return result\n\n    def explain(self, indent: int = 0) -&gt; List[str]:\n        \"\"\"Generate execution plan explanation\"\"\"\n        lines = [\n            \" \" * indent\n            + f\"HashJoin({self.join_type}, {self.left_key} = {self.right_key})\"\n        ]\n        lines.append(\" \" * (indent + 2) + \"Left:\")\n        lines.extend(self.left.explain(indent + 4))\n        lines.append(\" \" * (indent + 2) + \"Right:\")\n        lines.extend(self.right.explain(indent + 4))\n        return lines\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.join.HashJoinOperator.__init__","title":"__init__","text":"<pre><code>__init__(left: Operator, right: Operator, join_type: str, left_key: str, right_key: str)\n</code></pre> <p>Initialize Hash Join operator</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>Operator</code> <p>Left table operator</p> required <code>right</code> <code>Operator</code> <p>Right table operator</p> required <code>join_type</code> <code>str</code> <p>'INNER', 'LEFT', or 'RIGHT'</p> required <code>left_key</code> <code>str</code> <p>Column name in left table for join condition</p> required <code>right_key</code> <code>str</code> <p>Column name in right table for join condition</p> required Source code in <code>sqlstream/operators/join.py</code> <pre><code>def __init__(\n    self,\n    left: Operator,\n    right: Operator,\n    join_type: str,\n    left_key: str,\n    right_key: str,\n):\n    \"\"\"\n    Initialize Hash Join operator\n\n    Args:\n        left: Left table operator\n        right: Right table operator\n        join_type: 'INNER', 'LEFT', or 'RIGHT'\n        left_key: Column name in left table for join condition\n        right_key: Column name in right table for join condition\n    \"\"\"\n    # Store both children (join has two inputs)\n    super().__init__(left)\n    self.left = left\n    self.right = right\n    self.join_type = join_type.upper()\n    self.left_key = left_key\n    self.right_key = right_key\n\n    # Validate join type\n    if self.join_type not in (\"INNER\", \"LEFT\", \"RIGHT\"):\n        raise ValueError(f\"Unsupported join type: {join_type}\")\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.join.HashJoinOperator.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Execute hash join</p> <p>Yields:</p> Type Description <code>Dict[str, Any]</code> <p>Joined rows with columns from both tables</p> Source code in <code>sqlstream/operators/join.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Execute hash join\n\n    Yields:\n        Joined rows with columns from both tables\n    \"\"\"\n    if self.join_type == \"INNER\":\n        yield from self._inner_join()\n    elif self.join_type == \"LEFT\":\n        yield from self._left_join()\n    elif self.join_type == \"RIGHT\":\n        yield from self._right_join()\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.join.HashJoinOperator.explain","title":"explain","text":"<pre><code>explain(indent: int = 0) -&gt; List[str]\n</code></pre> <p>Generate execution plan explanation</p> Source code in <code>sqlstream/operators/join.py</code> <pre><code>def explain(self, indent: int = 0) -&gt; List[str]:\n    \"\"\"Generate execution plan explanation\"\"\"\n    lines = [\n        \" \" * indent\n        + f\"HashJoin({self.join_type}, {self.left_key} = {self.right_key})\"\n    ]\n    lines.append(\" \" * (indent + 2) + \"Left:\")\n    lines.extend(self.left.explain(indent + 4))\n    lines.append(\" \" * (indent + 2) + \"Right:\")\n    lines.extend(self.right.explain(indent + 4))\n    return lines\n</code></pre>"},{"location":"api/reference/operators/#limit","title":"Limit","text":""},{"location":"api/reference/operators/#sqlstream.operators.limit.Limit","title":"Limit","text":"<p>               Bases: <code>Operator</code></p> <p>Limit operator - restricts number of rows (LIMIT clause)</p> <p>Pulls rows from child and yields only the first N rows. This allows for early termination - we stop pulling from child once we've yielded enough rows.</p> Source code in <code>sqlstream/operators/limit.py</code> <pre><code>class Limit(Operator):\n    \"\"\"\n    Limit operator - restricts number of rows (LIMIT clause)\n\n    Pulls rows from child and yields only the first N rows.\n    This allows for early termination - we stop pulling from\n    child once we've yielded enough rows.\n    \"\"\"\n\n    def __init__(self, child: Operator, limit: int):\n        \"\"\"\n        Initialize limit operator\n\n        Args:\n            child: Child operator to pull rows from\n            limit: Maximum number of rows to yield\n        \"\"\"\n        super().__init__(child)\n        self.limit = limit\n\n    def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Yield at most limit rows\n\n        This is efficient because it stops pulling from child\n        as soon as we've yielded enough rows (early termination).\n        \"\"\"\n        count = 0\n\n        for row in self.child:\n            if count &gt;= self.limit:\n                break\n\n            yield row\n            count += 1\n\n    def __repr__(self) -&gt; str:\n        return f\"Limit({self.limit})\"\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.limit.Limit.__init__","title":"__init__","text":"<pre><code>__init__(child: Operator, limit: int)\n</code></pre> <p>Initialize limit operator</p> <p>Parameters:</p> Name Type Description Default <code>child</code> <code>Operator</code> <p>Child operator to pull rows from</p> required <code>limit</code> <code>int</code> <p>Maximum number of rows to yield</p> required Source code in <code>sqlstream/operators/limit.py</code> <pre><code>def __init__(self, child: Operator, limit: int):\n    \"\"\"\n    Initialize limit operator\n\n    Args:\n        child: Child operator to pull rows from\n        limit: Maximum number of rows to yield\n    \"\"\"\n    super().__init__(child)\n    self.limit = limit\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.limit.Limit.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Yield at most limit rows</p> <p>This is efficient because it stops pulling from child as soon as we've yielded enough rows (early termination).</p> Source code in <code>sqlstream/operators/limit.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Yield at most limit rows\n\n    This is efficient because it stops pulling from child\n    as soon as we've yielded enough rows (early termination).\n    \"\"\"\n    count = 0\n\n    for row in self.child:\n        if count &gt;= self.limit:\n            break\n\n        yield row\n        count += 1\n</code></pre>"},{"location":"api/reference/operators/#orderbyoperator","title":"OrderByOperator","text":""},{"location":"api/reference/operators/#sqlstream.operators.orderby.OrderByOperator","title":"OrderByOperator","text":"<p>               Bases: <code>Operator</code></p> <p>ORDER BY operator</p> <p>Sorts all input rows by specified columns.</p> <p>Note: This operator materializes all data in memory (not lazy). For large datasets that don't fit in memory, consider external sorting.</p> Source code in <code>sqlstream/operators/orderby.py</code> <pre><code>class OrderByOperator(Operator):\n    \"\"\"\n    ORDER BY operator\n\n    Sorts all input rows by specified columns.\n\n    Note: This operator materializes all data in memory (not lazy).\n    For large datasets that don't fit in memory, consider external sorting.\n    \"\"\"\n\n    def __init__(self, source: Operator, order_by: List[OrderByColumn]):\n        \"\"\"\n        Initialize OrderBy operator\n\n        Args:\n            source: Source operator\n            order_by: List of OrderByColumn specifications\n        \"\"\"\n        super().__init__(source)\n        self.order_by = order_by\n\n    def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Execute ORDER BY sorting\n\n        Yields:\n            Rows in sorted order\n        \"\"\"\n        # Materialize all rows\n        rows = list(self.child)\n\n        # Sort rows using multi-key sort\n        sorted_rows = sorted(rows, key=self._sort_key)\n\n        # Yield sorted rows\n        yield from sorted_rows\n\n    def _sort_key(self, row: Dict[str, Any]) -&gt; tuple:\n        \"\"\"\n        Generate sort key for a row\n\n        Args:\n            row: Input row\n\n        Returns:\n            Tuple of (value, reverse_flag) for multi-key sorting\n        \"\"\"\n        key_parts = []\n\n        for order_col in self.order_by:\n            value = row.get(order_col.column)\n\n            # Handle NULL values - sort them last\n            if value is None:\n                # Use a sentinel that sorts last\n                value = (1, None)  # (sort_last_flag, None)\n            else:\n                value = (0, value)  # (sort_first_flag, actual_value)\n\n            # For DESC, we need to reverse the comparison\n            if order_col.direction == \"DESC\":\n                # Invert the sort order by negating numbers or using reverse wrapper\n                if isinstance(value[1], (int, float)):\n                    value = (value[0], -value[1] if value[1] is not None else None)\n                else:\n                    # For non-numeric types, we'll use a reverse wrapper\n                    value = (value[0], ReverseCompare(value[1]))\n\n            key_parts.append(value)\n\n        return tuple(key_parts)\n\n    def explain(self, indent: int = 0) -&gt; List[str]:\n        \"\"\"Generate execution plan explanation\"\"\"\n        order_spec = \", \".join(\n            f\"{col.column} {col.direction}\" for col in self.order_by\n        )\n        lines = [\" \" * indent + f\"OrderBy({order_spec})\"]\n        lines.extend(self.child.explain(indent + 2))\n        return lines\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.orderby.OrderByOperator.__init__","title":"__init__","text":"<pre><code>__init__(source: Operator, order_by: List[OrderByColumn])\n</code></pre> <p>Initialize OrderBy operator</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Operator</code> <p>Source operator</p> required <code>order_by</code> <code>List[OrderByColumn]</code> <p>List of OrderByColumn specifications</p> required Source code in <code>sqlstream/operators/orderby.py</code> <pre><code>def __init__(self, source: Operator, order_by: List[OrderByColumn]):\n    \"\"\"\n    Initialize OrderBy operator\n\n    Args:\n        source: Source operator\n        order_by: List of OrderByColumn specifications\n    \"\"\"\n    super().__init__(source)\n    self.order_by = order_by\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.orderby.OrderByOperator.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Execute ORDER BY sorting</p> <p>Yields:</p> Type Description <code>Dict[str, Any]</code> <p>Rows in sorted order</p> Source code in <code>sqlstream/operators/orderby.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Execute ORDER BY sorting\n\n    Yields:\n        Rows in sorted order\n    \"\"\"\n    # Materialize all rows\n    rows = list(self.child)\n\n    # Sort rows using multi-key sort\n    sorted_rows = sorted(rows, key=self._sort_key)\n\n    # Yield sorted rows\n    yield from sorted_rows\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.orderby.OrderByOperator.explain","title":"explain","text":"<pre><code>explain(indent: int = 0) -&gt; List[str]\n</code></pre> <p>Generate execution plan explanation</p> Source code in <code>sqlstream/operators/orderby.py</code> <pre><code>def explain(self, indent: int = 0) -&gt; List[str]:\n    \"\"\"Generate execution plan explanation\"\"\"\n    order_spec = \", \".join(\n        f\"{col.column} {col.direction}\" for col in self.order_by\n    )\n    lines = [\" \" * indent + f\"OrderBy({order_spec})\"]\n    lines.extend(self.child.explain(indent + 2))\n    return lines\n</code></pre>"},{"location":"api/reference/operators/#reversecompare","title":"ReverseCompare","text":""},{"location":"api/reference/operators/#sqlstream.operators.orderby.ReverseCompare","title":"ReverseCompare","text":"<p>Wrapper class to reverse comparison order for non-numeric types</p> <p>Used for DESC sorting of strings and other non-numeric types.</p> Source code in <code>sqlstream/operators/orderby.py</code> <pre><code>class ReverseCompare:\n    \"\"\"\n    Wrapper class to reverse comparison order for non-numeric types\n\n    Used for DESC sorting of strings and other non-numeric types.\n    \"\"\"\n\n    def __init__(self, value):\n        self.value = value\n\n    def __lt__(self, other):\n        if isinstance(other, ReverseCompare):\n            return self.value &gt; other.value\n        return self.value &gt; other\n\n    def __le__(self, other):\n        if isinstance(other, ReverseCompare):\n            return self.value &gt;= other.value\n        return self.value &gt;= other\n\n    def __gt__(self, other):\n        if isinstance(other, ReverseCompare):\n            return self.value &lt; other.value\n        return self.value &lt; other\n\n    def __ge__(self, other):\n        if isinstance(other, ReverseCompare):\n            return self.value &lt;= other.value\n        return self.value &lt;= other\n\n    def __eq__(self, other):\n        if isinstance(other, ReverseCompare):\n            return self.value == other.value\n        return self.value == other\n\n    def __ne__(self, other):\n        if isinstance(other, ReverseCompare):\n            return self.value != other.value\n        return self.value != other\n\n    def __repr__(self):\n        return f\"ReverseCompare({self.value!r})\"\n</code></pre>"},{"location":"api/reference/operators/#project","title":"Project","text":""},{"location":"api/reference/operators/#sqlstream.operators.project.Project","title":"Project","text":"<p>               Bases: <code>Operator</code></p> <p>Project operator - selects columns (SELECT clause)</p> <p>Pulls rows from child and yields only the requested columns.</p> <p>For efficiency, we use dict views rather than copying data.</p> Source code in <code>sqlstream/operators/project.py</code> <pre><code>class Project(Operator):\n    \"\"\"\n    Project operator - selects columns (SELECT clause)\n\n    Pulls rows from child and yields only the requested columns.\n\n    For efficiency, we use dict views rather than copying data.\n    \"\"\"\n\n    def __init__(self, child: Operator, columns: List[str]):\n        \"\"\"\n        Initialize project operator\n\n        Args:\n            child: Child operator to pull rows from\n            columns: List of column names to select (or ['*'] for all)\n        \"\"\"\n        super().__init__(child)\n        self.columns = columns\n\n    def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Yield rows with only selected columns\n\n        If columns is ['*'], yields all columns unchanged.\n        Otherwise, creates a new dict with only the requested columns.\n        \"\"\"\n        # SELECT *\n        if self.columns == [\"*\"]:\n            yield from self.child\n            return\n\n        # SELECT specific columns\n        for row in self.child:\n            # Create projected row with only selected columns\n            projected = {}\n\n            for col in self.columns:\n                if col in row:\n                    projected[col] = row[col]\n                else:\n                    # Column not found - set to None\n                    projected[col] = None\n\n            yield projected\n\n    def __repr__(self) -&gt; str:\n        col_str = \", \".join(self.columns)\n        return f\"Project({col_str})\"\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.project.Project.__init__","title":"__init__","text":"<pre><code>__init__(child: Operator, columns: List[str])\n</code></pre> <p>Initialize project operator</p> <p>Parameters:</p> Name Type Description Default <code>child</code> <code>Operator</code> <p>Child operator to pull rows from</p> required <code>columns</code> <code>List[str]</code> <p>List of column names to select (or ['*'] for all)</p> required Source code in <code>sqlstream/operators/project.py</code> <pre><code>def __init__(self, child: Operator, columns: List[str]):\n    \"\"\"\n    Initialize project operator\n\n    Args:\n        child: Child operator to pull rows from\n        columns: List of column names to select (or ['*'] for all)\n    \"\"\"\n    super().__init__(child)\n    self.columns = columns\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.project.Project.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Yield rows with only selected columns</p> <p>If columns is ['*'], yields all columns unchanged. Otherwise, creates a new dict with only the requested columns.</p> Source code in <code>sqlstream/operators/project.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Yield rows with only selected columns\n\n    If columns is ['*'], yields all columns unchanged.\n    Otherwise, creates a new dict with only the requested columns.\n    \"\"\"\n    # SELECT *\n    if self.columns == [\"*\"]:\n        yield from self.child\n        return\n\n    # SELECT specific columns\n    for row in self.child:\n        # Create projected row with only selected columns\n        projected = {}\n\n        for col in self.columns:\n            if col in row:\n                projected[col] = row[col]\n            else:\n                # Column not found - set to None\n                projected[col] = None\n\n        yield projected\n</code></pre>"},{"location":"api/reference/operators/#scan","title":"Scan","text":""},{"location":"api/reference/operators/#sqlstream.operators.scan.Scan","title":"Scan","text":"<p>               Bases: <code>Operator</code></p> <p>Scan operator - wrapper around a data source reader</p> <p>This is the leaf of the operator tree. It pulls data from a reader and yields it to parent operators.</p> Source code in <code>sqlstream/operators/scan.py</code> <pre><code>class Scan(Operator):\n    \"\"\"\n    Scan operator - wrapper around a data source reader\n\n    This is the leaf of the operator tree. It pulls data from\n    a reader and yields it to parent operators.\n    \"\"\"\n\n    def __init__(self, reader: BaseReader):\n        \"\"\"\n        Initialize scan operator\n\n        Args:\n            reader: Data source reader to scan\n        \"\"\"\n        super().__init__(child=None)  # Scan has no child\n        self.reader = reader\n\n    def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Yield all rows from the reader\n\n        This delegates directly to the reader's lazy iterator.\n        \"\"\"\n        yield from self.reader.read_lazy()\n\n    def __repr__(self) -&gt; str:\n        return f\"Scan({self.reader.__class__.__name__})\"\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.scan.Scan.__init__","title":"__init__","text":"<pre><code>__init__(reader: BaseReader)\n</code></pre> <p>Initialize scan operator</p> <p>Parameters:</p> Name Type Description Default <code>reader</code> <code>BaseReader</code> <p>Data source reader to scan</p> required Source code in <code>sqlstream/operators/scan.py</code> <pre><code>def __init__(self, reader: BaseReader):\n    \"\"\"\n    Initialize scan operator\n\n    Args:\n        reader: Data source reader to scan\n    \"\"\"\n    super().__init__(child=None)  # Scan has no child\n    self.reader = reader\n</code></pre>"},{"location":"api/reference/operators/#sqlstream.operators.scan.Scan.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Yield all rows from the reader</p> <p>This delegates directly to the reader's lazy iterator.</p> Source code in <code>sqlstream/operators/scan.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Yield all rows from the reader\n\n    This delegates directly to the reader's lazy iterator.\n    \"\"\"\n    yield from self.reader.read_lazy()\n</code></pre>"},{"location":"api/reference/optimizers/","title":"Optimizers Reference","text":"<p>Query optimization strategies.</p>"},{"location":"api/reference/optimizers/#optimizer","title":"Optimizer","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.base.Optimizer","title":"Optimizer","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all query optimizers</p> <p>Each optimizer implements a single optimization rule. Optimizers are applied in a pipeline before query execution.</p> Source code in <code>sqlstream/optimizers/base.py</code> <pre><code>class Optimizer(ABC):\n    \"\"\"\n    Base class for all query optimizers\n\n    Each optimizer implements a single optimization rule.\n    Optimizers are applied in a pipeline before query execution.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize optimizer\"\"\"\n        self.applied = False\n        self.description = \"\"\n\n    @abstractmethod\n    def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n        \"\"\"\n        Check if this optimization can be applied\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n\n        Returns:\n            True if optimization is applicable\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n        \"\"\"\n        Apply the optimization\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n\n        Modifies:\n            - reader: Sets optimization hints\n            - self.applied: Marks optimization as applied\n            - self.description: Describes what was optimized\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_name(self) -&gt; str:\n        \"\"\"\n        Get the name of this optimizer\n\n        Returns:\n            Human-readable optimizer name\n        \"\"\"\n        pass\n\n    def get_description(self) -&gt; str:\n        \"\"\"\n        Get description of what was optimized\n\n        Returns:\n            Description string if applied, empty string otherwise\n        \"\"\"\n        return self.description if self.applied else \"\"\n\n    def was_applied(self) -&gt; bool:\n        \"\"\"\n        Check if optimization was applied\n\n        Returns:\n            True if optimization was applied\n        \"\"\"\n        return self.applied\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.base.Optimizer.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize optimizer</p> Source code in <code>sqlstream/optimizers/base.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize optimizer\"\"\"\n    self.applied = False\n    self.description = \"\"\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.base.Optimizer.can_optimize","title":"can_optimize  <code>abstractmethod</code>","text":"<pre><code>can_optimize(ast: SelectStatement, reader: BaseReader) -&gt; bool\n</code></pre> <p>Check if this optimization can be applied</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if optimization is applicable</p> Source code in <code>sqlstream/optimizers/base.py</code> <pre><code>@abstractmethod\ndef can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n    \"\"\"\n    Check if this optimization can be applied\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n\n    Returns:\n        True if optimization is applicable\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.base.Optimizer.optimize","title":"optimize  <code>abstractmethod</code>","text":"<pre><code>optimize(ast: SelectStatement, reader: BaseReader) -&gt; None\n</code></pre> <p>Apply the optimization</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required Modifies <ul> <li>reader: Sets optimization hints</li> <li>self.applied: Marks optimization as applied</li> <li>self.description: Describes what was optimized</li> </ul> Source code in <code>sqlstream/optimizers/base.py</code> <pre><code>@abstractmethod\ndef optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n    \"\"\"\n    Apply the optimization\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n\n    Modifies:\n        - reader: Sets optimization hints\n        - self.applied: Marks optimization as applied\n        - self.description: Describes what was optimized\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.base.Optimizer.get_name","title":"get_name  <code>abstractmethod</code>","text":"<pre><code>get_name() -&gt; str\n</code></pre> <p>Get the name of this optimizer</p> <p>Returns:</p> Type Description <code>str</code> <p>Human-readable optimizer name</p> Source code in <code>sqlstream/optimizers/base.py</code> <pre><code>@abstractmethod\ndef get_name(self) -&gt; str:\n    \"\"\"\n    Get the name of this optimizer\n\n    Returns:\n        Human-readable optimizer name\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.base.Optimizer.get_description","title":"get_description","text":"<pre><code>get_description() -&gt; str\n</code></pre> <p>Get description of what was optimized</p> <p>Returns:</p> Type Description <code>str</code> <p>Description string if applied, empty string otherwise</p> Source code in <code>sqlstream/optimizers/base.py</code> <pre><code>def get_description(self) -&gt; str:\n    \"\"\"\n    Get description of what was optimized\n\n    Returns:\n        Description string if applied, empty string otherwise\n    \"\"\"\n    return self.description if self.applied else \"\"\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.base.Optimizer.was_applied","title":"was_applied","text":"<pre><code>was_applied() -&gt; bool\n</code></pre> <p>Check if optimization was applied</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if optimization was applied</p> Source code in <code>sqlstream/optimizers/base.py</code> <pre><code>def was_applied(self) -&gt; bool:\n    \"\"\"\n    Check if optimization was applied\n\n    Returns:\n        True if optimization was applied\n    \"\"\"\n    return self.applied\n</code></pre>"},{"location":"api/reference/optimizers/#optimizerpipeline","title":"OptimizerPipeline","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.base.OptimizerPipeline","title":"OptimizerPipeline","text":"<p>Pipeline that applies multiple optimizers in sequence</p> <p>Optimizers are applied in order, and each can build on the previous optimizations.</p> Source code in <code>sqlstream/optimizers/base.py</code> <pre><code>class OptimizerPipeline:\n    \"\"\"\n    Pipeline that applies multiple optimizers in sequence\n\n    Optimizers are applied in order, and each can build on\n    the previous optimizations.\n    \"\"\"\n\n    def __init__(self, optimizers: List[Optimizer]):\n        \"\"\"\n        Initialize pipeline\n\n        Args:\n            optimizers: List of optimizers to apply in order\n        \"\"\"\n        self.optimizers = optimizers\n\n    def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n        \"\"\"\n        Apply all optimizers in sequence\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n        \"\"\"\n        for optimizer in self.optimizers:\n            if optimizer.can_optimize(ast, reader):\n                optimizer.optimize(ast, reader)\n\n    def get_applied_optimizations(self) -&gt; List[str]:\n        \"\"\"\n        Get list of optimizations that were applied\n\n        Returns:\n            List of optimization descriptions\n        \"\"\"\n        return [\n            f\"{opt.get_name()}: {opt.get_description()}\"\n            for opt in self.optimizers\n            if opt.was_applied()\n        ]\n\n    def get_summary(self) -&gt; str:\n        \"\"\"\n        Get summary of all applied optimizations\n\n        Returns:\n            Human-readable summary\n        \"\"\"\n        applied = self.get_applied_optimizations()\n\n        if not applied:\n            return \"No optimizations applied\"\n\n        summary = \"Optimizations applied:\\n\"\n        for opt_desc in applied:\n            summary += f\"  - {opt_desc}\\n\"\n\n        return summary.strip()\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.base.OptimizerPipeline.__init__","title":"__init__","text":"<pre><code>__init__(optimizers: List[Optimizer])\n</code></pre> <p>Initialize pipeline</p> <p>Parameters:</p> Name Type Description Default <code>optimizers</code> <code>List[Optimizer]</code> <p>List of optimizers to apply in order</p> required Source code in <code>sqlstream/optimizers/base.py</code> <pre><code>def __init__(self, optimizers: List[Optimizer]):\n    \"\"\"\n    Initialize pipeline\n\n    Args:\n        optimizers: List of optimizers to apply in order\n    \"\"\"\n    self.optimizers = optimizers\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.base.OptimizerPipeline.optimize","title":"optimize","text":"<pre><code>optimize(ast: SelectStatement, reader: BaseReader) -&gt; None\n</code></pre> <p>Apply all optimizers in sequence</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required Source code in <code>sqlstream/optimizers/base.py</code> <pre><code>def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n    \"\"\"\n    Apply all optimizers in sequence\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n    \"\"\"\n    for optimizer in self.optimizers:\n        if optimizer.can_optimize(ast, reader):\n            optimizer.optimize(ast, reader)\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.base.OptimizerPipeline.get_applied_optimizations","title":"get_applied_optimizations","text":"<pre><code>get_applied_optimizations() -&gt; List[str]\n</code></pre> <p>Get list of optimizations that were applied</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of optimization descriptions</p> Source code in <code>sqlstream/optimizers/base.py</code> <pre><code>def get_applied_optimizations(self) -&gt; List[str]:\n    \"\"\"\n    Get list of optimizations that were applied\n\n    Returns:\n        List of optimization descriptions\n    \"\"\"\n    return [\n        f\"{opt.get_name()}: {opt.get_description()}\"\n        for opt in self.optimizers\n        if opt.was_applied()\n    ]\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.base.OptimizerPipeline.get_summary","title":"get_summary","text":"<pre><code>get_summary() -&gt; str\n</code></pre> <p>Get summary of all applied optimizations</p> <p>Returns:</p> Type Description <code>str</code> <p>Human-readable summary</p> Source code in <code>sqlstream/optimizers/base.py</code> <pre><code>def get_summary(self) -&gt; str:\n    \"\"\"\n    Get summary of all applied optimizations\n\n    Returns:\n        Human-readable summary\n    \"\"\"\n    applied = self.get_applied_optimizations()\n\n    if not applied:\n        return \"No optimizations applied\"\n\n    summary = \"Optimizations applied:\\n\"\n    for opt_desc in applied:\n        summary += f\"  - {opt_desc}\\n\"\n\n    return summary.strip()\n</code></pre>"},{"location":"api/reference/optimizers/#columnpruningoptimizer","title":"ColumnPruningOptimizer","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.column_pruning.ColumnPruningOptimizer","title":"ColumnPruningOptimizer","text":"<p>               Bases: <code>Optimizer</code></p> <p>Prune (skip reading) unused columns</p> <p>Benefits: - Massive I/O reduction for wide tables - Reduces memory usage - Critical for columnar formats (Parquet, ORC) - Can read 10x faster if selecting 1 column from 10</p> Example <p>SELECT name, age FROM employees  -- 100 columns total</p> <p>Without pruning: Read all 100 columns \u2192 Project 2 With pruning: Read only 2 columns \u2192 Much faster</p> Source code in <code>sqlstream/optimizers/column_pruning.py</code> <pre><code>class ColumnPruningOptimizer(Optimizer):\n    \"\"\"\n    Prune (skip reading) unused columns\n\n    Benefits:\n    - Massive I/O reduction for wide tables\n    - Reduces memory usage\n    - Critical for columnar formats (Parquet, ORC)\n    - Can read 10x faster if selecting 1 column from 10\n\n    Example:\n        SELECT name, age FROM employees  -- 100 columns total\n\n        Without pruning: Read all 100 columns \u2192 Project 2\n        With pruning: Read only 2 columns \u2192 Much faster\n    \"\"\"\n\n    def get_name(self) -&gt; str:\n        return \"Column pruning\"\n\n    def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n        \"\"\"\n        Check if column pruning is applicable\n\n        Conditions:\n        1. Reader supports column selection\n        2. Not SELECT * (can't prune if all columns needed)\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n\n        Returns:\n            True if optimization can be applied\n        \"\"\"\n        # Reader must support column selection\n        if not reader.supports_column_selection():\n            return False\n\n        # Can't prune with SELECT *\n        if \"*\" in ast.columns:\n            return False\n\n        return True\n\n    def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n        \"\"\"\n        Apply column pruning optimization\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n        \"\"\"\n        # Analyze which columns are actually needed\n        needed_columns = self._analyze_column_dependencies(ast)\n\n        # Don't apply if SELECT * found during analysis\n        if \"*\" in needed_columns:\n            return\n\n        reader.set_columns(needed_columns)\n        self.applied = True\n        self.description = f\"{len(needed_columns)} column(s) selected\"\n\n    def _analyze_column_dependencies(self, ast: SelectStatement) -&gt; List[str]:\n        \"\"\"\n        Determine which columns are needed for the query\n\n        Columns are needed if they appear in:\n        - SELECT clause\n        - WHERE clause\n        - GROUP BY clause\n        - ORDER BY clause\n        - Aggregate functions\n        - JOIN conditions\n\n        Args:\n            ast: Parsed SQL statement\n\n        Returns:\n            List of required column names (or ['*'] for all)\n        \"\"\"\n        needed: Set[str] = set()\n\n        # Columns from SELECT clause\n        if \"*\" in ast.columns:\n            return [\"*\"]  # Can't prune if SELECT *\n\n        needed.update(ast.columns)\n\n        # Columns from WHERE clause\n        if ast.where:\n            for condition in ast.where.conditions:\n                needed.add(condition.column)\n\n        # Columns from GROUP BY clause\n        if ast.group_by:\n            needed.update(ast.group_by)\n\n        # Columns from ORDER BY clause\n        if ast.order_by:\n            for order_col in ast.order_by:\n                needed.add(order_col.column)\n\n        # Columns from aggregate functions\n        if ast.aggregates:\n            for agg in ast.aggregates:\n                if agg.column != \"*\":  # COUNT(*) doesn't need a column\n                    needed.add(agg.column)\n\n        # Columns from JOIN conditions\n        if ast.join:\n            # Need the left join key from the left table\n            needed.add(ast.join.on_left)\n            # Note: right join key is from right table, handled separately\n\n        return list(needed)\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.column_pruning.ColumnPruningOptimizer.can_optimize","title":"can_optimize","text":"<pre><code>can_optimize(ast: SelectStatement, reader: BaseReader) -&gt; bool\n</code></pre> <p>Check if column pruning is applicable</p> <p>Conditions: 1. Reader supports column selection 2. Not SELECT * (can't prune if all columns needed)</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if optimization can be applied</p> Source code in <code>sqlstream/optimizers/column_pruning.py</code> <pre><code>def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n    \"\"\"\n    Check if column pruning is applicable\n\n    Conditions:\n    1. Reader supports column selection\n    2. Not SELECT * (can't prune if all columns needed)\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n\n    Returns:\n        True if optimization can be applied\n    \"\"\"\n    # Reader must support column selection\n    if not reader.supports_column_selection():\n        return False\n\n    # Can't prune with SELECT *\n    if \"*\" in ast.columns:\n        return False\n\n    return True\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.column_pruning.ColumnPruningOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(ast: SelectStatement, reader: BaseReader) -&gt; None\n</code></pre> <p>Apply column pruning optimization</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required Source code in <code>sqlstream/optimizers/column_pruning.py</code> <pre><code>def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n    \"\"\"\n    Apply column pruning optimization\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n    \"\"\"\n    # Analyze which columns are actually needed\n    needed_columns = self._analyze_column_dependencies(ast)\n\n    # Don't apply if SELECT * found during analysis\n    if \"*\" in needed_columns:\n        return\n\n    reader.set_columns(needed_columns)\n    self.applied = True\n    self.description = f\"{len(needed_columns)} column(s) selected\"\n</code></pre>"},{"location":"api/reference/optimizers/#columnstatistics","title":"ColumnStatistics","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.cost_based.ColumnStatistics","title":"ColumnStatistics  <code>dataclass</code>","text":"<p>Statistics about a single column</p> <p>Attributes:</p> Name Type Description <code>distinct_count</code> <code>int</code> <p>Number of distinct values (cardinality)</p> <code>null_count</code> <code>int</code> <p>Number of NULL values</p> <code>min_value</code> <code>Any</code> <p>Minimum value</p> <code>max_value</code> <code>Any</code> <p>Maximum value</p> <code>avg_length</code> <code>float</code> <p>Average value length (for strings)</p> Source code in <code>sqlstream/optimizers/cost_based.py</code> <pre><code>@dataclass\nclass ColumnStatistics:\n    \"\"\"\n    Statistics about a single column\n\n    Attributes:\n        distinct_count: Number of distinct values (cardinality)\n        null_count: Number of NULL values\n        min_value: Minimum value\n        max_value: Maximum value\n        avg_length: Average value length (for strings)\n    \"\"\"\n\n    distinct_count: int = 0\n    null_count: int = 0\n    min_value: Any = None\n    max_value: Any = None\n    avg_length: float = 0.0\n</code></pre>"},{"location":"api/reference/optimizers/#costbasedoptimizer","title":"CostBasedOptimizer","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.cost_based.CostBasedOptimizer","title":"CostBasedOptimizer","text":"<p>               Bases: <code>Optimizer</code></p> <p>Cost-based optimization framework</p> <p>This is a meta-optimizer that provides infrastructure for cost-based decisions in other optimizers.</p> <p>Benefits: - Statistics-driven decisions - Better join ordering - Better index selection (future) - Adaptive query execution (future)</p> Note <p>This is a framework/placeholder. Real cost-based optimization requires statistics collection, which is expensive. For now, we just provide the infrastructure and simple cost models.</p> Source code in <code>sqlstream/optimizers/cost_based.py</code> <pre><code>class CostBasedOptimizer(Optimizer):\n    \"\"\"\n    Cost-based optimization framework\n\n    This is a meta-optimizer that provides infrastructure for\n    cost-based decisions in other optimizers.\n\n    Benefits:\n    - Statistics-driven decisions\n    - Better join ordering\n    - Better index selection (future)\n    - Adaptive query execution (future)\n\n    Note:\n        This is a framework/placeholder. Real cost-based optimization\n        requires statistics collection, which is expensive. For now,\n        we just provide the infrastructure and simple cost models.\n    \"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.statistics_cache: Dict[str, TableStatistics] = {}\n\n    def get_name(self) -&gt; str:\n        return \"Cost-based optimization\"\n\n    def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n        \"\"\"\n        Check if cost-based optimization is applicable\n\n        For now, this is disabled as it requires statistics collection.\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n\n        Returns:\n            False (disabled for now)\n        \"\"\"\n        # Cost-based optimization requires:\n        # 1. Statistics collection (expensive - need to scan data)\n        # 2. Cost models for all operations\n        # 3. Plan enumeration and comparison\n        # 4. Plan selection\n\n        # This is complex and expensive, so we disable it for now\n        return False\n\n    def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n        \"\"\"\n        Apply cost-based optimizations\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n\n        Note:\n            This is a placeholder for future implementation\n        \"\"\"\n        # Future implementation would:\n        # 1. Collect or lookup table statistics\n        # 2. Estimate costs for different query plans\n        # 3. Choose the lowest-cost plan\n        # 4. Rewrite AST to execute chosen plan\n\n        self.applied = True\n        self.description = \"framework ready (not yet active)\"\n\n    def collect_statistics(self, reader: BaseReader, sample_size: int = 1000) -&gt; TableStatistics:\n        \"\"\"\n        Collect statistics from a data source\n\n        Args:\n            reader: Data source to collect stats from\n            sample_size: Number of rows to sample (for efficiency)\n\n        Returns:\n            Table statistics\n\n        Note:\n            This is expensive - requires reading data\n            In production, stats would be cached and updated periodically\n        \"\"\"\n        stats = TableStatistics()\n\n        # Sample rows\n        rows_sampled = 0\n        column_values: Dict[str, set] = {}\n\n        for row in reader.read_lazy():\n            rows_sampled += 1\n\n            # Track distinct values per column\n            for col, value in row.items():\n                if col not in column_values:\n                    column_values[col] = set()\n                column_values[col].add(value)\n\n            if rows_sampled &gt;= sample_size:\n                break\n\n        # Estimate total row count (extrapolate from sample)\n        # This is a rough estimate - real implementation would use metadata\n        stats.row_count = rows_sampled\n\n        # Calculate column statistics\n        for col, values in column_values.items():\n            col_stats = ColumnStatistics(\n                distinct_count=len(values),\n                null_count=sum(1 for v in values if v is None),\n            )\n\n            # Calculate min/max if comparable\n            non_null_values = [v for v in values if v is not None]\n            if non_null_values:\n                try:\n                    col_stats.min_value = min(non_null_values)\n                    col_stats.max_value = max(non_null_values)\n                except TypeError:\n                    # Values not comparable\n                    pass\n\n            stats.column_stats[col] = col_stats\n\n        return stats\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.cost_based.CostBasedOptimizer.can_optimize","title":"can_optimize","text":"<pre><code>can_optimize(ast: SelectStatement, reader: BaseReader) -&gt; bool\n</code></pre> <p>Check if cost-based optimization is applicable</p> <p>For now, this is disabled as it requires statistics collection.</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required <p>Returns:</p> Type Description <code>bool</code> <p>False (disabled for now)</p> Source code in <code>sqlstream/optimizers/cost_based.py</code> <pre><code>def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n    \"\"\"\n    Check if cost-based optimization is applicable\n\n    For now, this is disabled as it requires statistics collection.\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n\n    Returns:\n        False (disabled for now)\n    \"\"\"\n    # Cost-based optimization requires:\n    # 1. Statistics collection (expensive - need to scan data)\n    # 2. Cost models for all operations\n    # 3. Plan enumeration and comparison\n    # 4. Plan selection\n\n    # This is complex and expensive, so we disable it for now\n    return False\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.cost_based.CostBasedOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(ast: SelectStatement, reader: BaseReader) -&gt; None\n</code></pre> <p>Apply cost-based optimizations</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required Note <p>This is a placeholder for future implementation</p> Source code in <code>sqlstream/optimizers/cost_based.py</code> <pre><code>def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n    \"\"\"\n    Apply cost-based optimizations\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n\n    Note:\n        This is a placeholder for future implementation\n    \"\"\"\n    # Future implementation would:\n    # 1. Collect or lookup table statistics\n    # 2. Estimate costs for different query plans\n    # 3. Choose the lowest-cost plan\n    # 4. Rewrite AST to execute chosen plan\n\n    self.applied = True\n    self.description = \"framework ready (not yet active)\"\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.cost_based.CostBasedOptimizer.collect_statistics","title":"collect_statistics","text":"<pre><code>collect_statistics(reader: BaseReader, sample_size: int = 1000) -&gt; TableStatistics\n</code></pre> <p>Collect statistics from a data source</p> <p>Parameters:</p> Name Type Description Default <code>reader</code> <code>BaseReader</code> <p>Data source to collect stats from</p> required <code>sample_size</code> <code>int</code> <p>Number of rows to sample (for efficiency)</p> <code>1000</code> <p>Returns:</p> Type Description <code>TableStatistics</code> <p>Table statistics</p> Note <p>This is expensive - requires reading data In production, stats would be cached and updated periodically</p> Source code in <code>sqlstream/optimizers/cost_based.py</code> <pre><code>def collect_statistics(self, reader: BaseReader, sample_size: int = 1000) -&gt; TableStatistics:\n    \"\"\"\n    Collect statistics from a data source\n\n    Args:\n        reader: Data source to collect stats from\n        sample_size: Number of rows to sample (for efficiency)\n\n    Returns:\n        Table statistics\n\n    Note:\n        This is expensive - requires reading data\n        In production, stats would be cached and updated periodically\n    \"\"\"\n    stats = TableStatistics()\n\n    # Sample rows\n    rows_sampled = 0\n    column_values: Dict[str, set] = {}\n\n    for row in reader.read_lazy():\n        rows_sampled += 1\n\n        # Track distinct values per column\n        for col, value in row.items():\n            if col not in column_values:\n                column_values[col] = set()\n            column_values[col].add(value)\n\n        if rows_sampled &gt;= sample_size:\n            break\n\n    # Estimate total row count (extrapolate from sample)\n    # This is a rough estimate - real implementation would use metadata\n    stats.row_count = rows_sampled\n\n    # Calculate column statistics\n    for col, values in column_values.items():\n        col_stats = ColumnStatistics(\n            distinct_count=len(values),\n            null_count=sum(1 for v in values if v is None),\n        )\n\n        # Calculate min/max if comparable\n        non_null_values = [v for v in values if v is not None]\n        if non_null_values:\n            try:\n                col_stats.min_value = min(non_null_values)\n                col_stats.max_value = max(non_null_values)\n            except TypeError:\n                # Values not comparable\n                pass\n\n        stats.column_stats[col] = col_stats\n\n    return stats\n</code></pre>"},{"location":"api/reference/optimizers/#costmodel","title":"CostModel","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.cost_based.CostModel","title":"CostModel","text":"<p>Cost model for estimating query operation costs</p> <p>Costs are in abstract units. Lower is better. The goal is to compare different plans, not to predict absolute runtime.</p> Source code in <code>sqlstream/optimizers/cost_based.py</code> <pre><code>class CostModel:\n    \"\"\"\n    Cost model for estimating query operation costs\n\n    Costs are in abstract units. Lower is better.\n    The goal is to compare different plans, not to predict absolute runtime.\n    \"\"\"\n\n    # Cost constants (tunable)\n    COST_PER_ROW_SCAN = 1.0  # Cost to read one row\n    COST_PER_ROW_FILTER = 0.1  # Cost to evaluate filter on one row\n    COST_PER_ROW_PROJECT = 0.05  # Cost to project one row\n    COST_PER_ROW_SORT = 2.0  # Cost to sort one row (N log N)\n    COST_PER_ROW_HASH = 1.5  # Cost to hash one row (for joins/groups)\n    COST_PER_ROW_JOIN = 0.5  # Cost to join one row\n\n    @classmethod\n    def estimate_scan_cost(cls, row_count: int) -&gt; float:\n        \"\"\"\n        Estimate cost of scanning a table\n\n        Args:\n            row_count: Number of rows to scan\n\n        Returns:\n            Estimated cost\n        \"\"\"\n        return row_count * cls.COST_PER_ROW_SCAN\n\n    @classmethod\n    def estimate_filter_cost(\n        cls, row_count: int, selectivity: float = 0.1\n    ) -&gt; float:\n        \"\"\"\n        Estimate cost of filtering rows\n\n        Args:\n            row_count: Number of input rows\n            selectivity: Fraction of rows that pass filter (0.0-1.0)\n\n        Returns:\n            Estimated cost\n        \"\"\"\n        # Cost to evaluate filter on all rows\n        filter_cost = row_count * cls.COST_PER_ROW_FILTER\n        # Output row count for downstream operations\n        output_rows = row_count * selectivity\n        return filter_cost\n\n    @classmethod\n    def estimate_join_cost(\n        cls, left_rows: int, right_rows: int, selectivity: float = 0.1\n    ) -&gt; float:\n        \"\"\"\n        Estimate cost of hash join\n\n        Args:\n            left_rows: Number of rows in left table\n            right_rows: Number of rows in right table\n            selectivity: Fraction of cartesian product that matches\n\n        Returns:\n            Estimated cost\n        \"\"\"\n        # Build hash table on smaller table\n        build_rows = min(left_rows, right_rows)\n        probe_rows = max(left_rows, right_rows)\n\n        # Cost to build hash table\n        build_cost = build_rows * cls.COST_PER_ROW_HASH\n\n        # Cost to probe hash table\n        probe_cost = probe_rows * cls.COST_PER_ROW_JOIN\n\n        # Output row count\n        output_rows = left_rows * right_rows * selectivity\n\n        return build_cost + probe_cost\n\n    @classmethod\n    def estimate_sort_cost(cls, row_count: int) -&gt; float:\n        \"\"\"\n        Estimate cost of sorting\n\n        Args:\n            row_count: Number of rows to sort\n\n        Returns:\n            Estimated cost (O(N log N))\n        \"\"\"\n        import math\n\n        if row_count &lt;= 1:\n            return 0.0\n\n        return row_count * math.log2(row_count) * cls.COST_PER_ROW_SORT\n\n    @classmethod\n    def estimate_selectivity(cls, condition: Condition, stats: Optional[ColumnStatistics] = None) -&gt; float:\n        \"\"\"\n        Estimate selectivity of a filter condition\n\n        Args:\n            condition: Filter condition\n            stats: Column statistics (if available)\n\n        Returns:\n            Estimated selectivity (0.0-1.0)\n\n        Note:\n            These are rough heuristics. Real databases use histograms.\n        \"\"\"\n        op = condition.operator\n\n        # Default selectivities (rough heuristics)\n        if op == \"=\":\n            # Equality: depends on cardinality\n            if stats and stats.distinct_count &gt; 0:\n                return 1.0 / stats.distinct_count\n            return 0.1  # Default guess\n\n        elif op in (\"&gt;\", \"&lt;\"):\n            # Range: assume half the rows\n            return 0.5\n\n        elif op in (\"&gt;=\", \"&lt;=\"):\n            # Range: assume half the rows plus equals\n            return 0.5\n\n        elif op == \"!=\":\n            # Not equals: most rows\n            if stats and stats.distinct_count &gt; 0:\n                return 1.0 - (1.0 / stats.distinct_count)\n            return 0.9  # Default guess\n\n        else:\n            # Unknown operator\n            return 0.5  # Middle ground\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.cost_based.CostModel.estimate_scan_cost","title":"estimate_scan_cost  <code>classmethod</code>","text":"<pre><code>estimate_scan_cost(row_count: int) -&gt; float\n</code></pre> <p>Estimate cost of scanning a table</p> <p>Parameters:</p> Name Type Description Default <code>row_count</code> <code>int</code> <p>Number of rows to scan</p> required <p>Returns:</p> Type Description <code>float</code> <p>Estimated cost</p> Source code in <code>sqlstream/optimizers/cost_based.py</code> <pre><code>@classmethod\ndef estimate_scan_cost(cls, row_count: int) -&gt; float:\n    \"\"\"\n    Estimate cost of scanning a table\n\n    Args:\n        row_count: Number of rows to scan\n\n    Returns:\n        Estimated cost\n    \"\"\"\n    return row_count * cls.COST_PER_ROW_SCAN\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.cost_based.CostModel.estimate_filter_cost","title":"estimate_filter_cost  <code>classmethod</code>","text":"<pre><code>estimate_filter_cost(row_count: int, selectivity: float = 0.1) -&gt; float\n</code></pre> <p>Estimate cost of filtering rows</p> <p>Parameters:</p> Name Type Description Default <code>row_count</code> <code>int</code> <p>Number of input rows</p> required <code>selectivity</code> <code>float</code> <p>Fraction of rows that pass filter (0.0-1.0)</p> <code>0.1</code> <p>Returns:</p> Type Description <code>float</code> <p>Estimated cost</p> Source code in <code>sqlstream/optimizers/cost_based.py</code> <pre><code>@classmethod\ndef estimate_filter_cost(\n    cls, row_count: int, selectivity: float = 0.1\n) -&gt; float:\n    \"\"\"\n    Estimate cost of filtering rows\n\n    Args:\n        row_count: Number of input rows\n        selectivity: Fraction of rows that pass filter (0.0-1.0)\n\n    Returns:\n        Estimated cost\n    \"\"\"\n    # Cost to evaluate filter on all rows\n    filter_cost = row_count * cls.COST_PER_ROW_FILTER\n    # Output row count for downstream operations\n    output_rows = row_count * selectivity\n    return filter_cost\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.cost_based.CostModel.estimate_join_cost","title":"estimate_join_cost  <code>classmethod</code>","text":"<pre><code>estimate_join_cost(left_rows: int, right_rows: int, selectivity: float = 0.1) -&gt; float\n</code></pre> <p>Estimate cost of hash join</p> <p>Parameters:</p> Name Type Description Default <code>left_rows</code> <code>int</code> <p>Number of rows in left table</p> required <code>right_rows</code> <code>int</code> <p>Number of rows in right table</p> required <code>selectivity</code> <code>float</code> <p>Fraction of cartesian product that matches</p> <code>0.1</code> <p>Returns:</p> Type Description <code>float</code> <p>Estimated cost</p> Source code in <code>sqlstream/optimizers/cost_based.py</code> <pre><code>@classmethod\ndef estimate_join_cost(\n    cls, left_rows: int, right_rows: int, selectivity: float = 0.1\n) -&gt; float:\n    \"\"\"\n    Estimate cost of hash join\n\n    Args:\n        left_rows: Number of rows in left table\n        right_rows: Number of rows in right table\n        selectivity: Fraction of cartesian product that matches\n\n    Returns:\n        Estimated cost\n    \"\"\"\n    # Build hash table on smaller table\n    build_rows = min(left_rows, right_rows)\n    probe_rows = max(left_rows, right_rows)\n\n    # Cost to build hash table\n    build_cost = build_rows * cls.COST_PER_ROW_HASH\n\n    # Cost to probe hash table\n    probe_cost = probe_rows * cls.COST_PER_ROW_JOIN\n\n    # Output row count\n    output_rows = left_rows * right_rows * selectivity\n\n    return build_cost + probe_cost\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.cost_based.CostModel.estimate_sort_cost","title":"estimate_sort_cost  <code>classmethod</code>","text":"<pre><code>estimate_sort_cost(row_count: int) -&gt; float\n</code></pre> <p>Estimate cost of sorting</p> <p>Parameters:</p> Name Type Description Default <code>row_count</code> <code>int</code> <p>Number of rows to sort</p> required <p>Returns:</p> Type Description <code>float</code> <p>Estimated cost (O(N log N))</p> Source code in <code>sqlstream/optimizers/cost_based.py</code> <pre><code>@classmethod\ndef estimate_sort_cost(cls, row_count: int) -&gt; float:\n    \"\"\"\n    Estimate cost of sorting\n\n    Args:\n        row_count: Number of rows to sort\n\n    Returns:\n        Estimated cost (O(N log N))\n    \"\"\"\n    import math\n\n    if row_count &lt;= 1:\n        return 0.0\n\n    return row_count * math.log2(row_count) * cls.COST_PER_ROW_SORT\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.cost_based.CostModel.estimate_selectivity","title":"estimate_selectivity  <code>classmethod</code>","text":"<pre><code>estimate_selectivity(condition: Condition, stats: Optional[ColumnStatistics] = None) -&gt; float\n</code></pre> <p>Estimate selectivity of a filter condition</p> <p>Parameters:</p> Name Type Description Default <code>condition</code> <code>Condition</code> <p>Filter condition</p> required <code>stats</code> <code>Optional[ColumnStatistics]</code> <p>Column statistics (if available)</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>Estimated selectivity (0.0-1.0)</p> Note <p>These are rough heuristics. Real databases use histograms.</p> Source code in <code>sqlstream/optimizers/cost_based.py</code> <pre><code>@classmethod\ndef estimate_selectivity(cls, condition: Condition, stats: Optional[ColumnStatistics] = None) -&gt; float:\n    \"\"\"\n    Estimate selectivity of a filter condition\n\n    Args:\n        condition: Filter condition\n        stats: Column statistics (if available)\n\n    Returns:\n        Estimated selectivity (0.0-1.0)\n\n    Note:\n        These are rough heuristics. Real databases use histograms.\n    \"\"\"\n    op = condition.operator\n\n    # Default selectivities (rough heuristics)\n    if op == \"=\":\n        # Equality: depends on cardinality\n        if stats and stats.distinct_count &gt; 0:\n            return 1.0 / stats.distinct_count\n        return 0.1  # Default guess\n\n    elif op in (\"&gt;\", \"&lt;\"):\n        # Range: assume half the rows\n        return 0.5\n\n    elif op in (\"&gt;=\", \"&lt;=\"):\n        # Range: assume half the rows plus equals\n        return 0.5\n\n    elif op == \"!=\":\n        # Not equals: most rows\n        if stats and stats.distinct_count &gt; 0:\n            return 1.0 - (1.0 / stats.distinct_count)\n        return 0.9  # Default guess\n\n    else:\n        # Unknown operator\n        return 0.5  # Middle ground\n</code></pre>"},{"location":"api/reference/optimizers/#tablestatistics","title":"TableStatistics","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.cost_based.TableStatistics","title":"TableStatistics  <code>dataclass</code>","text":"<p>Statistics about a table/data source</p> <p>Attributes:</p> Name Type Description <code>row_count</code> <code>int</code> <p>Total number of rows</p> <code>column_stats</code> <code>Dict[str, ColumnStatistics]</code> <p>Per-column statistics (cardinality, min/max, nulls)</p> <code>size_bytes</code> <code>int</code> <p>Approximate size in bytes</p> Source code in <code>sqlstream/optimizers/cost_based.py</code> <pre><code>@dataclass\nclass TableStatistics:\n    \"\"\"\n    Statistics about a table/data source\n\n    Attributes:\n        row_count: Total number of rows\n        column_stats: Per-column statistics (cardinality, min/max, nulls)\n        size_bytes: Approximate size in bytes\n    \"\"\"\n\n    row_count: int = 0\n    column_stats: Dict[str, \"ColumnStatistics\"] = None\n    size_bytes: int = 0\n\n    def __post_init__(self):\n        if self.column_stats is None:\n            self.column_stats = {}\n</code></pre>"},{"location":"api/reference/optimizers/#joinreorderingoptimizer","title":"JoinReorderingOptimizer","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.join_reordering.JoinReorderingOptimizer","title":"JoinReorderingOptimizer","text":"<p>               Bases: <code>Optimizer</code></p> <p>Reorder joins to minimize intermediate result size</p> <p>Benefits: - Smaller intermediate results = less memory - Faster execution (less data to process) - Better cache utilization</p> <p>Strategy: - For now: Simple heuristic (join smallest first) - Future: Cost-based with statistics</p> Example <p>Tables: A (1M rows), B (100 rows), C (1K rows)</p> <p>Bad order:  A JOIN B JOIN C \u2192 (1M \u00d7 100) JOIN C = huge intermediate result</p> <p>Good order: B JOIN C JOIN A \u2192 (100 \u00d7 1K) JOIN A = smaller intermediate result</p> Note <p>This is a placeholder implementation. Full join reordering requires table statistics and is complex. For now, we just track that joins could be reordered.</p> Source code in <code>sqlstream/optimizers/join_reordering.py</code> <pre><code>class JoinReorderingOptimizer(Optimizer):\n    \"\"\"\n    Reorder joins to minimize intermediate result size\n\n    Benefits:\n    - Smaller intermediate results = less memory\n    - Faster execution (less data to process)\n    - Better cache utilization\n\n    Strategy:\n    - For now: Simple heuristic (join smallest first)\n    - Future: Cost-based with statistics\n\n    Example:\n        Tables: A (1M rows), B (100 rows), C (1K rows)\n\n        Bad order:  A JOIN B JOIN C\n        \u2192 (1M \u00d7 100) JOIN C = huge intermediate result\n\n        Good order: B JOIN C JOIN A\n        \u2192 (100 \u00d7 1K) JOIN A = smaller intermediate result\n\n    Note:\n        This is a placeholder implementation. Full join reordering\n        requires table statistics and is complex. For now, we\n        just track that joins could be reordered.\n    \"\"\"\n\n    def get_name(self) -&gt; str:\n        return \"Join reordering\"\n\n    def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n        \"\"\"\n        Check if join reordering is applicable\n\n        Conditions:\n        1. Query has JOIN clause\n        2. No circular dependencies in join conditions\n        3. All joins are inner joins (outer joins have order constraints)\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n\n        Returns:\n            True if optimization can be applied\n        \"\"\"\n        # Must have a join\n        if not ast.join:\n            return False\n\n        # For now, we don't actually reorder (placeholder)\n        # Full implementation would need:\n        # - Table statistics (row counts)\n        # - Join selectivity estimation\n        # - Graph analysis to find optimal order\n        # - Preservation of join semantics\n\n        # This is a marker that join reordering could be applied\n        # but we don't implement it yet to avoid breaking correctness\n\n        return False  # Disabled for now\n\n    def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n        \"\"\"\n        Apply join reordering optimization\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n\n        Note:\n            This is a placeholder. Real implementation would:\n            1. Collect table statistics (row counts)\n            2. Estimate join selectivity\n            3. Build join graph\n            4. Find optimal join order (dynamic programming or greedy)\n            5. Rewrite AST with new join order\n            6. Preserve join semantics (INNER vs OUTER)\n        \"\"\"\n        # Placeholder - actual implementation would reorder joins here\n        # For now, just mark as applied if we detected potential\n        self.applied = True\n        self.description = \"placeholder (not yet implemented)\"\n\n    def _analyze_join_graph(self, ast: SelectStatement) -&gt; List[Tuple[str, str]]:\n        \"\"\"\n        Analyze join graph to understand table relationships\n\n        Args:\n            ast: Parsed SQL statement\n\n        Returns:\n            List of (left_table, right_table) pairs\n\n        Note:\n            This is a helper for future implementation\n        \"\"\"\n        edges = []\n\n        if ast.join:\n            # Extract join relationships\n            # This would need to parse join conditions to build graph\n            pass\n\n        return edges\n\n    def _estimate_join_cost(\n        self, left_table: str, right_table: str, selectivity: float = 0.1\n    ) -&gt; float:\n        \"\"\"\n        Estimate cost of joining two tables\n\n        Args:\n            left_table: Name of left table\n            right_table: Name of right table\n            selectivity: Estimated fraction of rows that match (0.0-1.0)\n\n        Returns:\n            Estimated cost (lower is better)\n\n        Note:\n            This is a helper for future implementation\n            Real implementation would use actual table statistics\n        \"\"\"\n        # Placeholder - would need real table statistics\n        # Cost = (left_size * right_size) * selectivity\n        return 0.0\n\n    def _find_optimal_join_order(\n        self, tables: List[str], join_graph: List[Tuple[str, str]]\n    ) -&gt; List[str]:\n        \"\"\"\n        Find optimal order to join tables\n\n        Args:\n            tables: List of table names\n            join_graph: List of (table1, table2) join pairs\n\n        Returns:\n            Optimal order to join tables\n\n        Note:\n            This is a helper for future implementation\n            Classic dynamic programming problem (like traveling salesman)\n        \"\"\"\n        # Placeholder - would implement DP or greedy algorithm\n        # For now, just return original order\n        return tables\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.join_reordering.JoinReorderingOptimizer.can_optimize","title":"can_optimize","text":"<pre><code>can_optimize(ast: SelectStatement, reader: BaseReader) -&gt; bool\n</code></pre> <p>Check if join reordering is applicable</p> <p>Conditions: 1. Query has JOIN clause 2. No circular dependencies in join conditions 3. All joins are inner joins (outer joins have order constraints)</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if optimization can be applied</p> Source code in <code>sqlstream/optimizers/join_reordering.py</code> <pre><code>def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n    \"\"\"\n    Check if join reordering is applicable\n\n    Conditions:\n    1. Query has JOIN clause\n    2. No circular dependencies in join conditions\n    3. All joins are inner joins (outer joins have order constraints)\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n\n    Returns:\n        True if optimization can be applied\n    \"\"\"\n    # Must have a join\n    if not ast.join:\n        return False\n\n    # For now, we don't actually reorder (placeholder)\n    # Full implementation would need:\n    # - Table statistics (row counts)\n    # - Join selectivity estimation\n    # - Graph analysis to find optimal order\n    # - Preservation of join semantics\n\n    # This is a marker that join reordering could be applied\n    # but we don't implement it yet to avoid breaking correctness\n\n    return False  # Disabled for now\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.join_reordering.JoinReorderingOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(ast: SelectStatement, reader: BaseReader) -&gt; None\n</code></pre> <p>Apply join reordering optimization</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required Note <p>This is a placeholder. Real implementation would: 1. Collect table statistics (row counts) 2. Estimate join selectivity 3. Build join graph 4. Find optimal join order (dynamic programming or greedy) 5. Rewrite AST with new join order 6. Preserve join semantics (INNER vs OUTER)</p> Source code in <code>sqlstream/optimizers/join_reordering.py</code> <pre><code>def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n    \"\"\"\n    Apply join reordering optimization\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n\n    Note:\n        This is a placeholder. Real implementation would:\n        1. Collect table statistics (row counts)\n        2. Estimate join selectivity\n        3. Build join graph\n        4. Find optimal join order (dynamic programming or greedy)\n        5. Rewrite AST with new join order\n        6. Preserve join semantics (INNER vs OUTER)\n    \"\"\"\n    # Placeholder - actual implementation would reorder joins here\n    # For now, just mark as applied if we detected potential\n    self.applied = True\n    self.description = \"placeholder (not yet implemented)\"\n</code></pre>"},{"location":"api/reference/optimizers/#limitpushdownoptimizer","title":"LimitPushdownOptimizer","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.limit_pushdown.LimitPushdownOptimizer","title":"LimitPushdownOptimizer","text":"<p>               Bases: <code>Optimizer</code></p> <p>Push LIMIT to the reader for early termination</p> <p>Benefits: - Stop reading after N rows - Massive speedup for large files - Reduces memory usage</p> Example <p>SELECT * FROM large_file.csv LIMIT 10</p> <p>Without pushdown: Read entire file \u2192 Take first 10 With pushdown: Stop reading after 10 rows \u2192 Much faster</p> Note <p>Cannot push down if query has: - ORDER BY (need to see all rows to sort) - GROUP BY (need to see all rows to group) - Aggregates (need all rows to aggregate) - JOIN (complex - may need all rows)</p> Source code in <code>sqlstream/optimizers/limit_pushdown.py</code> <pre><code>class LimitPushdownOptimizer(Optimizer):\n    \"\"\"\n    Push LIMIT to the reader for early termination\n\n    Benefits:\n    - Stop reading after N rows\n    - Massive speedup for large files\n    - Reduces memory usage\n\n    Example:\n        SELECT * FROM large_file.csv LIMIT 10\n\n        Without pushdown: Read entire file \u2192 Take first 10\n        With pushdown: Stop reading after 10 rows \u2192 Much faster\n\n    Note:\n        Cannot push down if query has:\n        - ORDER BY (need to see all rows to sort)\n        - GROUP BY (need to see all rows to group)\n        - Aggregates (need all rows to aggregate)\n        - JOIN (complex - may need all rows)\n    \"\"\"\n\n    def get_name(self) -&gt; str:\n        return \"Limit pushdown\"\n\n    def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n        \"\"\"\n        Check if limit pushdown is applicable\n\n        Conditions:\n        1. Query has LIMIT clause\n        2. Reader supports limit pushdown\n        3. No ORDER BY (would need to read all rows first)\n        4. No GROUP BY (would need to read all rows first)\n        5. No aggregates (would need to read all rows first)\n        6. No JOIN (complex - skip for now)\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n\n        Returns:\n            True if optimization can be applied\n        \"\"\"\n        # Must have LIMIT\n        if ast.limit is None:\n            return False\n\n        # Reader must support limit pushdown\n        if not reader.supports_limit():\n            return False\n\n        # Cannot push down with ORDER BY\n        if ast.order_by:\n            return False\n\n        # Cannot push down with GROUP BY\n        if ast.group_by:\n            return False\n\n        # Cannot push down with aggregates\n        if ast.aggregates:\n            return False\n\n        # Cannot push down with JOIN (for now)\n        if ast.join:\n            return False\n\n        return True\n\n    def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n        \"\"\"\n        Apply limit pushdown optimization\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n        \"\"\"\n        reader.set_limit(ast.limit)\n        self.applied = True\n        self.description = f\"limit {ast.limit}\"\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.limit_pushdown.LimitPushdownOptimizer.can_optimize","title":"can_optimize","text":"<pre><code>can_optimize(ast: SelectStatement, reader: BaseReader) -&gt; bool\n</code></pre> <p>Check if limit pushdown is applicable</p> <p>Conditions: 1. Query has LIMIT clause 2. Reader supports limit pushdown 3. No ORDER BY (would need to read all rows first) 4. No GROUP BY (would need to read all rows first) 5. No aggregates (would need to read all rows first) 6. No JOIN (complex - skip for now)</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if optimization can be applied</p> Source code in <code>sqlstream/optimizers/limit_pushdown.py</code> <pre><code>def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n    \"\"\"\n    Check if limit pushdown is applicable\n\n    Conditions:\n    1. Query has LIMIT clause\n    2. Reader supports limit pushdown\n    3. No ORDER BY (would need to read all rows first)\n    4. No GROUP BY (would need to read all rows first)\n    5. No aggregates (would need to read all rows first)\n    6. No JOIN (complex - skip for now)\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n\n    Returns:\n        True if optimization can be applied\n    \"\"\"\n    # Must have LIMIT\n    if ast.limit is None:\n        return False\n\n    # Reader must support limit pushdown\n    if not reader.supports_limit():\n        return False\n\n    # Cannot push down with ORDER BY\n    if ast.order_by:\n        return False\n\n    # Cannot push down with GROUP BY\n    if ast.group_by:\n        return False\n\n    # Cannot push down with aggregates\n    if ast.aggregates:\n        return False\n\n    # Cannot push down with JOIN (for now)\n    if ast.join:\n        return False\n\n    return True\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.limit_pushdown.LimitPushdownOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(ast: SelectStatement, reader: BaseReader) -&gt; None\n</code></pre> <p>Apply limit pushdown optimization</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required Source code in <code>sqlstream/optimizers/limit_pushdown.py</code> <pre><code>def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n    \"\"\"\n    Apply limit pushdown optimization\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n    \"\"\"\n    reader.set_limit(ast.limit)\n    self.applied = True\n    self.description = f\"limit {ast.limit}\"\n</code></pre>"},{"location":"api/reference/optimizers/#partitionpruningoptimizer","title":"PartitionPruningOptimizer","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.partition_pruning.PartitionPruningOptimizer","title":"PartitionPruningOptimizer","text":"<p>               Bases: <code>Optimizer</code></p> <p>Prune (skip) partitions that don't match filter conditions</p> <p>Benefits: - Massive I/O reduction for partitioned datasets - Skip entire directories/files - Critical for data lakes and big data - Can reduce data read by 10x-1000x</p> Example <p>Dataset partitioned by date: year=YYYY/month=MM/day=DD/ Query: WHERE date &gt;= '2024-01-01' \u2192 Skip all partitions before 2024</p> Source code in <code>sqlstream/optimizers/partition_pruning.py</code> <pre><code>class PartitionPruningOptimizer(Optimizer):\n    \"\"\"\n    Prune (skip) partitions that don't match filter conditions\n\n    Benefits:\n    - Massive I/O reduction for partitioned datasets\n    - Skip entire directories/files\n    - Critical for data lakes and big data\n    - Can reduce data read by 10x-1000x\n\n    Example:\n        Dataset partitioned by date: year=YYYY/month=MM/day=DD/\n        Query: WHERE date &gt;= '2024-01-01'\n        \u2192 Skip all partitions before 2024\n    \"\"\"\n\n    def get_name(self) -&gt; str:\n        return \"Partition pruning\"\n\n    def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n        \"\"\"\n        Check if partition pruning is applicable\n\n        Conditions:\n        1. Reader supports partition pruning\n        2. Query has WHERE clause\n        3. WHERE clause references partition columns\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n\n        Returns:\n            True if optimization can be applied\n        \"\"\"\n        # Reader must support partition pruning\n        if not hasattr(reader, 'supports_partition_pruning'):\n            return False\n\n        if not reader.supports_partition_pruning():\n            return False\n\n        # Must have WHERE clause\n        if not ast.where:\n            return False\n\n        # Check if any filter conditions reference partition columns\n        partition_cols = reader.get_partition_columns()\n        if not partition_cols:\n            return False\n\n        filter_cols = {cond.column for cond in ast.where.conditions}\n        if not filter_cols.intersection(partition_cols):\n            return False\n\n        return True\n\n    def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n        \"\"\"\n        Apply partition pruning optimization\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n        \"\"\"\n        # Extract conditions that reference partition columns\n        partition_cols = reader.get_partition_columns()\n        partition_filters = []\n        non_partition_filters = []\n\n        for cond in ast.where.conditions:\n            if cond.column in partition_cols:\n                partition_filters.append(cond)\n            else:\n                non_partition_filters.append(cond)\n\n        if partition_filters:\n            reader.set_partition_filters(partition_filters)\n\n            # IMPORTANT: Remove partition filters from WHERE clause\n            # Partition columns are virtual (from directory path) and don't exist in data\n            # They should only be used for partition pruning, not row-level filtering\n            ast.where.conditions = non_partition_filters\n\n            self.applied = True\n            self.description = f\"{len(partition_filters)} partition filter(s)\"\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.partition_pruning.PartitionPruningOptimizer.can_optimize","title":"can_optimize","text":"<pre><code>can_optimize(ast: SelectStatement, reader: BaseReader) -&gt; bool\n</code></pre> <p>Check if partition pruning is applicable</p> <p>Conditions: 1. Reader supports partition pruning 2. Query has WHERE clause 3. WHERE clause references partition columns</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if optimization can be applied</p> Source code in <code>sqlstream/optimizers/partition_pruning.py</code> <pre><code>def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n    \"\"\"\n    Check if partition pruning is applicable\n\n    Conditions:\n    1. Reader supports partition pruning\n    2. Query has WHERE clause\n    3. WHERE clause references partition columns\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n\n    Returns:\n        True if optimization can be applied\n    \"\"\"\n    # Reader must support partition pruning\n    if not hasattr(reader, 'supports_partition_pruning'):\n        return False\n\n    if not reader.supports_partition_pruning():\n        return False\n\n    # Must have WHERE clause\n    if not ast.where:\n        return False\n\n    # Check if any filter conditions reference partition columns\n    partition_cols = reader.get_partition_columns()\n    if not partition_cols:\n        return False\n\n    filter_cols = {cond.column for cond in ast.where.conditions}\n    if not filter_cols.intersection(partition_cols):\n        return False\n\n    return True\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.partition_pruning.PartitionPruningOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(ast: SelectStatement, reader: BaseReader) -&gt; None\n</code></pre> <p>Apply partition pruning optimization</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required Source code in <code>sqlstream/optimizers/partition_pruning.py</code> <pre><code>def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n    \"\"\"\n    Apply partition pruning optimization\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n    \"\"\"\n    # Extract conditions that reference partition columns\n    partition_cols = reader.get_partition_columns()\n    partition_filters = []\n    non_partition_filters = []\n\n    for cond in ast.where.conditions:\n        if cond.column in partition_cols:\n            partition_filters.append(cond)\n        else:\n            non_partition_filters.append(cond)\n\n    if partition_filters:\n        reader.set_partition_filters(partition_filters)\n\n        # IMPORTANT: Remove partition filters from WHERE clause\n        # Partition columns are virtual (from directory path) and don't exist in data\n        # They should only be used for partition pruning, not row-level filtering\n        ast.where.conditions = non_partition_filters\n\n        self.applied = True\n        self.description = f\"{len(partition_filters)} partition filter(s)\"\n</code></pre>"},{"location":"api/reference/optimizers/#queryplanner","title":"QueryPlanner","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.planner.QueryPlanner","title":"QueryPlanner","text":"<p>Query planner and optimizer orchestrator</p> <p>Applies a pipeline of optimizations to improve query performance: 1. Join reordering - optimize join order for performance 2. Partition pruning - skip entire partitions/files based on filters 3. Predicate pushdown - push WHERE filters to readers 4. Column pruning - tell readers which columns to read 5. Limit pushdown - early termination for LIMIT queries 6. Projection pushdown - push computed expressions (future)</p> <p>The planner modifies the reader in-place with optimization hints.</p> Example <pre><code>planner = QueryPlanner()\nplanner.optimize(ast, reader)\nprint(planner.get_optimization_summary())\n</code></pre> Source code in <code>sqlstream/optimizers/planner.py</code> <pre><code>class QueryPlanner:\n    \"\"\"\n    Query planner and optimizer orchestrator\n\n    Applies a pipeline of optimizations to improve query performance:\n    1. Join reordering - optimize join order for performance\n    2. Partition pruning - skip entire partitions/files based on filters\n    3. Predicate pushdown - push WHERE filters to readers\n    4. Column pruning - tell readers which columns to read\n    5. Limit pushdown - early termination for LIMIT queries\n    6. Projection pushdown - push computed expressions (future)\n\n    The planner modifies the reader in-place with optimization hints.\n\n    Example:\n        ```python\n        planner = QueryPlanner()\n        planner.optimize(ast, reader)\n        print(planner.get_optimization_summary())\n        ```\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialize planner with default optimization pipeline\n\n        The order matters:\n        1. Join reordering first (affects join execution plan)\n        2. Partition pruning second (can skip entire files!)\n        3. Predicate pushdown third (reduces data read)\n        4. Column pruning fourth (narrows columns)\n        5. Limit pushdown fifth (early termination)\n        6. Projection pushdown last (transform data at source)\n        \"\"\"\n        self.pipeline = OptimizerPipeline(\n            [\n                JoinReorderingOptimizer(),\n                PartitionPruningOptimizer(),\n                PredicatePushdownOptimizer(),\n                ColumnPruningOptimizer(),\n                LimitPushdownOptimizer(),\n                ProjectionPushdownOptimizer(),\n            ]\n        )\n        # For backward compatibility with old API\n        self.optimizations_applied: List[str] = []\n\n    def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n        \"\"\"\n        Apply all applicable optimizations\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n\n        Modifies:\n            - reader: Sets optimization hints (filters, columns, limit, etc.)\n            - self.optimizations_applied: List of applied optimizations\n        \"\"\"\n        # Run the optimization pipeline\n        self.pipeline.optimize(ast, reader)\n\n        # Update the backward-compatible optimizations_applied list\n        self.optimizations_applied = self.pipeline.get_applied_optimizations()\n\n    def get_optimization_summary(self) -&gt; str:\n        \"\"\"\n        Get summary of optimizations applied\n\n        Returns:\n            Human-readable summary\n\n        Example:\n            \"Optimizations applied:\n             - Predicate pushdown: 2 condition(s)\n             - Column pruning: 3 column(s) selected\"\n        \"\"\"\n        return self.pipeline.get_summary()\n\n    def get_optimizers(self) -&gt; List:\n        \"\"\"\n        Get list of all optimizers in the pipeline\n\n        Returns:\n            List of optimizer instances\n        \"\"\"\n        return self.pipeline.optimizers\n\n    def add_optimizer(self, optimizer) -&gt; None:\n        \"\"\"\n        Add a custom optimizer to the pipeline\n\n        Args:\n            optimizer: Optimizer instance to add\n\n        Example:\n            ```python\n            planner = QueryPlanner()\n            planner.add_optimizer(MyCustomOptimizer())\n            ```\n        \"\"\"\n        self.pipeline.optimizers.append(optimizer)\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.planner.QueryPlanner.__init__","title":"__init__","text":"<pre><code>__init__()\n</code></pre> <p>Initialize planner with default optimization pipeline</p> <p>The order matters: 1. Join reordering first (affects join execution plan) 2. Partition pruning second (can skip entire files!) 3. Predicate pushdown third (reduces data read) 4. Column pruning fourth (narrows columns) 5. Limit pushdown fifth (early termination) 6. Projection pushdown last (transform data at source)</p> Source code in <code>sqlstream/optimizers/planner.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize planner with default optimization pipeline\n\n    The order matters:\n    1. Join reordering first (affects join execution plan)\n    2. Partition pruning second (can skip entire files!)\n    3. Predicate pushdown third (reduces data read)\n    4. Column pruning fourth (narrows columns)\n    5. Limit pushdown fifth (early termination)\n    6. Projection pushdown last (transform data at source)\n    \"\"\"\n    self.pipeline = OptimizerPipeline(\n        [\n            JoinReorderingOptimizer(),\n            PartitionPruningOptimizer(),\n            PredicatePushdownOptimizer(),\n            ColumnPruningOptimizer(),\n            LimitPushdownOptimizer(),\n            ProjectionPushdownOptimizer(),\n        ]\n    )\n    # For backward compatibility with old API\n    self.optimizations_applied: List[str] = []\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.planner.QueryPlanner.optimize","title":"optimize","text":"<pre><code>optimize(ast: SelectStatement, reader: BaseReader) -&gt; None\n</code></pre> <p>Apply all applicable optimizations</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required Modifies <ul> <li>reader: Sets optimization hints (filters, columns, limit, etc.)</li> <li>self.optimizations_applied: List of applied optimizations</li> </ul> Source code in <code>sqlstream/optimizers/planner.py</code> <pre><code>def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n    \"\"\"\n    Apply all applicable optimizations\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n\n    Modifies:\n        - reader: Sets optimization hints (filters, columns, limit, etc.)\n        - self.optimizations_applied: List of applied optimizations\n    \"\"\"\n    # Run the optimization pipeline\n    self.pipeline.optimize(ast, reader)\n\n    # Update the backward-compatible optimizations_applied list\n    self.optimizations_applied = self.pipeline.get_applied_optimizations()\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.planner.QueryPlanner.get_optimization_summary","title":"get_optimization_summary","text":"<pre><code>get_optimization_summary() -&gt; str\n</code></pre> <p>Get summary of optimizations applied</p> <p>Returns:</p> Type Description <code>str</code> <p>Human-readable summary</p> Example <p>\"Optimizations applied:  - Predicate pushdown: 2 condition(s)  - Column pruning: 3 column(s) selected\"</p> Source code in <code>sqlstream/optimizers/planner.py</code> <pre><code>def get_optimization_summary(self) -&gt; str:\n    \"\"\"\n    Get summary of optimizations applied\n\n    Returns:\n        Human-readable summary\n\n    Example:\n        \"Optimizations applied:\n         - Predicate pushdown: 2 condition(s)\n         - Column pruning: 3 column(s) selected\"\n    \"\"\"\n    return self.pipeline.get_summary()\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.planner.QueryPlanner.get_optimizers","title":"get_optimizers","text":"<pre><code>get_optimizers() -&gt; List\n</code></pre> <p>Get list of all optimizers in the pipeline</p> <p>Returns:</p> Type Description <code>List</code> <p>List of optimizer instances</p> Source code in <code>sqlstream/optimizers/planner.py</code> <pre><code>def get_optimizers(self) -&gt; List:\n    \"\"\"\n    Get list of all optimizers in the pipeline\n\n    Returns:\n        List of optimizer instances\n    \"\"\"\n    return self.pipeline.optimizers\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.planner.QueryPlanner.add_optimizer","title":"add_optimizer","text":"<pre><code>add_optimizer(optimizer) -&gt; None\n</code></pre> <p>Add a custom optimizer to the pipeline</p> <p>Parameters:</p> Name Type Description Default <code>optimizer</code> <p>Optimizer instance to add</p> required Example <pre><code>planner = QueryPlanner()\nplanner.add_optimizer(MyCustomOptimizer())\n</code></pre> Source code in <code>sqlstream/optimizers/planner.py</code> <pre><code>def add_optimizer(self, optimizer) -&gt; None:\n    \"\"\"\n    Add a custom optimizer to the pipeline\n\n    Args:\n        optimizer: Optimizer instance to add\n\n    Example:\n        ```python\n        planner = QueryPlanner()\n        planner.add_optimizer(MyCustomOptimizer())\n        ```\n    \"\"\"\n    self.pipeline.optimizers.append(optimizer)\n</code></pre>"},{"location":"api/reference/optimizers/#predicatepushdownoptimizer","title":"PredicatePushdownOptimizer","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.predicate_pushdown.PredicatePushdownOptimizer","title":"PredicatePushdownOptimizer","text":"<p>               Bases: <code>Optimizer</code></p> <p>Push WHERE conditions to the reader</p> <p>Benefits: - Reduces I/O by filtering at the source - Reduces memory usage - Especially effective for columnar formats (Parquet) - Can leverage indexes if available</p> Example <p>SELECT * FROM data WHERE age &gt; 30</p> <p>Without pushdown: Read all rows \u2192 Filter in memory With pushdown: Filter while reading \u2192 Less data read</p> Source code in <code>sqlstream/optimizers/predicate_pushdown.py</code> <pre><code>class PredicatePushdownOptimizer(Optimizer):\n    \"\"\"\n    Push WHERE conditions to the reader\n\n    Benefits:\n    - Reduces I/O by filtering at the source\n    - Reduces memory usage\n    - Especially effective for columnar formats (Parquet)\n    - Can leverage indexes if available\n\n    Example:\n        SELECT * FROM data WHERE age &gt; 30\n\n        Without pushdown: Read all rows \u2192 Filter in memory\n        With pushdown: Filter while reading \u2192 Less data read\n    \"\"\"\n\n    def get_name(self) -&gt; str:\n        return \"Predicate pushdown\"\n\n    def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n        \"\"\"\n        Check if predicate pushdown is applicable\n\n        Conditions:\n        1. Query has WHERE clause\n        2. Reader supports pushdown\n        3. Not a JOIN query (complex - needs smarter analysis)\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n\n        Returns:\n            True if optimization can be applied\n        \"\"\"\n        # Must have WHERE clause\n        if not ast.where:\n            return False\n\n        # Reader must support pushdown\n        if not reader.supports_pushdown():\n            return False\n\n        # Skip JOINs for now - WHERE conditions may reference either table\n        # TODO: Make this smarter by analyzing which conditions apply to which table\n        if ast.join:\n            return False\n\n        return True\n\n    def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n        \"\"\"\n        Apply predicate pushdown optimization\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n        \"\"\"\n        # Extract conditions that can be pushed down\n        pushable = self._extract_pushable_conditions(ast.where.conditions)\n\n        if pushable:\n            reader.set_filter(pushable)\n            self.applied = True\n            self.description = f\"{len(pushable)} condition(s)\"\n\n    def _extract_pushable_conditions(\n        self, conditions: List[Condition]\n    ) -&gt; List[Condition]:\n        \"\"\"\n        Determine which conditions can be safely pushed to readers\n\n        Pushable conditions:\n        - Simple column comparisons: column op value\n        - Where value is a literal (not another column or expression)\n\n        NOT pushable (future work):\n        - Complex expressions: LENGTH(name) &gt; 5\n        - Cross-column comparisons: age &gt; salary\n        - User-defined functions\n        - Conditions involving aggregates\n\n        Args:\n            conditions: List of WHERE conditions\n\n        Returns:\n            List of conditions safe to push down\n        \"\"\"\n        pushable = []\n\n        for condition in conditions:\n            # For now, all simple conditions are pushable\n            # Future: Check for complex expressions, UDFs, etc.\n            if self._is_simple_condition(condition):\n                pushable.append(condition)\n\n        return pushable\n\n    def _is_simple_condition(self, condition: Condition) -&gt; bool:\n        \"\"\"\n        Check if condition is a simple column comparison\n\n        Simple conditions:\n        - column = value (literal)\n        - column &gt; value (literal)\n        - column &lt; value (literal)\n        - etc.\n\n        Args:\n            condition: Condition to check\n\n        Returns:\n            True if condition is simple and pushable\n        \"\"\"\n        # For now, all our conditions are simple\n        # Future improvements:\n        # - Check if value is a literal vs expression\n        # - Detect cross-column comparisons\n        # - Detect function calls\n        return True\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.predicate_pushdown.PredicatePushdownOptimizer.can_optimize","title":"can_optimize","text":"<pre><code>can_optimize(ast: SelectStatement, reader: BaseReader) -&gt; bool\n</code></pre> <p>Check if predicate pushdown is applicable</p> <p>Conditions: 1. Query has WHERE clause 2. Reader supports pushdown 3. Not a JOIN query (complex - needs smarter analysis)</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if optimization can be applied</p> Source code in <code>sqlstream/optimizers/predicate_pushdown.py</code> <pre><code>def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n    \"\"\"\n    Check if predicate pushdown is applicable\n\n    Conditions:\n    1. Query has WHERE clause\n    2. Reader supports pushdown\n    3. Not a JOIN query (complex - needs smarter analysis)\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n\n    Returns:\n        True if optimization can be applied\n    \"\"\"\n    # Must have WHERE clause\n    if not ast.where:\n        return False\n\n    # Reader must support pushdown\n    if not reader.supports_pushdown():\n        return False\n\n    # Skip JOINs for now - WHERE conditions may reference either table\n    # TODO: Make this smarter by analyzing which conditions apply to which table\n    if ast.join:\n        return False\n\n    return True\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.predicate_pushdown.PredicatePushdownOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(ast: SelectStatement, reader: BaseReader) -&gt; None\n</code></pre> <p>Apply predicate pushdown optimization</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required Source code in <code>sqlstream/optimizers/predicate_pushdown.py</code> <pre><code>def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n    \"\"\"\n    Apply predicate pushdown optimization\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n    \"\"\"\n    # Extract conditions that can be pushed down\n    pushable = self._extract_pushable_conditions(ast.where.conditions)\n\n    if pushable:\n        reader.set_filter(pushable)\n        self.applied = True\n        self.description = f\"{len(pushable)} condition(s)\"\n</code></pre>"},{"location":"api/reference/optimizers/#projectionpushdownoptimizer","title":"ProjectionPushdownOptimizer","text":""},{"location":"api/reference/optimizers/#sqlstream.optimizers.projection_pushdown.ProjectionPushdownOptimizer","title":"ProjectionPushdownOptimizer","text":"<p>               Bases: <code>Optimizer</code></p> <p>Push projection (SELECT expressions) to the reader</p> <p>Benefits (when implemented): - Evaluate expressions at read time - Reduce data movement - Leverage database/engine native functions</p> <p>Example (future):     SELECT UPPER(name), age * 2 FROM data</p> <pre><code>With pushdown: Reader evaluates UPPER() and age*2\nWithout: Read raw data \u2192 Apply transformations later\n</code></pre> <p>Status: Placeholder - not yet implemented Reason: Requires expression evaluation framework</p> Source code in <code>sqlstream/optimizers/projection_pushdown.py</code> <pre><code>class ProjectionPushdownOptimizer(Optimizer):\n    \"\"\"\n    Push projection (SELECT expressions) to the reader\n\n    Benefits (when implemented):\n    - Evaluate expressions at read time\n    - Reduce data movement\n    - Leverage database/engine native functions\n\n    Example (future):\n        SELECT UPPER(name), age * 2 FROM data\n\n        With pushdown: Reader evaluates UPPER() and age*2\n        Without: Read raw data \u2192 Apply transformations later\n\n    Status: Placeholder - not yet implemented\n    Reason: Requires expression evaluation framework\n    \"\"\"\n\n    def get_name(self) -&gt; str:\n        return \"Projection pushdown\"\n\n    def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n        \"\"\"\n        Check if projection pushdown is applicable\n\n        Currently always returns False as this is not yet implemented.\n\n        Future conditions:\n        1. Reader supports expression evaluation\n        2. Expressions are supported by reader (native functions)\n        3. Not complex nested expressions\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n\n        Returns:\n            False (not yet implemented)\n        \"\"\"\n        # TODO: Implement when we have expression evaluation framework\n        return False\n\n    def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n        \"\"\"\n        Apply projection pushdown optimization\n\n        Currently a no-op placeholder.\n\n        Args:\n            ast: Parsed SQL statement\n            reader: Data source reader\n        \"\"\"\n        # Placeholder for future implementation\n        pass\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.projection_pushdown.ProjectionPushdownOptimizer.can_optimize","title":"can_optimize","text":"<pre><code>can_optimize(ast: SelectStatement, reader: BaseReader) -&gt; bool\n</code></pre> <p>Check if projection pushdown is applicable</p> <p>Currently always returns False as this is not yet implemented.</p> <p>Future conditions: 1. Reader supports expression evaluation 2. Expressions are supported by reader (native functions) 3. Not complex nested expressions</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required <p>Returns:</p> Type Description <code>bool</code> <p>False (not yet implemented)</p> Source code in <code>sqlstream/optimizers/projection_pushdown.py</code> <pre><code>def can_optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; bool:\n    \"\"\"\n    Check if projection pushdown is applicable\n\n    Currently always returns False as this is not yet implemented.\n\n    Future conditions:\n    1. Reader supports expression evaluation\n    2. Expressions are supported by reader (native functions)\n    3. Not complex nested expressions\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n\n    Returns:\n        False (not yet implemented)\n    \"\"\"\n    # TODO: Implement when we have expression evaluation framework\n    return False\n</code></pre>"},{"location":"api/reference/optimizers/#sqlstream.optimizers.projection_pushdown.ProjectionPushdownOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(ast: SelectStatement, reader: BaseReader) -&gt; None\n</code></pre> <p>Apply projection pushdown optimization</p> <p>Currently a no-op placeholder.</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <code>SelectStatement</code> <p>Parsed SQL statement</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required Source code in <code>sqlstream/optimizers/projection_pushdown.py</code> <pre><code>def optimize(self, ast: SelectStatement, reader: BaseReader) -&gt; None:\n    \"\"\"\n    Apply projection pushdown optimization\n\n    Currently a no-op placeholder.\n\n    Args:\n        ast: Parsed SQL statement\n        reader: Data source reader\n    \"\"\"\n    # Placeholder for future implementation\n    pass\n</code></pre>"},{"location":"api/reference/query/","title":"Query API Reference","text":"<p>API documentation for query execution.</p>"},{"location":"api/reference/query/#query","title":"Query","text":""},{"location":"api/reference/query/#sqlstream.core.query.Query","title":"Query","text":"<p>Main query builder class</p> <p>Provides a fluent API for building and executing queries.</p> Source code in <code>sqlstream/core/query.py</code> <pre><code>class Query:\n    \"\"\"\n    Main query builder class\n\n    Provides a fluent API for building and executing queries.\n    \"\"\"\n\n    def __init__(self, source: Optional[str] = None):\n        \"\"\"\n        Initialize query with an optional data source\n\n        Args:\n            source: Optional path to data file or URL. If not provided,\n                   sources will be extracted from the SQL query itself.\n\n        Example:\n            &gt;&gt;&gt; # With explicit source\n            &gt;&gt;&gt; query = Query(\"data.csv\")\n            &gt;&gt;&gt; query = Query(\"/path/to/data.parquet\")\n            &gt;&gt;&gt; query = Query(\"https://example.com/data.csv\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Without source - extracted from SQL\n            &gt;&gt;&gt; query = Query()\n            &gt;&gt;&gt; query.sql(\"SELECT * FROM 'data.csv' WHERE age &gt; 25\")\n        \"\"\"\n        self.source = source\n        self.reader = self._create_reader(source) if source else None\n\n    def _create_reader(self, source: str) -&gt; BaseReader:\n        \"\"\"\n        Auto-detect source type and create appropriate reader\n\n        Supports URL fragments: source#format:table\n\n        Args:\n            source: Path to data file or URL, optionally with #format:table fragment\n\n        Returns:\n            Reader instance for the source\n\n        Raises:\n            ValueError: If file format is not supported\n        \"\"\"\n        from sqlstream.core.fragment_parser import parse_source_fragment\n\n        # Parse URL fragment if present\n        source_path, format_hint, table_hint = parse_source_fragment(source)\n\n        # Check if source is HTTP/HTTPS URL\n        if source_path.startswith((\"http://\", \"https://\")):\n            from sqlstream.readers.http_reader import HTTPReader\n\n            kwargs = {}\n            if format_hint:\n                kwargs['format'] = format_hint\n            if table_hint is not None:\n                kwargs['table'] = table_hint\n            return HTTPReader(source_path, **kwargs)\n\n        path = Path(source_path)\n\n        # Check file extension to determine format\n        suffix = path.suffix.lower()\n\n        # Explicit format from fragment takes precedence\n        if format_hint == 'html' or (not format_hint and suffix in ['.html', '.htm']):\n            from sqlstream.readers.html_reader import HTMLReader\n            table = table_hint if table_hint is not None else 0\n            return HTMLReader(source_path, table=table)\n\n        elif format_hint == 'markdown' or (not format_hint and suffix in ['.md', '.markdown']):\n            from sqlstream.readers.markdown_reader import MarkdownReader\n            table = table_hint if table_hint is not None else 0\n            return MarkdownReader(source_path, table=table)\n\n        elif format_hint == 'parquet' or (not format_hint and suffix == \".parquet\"):\n            from sqlstream.readers.parquet_reader import ParquetReader\n            return ParquetReader(source_path)\n\n        elif format_hint == 'csv' or (not format_hint and suffix == \".csv\"):\n            return CSVReader(source_path)\n\n        else:\n            # Try CSV as default\n            try:\n                return CSVReader(source_path)\n            except Exception as e:\n                raise ValueError(\n                    f\"Unsupported file format: {suffix}. \"\n                    f\"Supported formats: .csv, .parquet, .html, .md\"\n                ) from e\n\n    def sql(\n        self, query: str, backend: Optional[Literal[\"auto\", \"pandas\", \"python\", \"duckdb\"]] = \"auto\"\n    ) -&gt; \"QueryResult\":\n        \"\"\"\n        Execute SQL query on the data source\n\n        Args:\n            query: SQL query string\n            backend: Execution backend to use\n                - \"auto\": Smart selection - uses custom parser for simple queries,\n                         DuckDB for complex queries, pandas if available, else python\n                - \"duckdb\": Force DuckDB backend (full SQL support)\n                - \"pandas\": Force pandas backend (10-100x faster than python)\n                - \"python\": Force pure Python Volcano model (educational)\n\n        Returns:\n            QueryResult object that can be iterated over\n\n        Example:\n            &gt;&gt;&gt; # With explicit source\n            &gt;&gt;&gt; result = query(\"data.csv\").sql(\"SELECT * WHERE age &gt; 25\")\n            &gt;&gt;&gt; for row in result:\n            ...     print(row)\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Without source - from SQL\n            &gt;&gt;&gt; result = query().sql(\"SELECT * FROM 'data.csv' WHERE age &gt; 25\")\n            &gt;&gt;&gt;\n            &gt;&gt;&gt; # Force DuckDB backend for full SQL support\n            &gt;&gt;&gt; result = query(\"data.csv\").sql(\n            ...     \"SELECT *, ROW_NUMBER() OVER (PARTITION BY dept ORDER BY salary DESC) as rank \"\n            ...     \"FROM data\",\n            ...     backend=\"duckdb\"\n            ... )\n        \"\"\"\n        # Note: Backend selection is now handled in QueryResult._select_backend()\n        # to preserve the 'auto' value while still doing smart selection\n\n        # If query can be parsed with custom parser\n        if _can_parse_with_custom_parser(query):\n            # Try to parse with custom parser\n            try:\n                ast = parse(query)\n            except Exception:\n                # Cannot be parsed with custom parser, need DuckDB\n                ast = None\n        else:\n            # Cannot be parsed with custom parser, need DuckDB\n            ast = None\n\n        # Create QueryResult with reader factory for JOIN support\n        return QueryResult(ast=ast, reader=self.reader, reader_factory=self._create_reader,\n                          source=self.source, backend=backend, raw_sql=query)\n\n    def schema(self) -&gt; Optional[Schema]:\n        \"\"\"\n        Get schema information for the data source\n\n        Returns:\n            Schema object with inferred types, or None if schema cannot be inferred\n\n        Example:\n            &gt;&gt;&gt; schema = query(\"data.csv\").schema()\n            &gt;&gt;&gt; print(schema)\n            Schema(name: STRING, age: INTEGER, salary: FLOAT)\n        \"\"\"\n        if not self.reader:\n            raise ValueError(\"Cannot get schema without a source. Provide a source when creating the Query object.\")\n        return self.reader.get_schema()\n</code></pre>"},{"location":"api/reference/query/#sqlstream.core.query.Query.__init__","title":"__init__","text":"<pre><code>__init__(source: Optional[str] = None)\n</code></pre> <p>Initialize query with an optional data source</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Optional[str]</code> <p>Optional path to data file or URL. If not provided,    sources will be extracted from the SQL query itself.</p> <code>None</code> Example Source code in <code>sqlstream/core/query.py</code> <pre><code>def __init__(self, source: Optional[str] = None):\n    \"\"\"\n    Initialize query with an optional data source\n\n    Args:\n        source: Optional path to data file or URL. If not provided,\n               sources will be extracted from the SQL query itself.\n\n    Example:\n        &gt;&gt;&gt; # With explicit source\n        &gt;&gt;&gt; query = Query(\"data.csv\")\n        &gt;&gt;&gt; query = Query(\"/path/to/data.parquet\")\n        &gt;&gt;&gt; query = Query(\"https://example.com/data.csv\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Without source - extracted from SQL\n        &gt;&gt;&gt; query = Query()\n        &gt;&gt;&gt; query.sql(\"SELECT * FROM 'data.csv' WHERE age &gt; 25\")\n    \"\"\"\n    self.source = source\n    self.reader = self._create_reader(source) if source else None\n</code></pre>"},{"location":"api/reference/query/#sqlstream.core.query.Query.__init__--with-explicit-source","title":"With explicit source","text":"<p>query = Query(\"data.csv\") query = Query(\"/path/to/data.parquet\") query = Query(\"https://example.com/data.csv\")</p>"},{"location":"api/reference/query/#sqlstream.core.query.Query.__init__--without-source-extracted-from-sql","title":"Without source - extracted from SQL","text":"<p>query = Query() query.sql(\"SELECT * FROM 'data.csv' WHERE age &gt; 25\")</p>"},{"location":"api/reference/query/#sqlstream.core.query.Query.sql","title":"sql","text":"<pre><code>sql(query: str, backend: Optional[Literal['auto', 'pandas', 'python', 'duckdb']] = 'auto') -&gt; QueryResult\n</code></pre> <p>Execute SQL query on the data source</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query string</p> required <code>backend</code> <code>Optional[Literal['auto', 'pandas', 'python', 'duckdb']]</code> <p>Execution backend to use - \"auto\": Smart selection - uses custom parser for simple queries,          DuckDB for complex queries, pandas if available, else python - \"duckdb\": Force DuckDB backend (full SQL support) - \"pandas\": Force pandas backend (10-100x faster than python) - \"python\": Force pure Python Volcano model (educational)</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>QueryResult</code> <p>QueryResult object that can be iterated over</p> Example Source code in <code>sqlstream/core/query.py</code> <pre><code>def sql(\n    self, query: str, backend: Optional[Literal[\"auto\", \"pandas\", \"python\", \"duckdb\"]] = \"auto\"\n) -&gt; \"QueryResult\":\n    \"\"\"\n    Execute SQL query on the data source\n\n    Args:\n        query: SQL query string\n        backend: Execution backend to use\n            - \"auto\": Smart selection - uses custom parser for simple queries,\n                     DuckDB for complex queries, pandas if available, else python\n            - \"duckdb\": Force DuckDB backend (full SQL support)\n            - \"pandas\": Force pandas backend (10-100x faster than python)\n            - \"python\": Force pure Python Volcano model (educational)\n\n    Returns:\n        QueryResult object that can be iterated over\n\n    Example:\n        &gt;&gt;&gt; # With explicit source\n        &gt;&gt;&gt; result = query(\"data.csv\").sql(\"SELECT * WHERE age &gt; 25\")\n        &gt;&gt;&gt; for row in result:\n        ...     print(row)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Without source - from SQL\n        &gt;&gt;&gt; result = query().sql(\"SELECT * FROM 'data.csv' WHERE age &gt; 25\")\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Force DuckDB backend for full SQL support\n        &gt;&gt;&gt; result = query(\"data.csv\").sql(\n        ...     \"SELECT *, ROW_NUMBER() OVER (PARTITION BY dept ORDER BY salary DESC) as rank \"\n        ...     \"FROM data\",\n        ...     backend=\"duckdb\"\n        ... )\n    \"\"\"\n    # Note: Backend selection is now handled in QueryResult._select_backend()\n    # to preserve the 'auto' value while still doing smart selection\n\n    # If query can be parsed with custom parser\n    if _can_parse_with_custom_parser(query):\n        # Try to parse with custom parser\n        try:\n            ast = parse(query)\n        except Exception:\n            # Cannot be parsed with custom parser, need DuckDB\n            ast = None\n    else:\n        # Cannot be parsed with custom parser, need DuckDB\n        ast = None\n\n    # Create QueryResult with reader factory for JOIN support\n    return QueryResult(ast=ast, reader=self.reader, reader_factory=self._create_reader,\n                      source=self.source, backend=backend, raw_sql=query)\n</code></pre>"},{"location":"api/reference/query/#sqlstream.core.query.Query.sql--with-explicit-source","title":"With explicit source","text":"<p>result = query(\"data.csv\").sql(\"SELECT * WHERE age &gt; 25\") for row in result: ...     print(row)</p>"},{"location":"api/reference/query/#sqlstream.core.query.Query.sql--without-source-from-sql","title":"Without source - from SQL","text":"<p>result = query().sql(\"SELECT * FROM 'data.csv' WHERE age &gt; 25\")</p>"},{"location":"api/reference/query/#sqlstream.core.query.Query.sql--force-duckdb-backend-for-full-sql-support","title":"Force DuckDB backend for full SQL support","text":"<p>result = query(\"data.csv\").sql( ...     \"SELECT *, ROW_NUMBER() OVER (PARTITION BY dept ORDER BY salary DESC) as rank \" ...     \"FROM data\", ...     backend=\"duckdb\" ... )</p>"},{"location":"api/reference/query/#sqlstream.core.query.Query.schema","title":"schema","text":"<pre><code>schema() -&gt; Optional[Schema]\n</code></pre> <p>Get schema information for the data source</p> <p>Returns:</p> Type Description <code>Optional[Schema]</code> <p>Schema object with inferred types, or None if schema cannot be inferred</p> Example <p>schema = query(\"data.csv\").schema() print(schema) Schema(name: STRING, age: INTEGER, salary: FLOAT)</p> Source code in <code>sqlstream/core/query.py</code> <pre><code>def schema(self) -&gt; Optional[Schema]:\n    \"\"\"\n    Get schema information for the data source\n\n    Returns:\n        Schema object with inferred types, or None if schema cannot be inferred\n\n    Example:\n        &gt;&gt;&gt; schema = query(\"data.csv\").schema()\n        &gt;&gt;&gt; print(schema)\n        Schema(name: STRING, age: INTEGER, salary: FLOAT)\n    \"\"\"\n    if not self.reader:\n        raise ValueError(\"Cannot get schema without a source. Provide a source when creating the Query object.\")\n    return self.reader.get_schema()\n</code></pre>"},{"location":"api/reference/query/#queryresult","title":"QueryResult","text":""},{"location":"api/reference/query/#sqlstream.core.query.QueryResult","title":"QueryResult","text":"<p>Query result - lazy iterator over query results</p> <p>This class wraps the execution of a query and provides a lazy iterator over the results.</p> Source code in <code>sqlstream/core/query.py</code> <pre><code>class QueryResult:\n    \"\"\"\n    Query result - lazy iterator over query results\n\n    This class wraps the execution of a query and provides\n    a lazy iterator over the results.\n    \"\"\"\n\n    def __init__(\n        self,\n        ast,\n        reader: BaseReader,\n        reader_factory: Callable[[str], BaseReader],\n        source: str,\n        backend: str = \"auto\",\n        raw_sql: str = None,\n    ):\n        \"\"\"\n        Initialize query result\n\n        Args:\n            ast: Parsed SQL AST (None for DuckDB backend)\n            reader: Data source reader\n            reader_factory: Factory function to create readers for JOIN tables\n            source: Path to data source file\n            backend: Execution backend (\"auto\", \"pandas\", \"python\", or \"duckdb\")\n            raw_sql: Original SQL query string (required for DuckDB)\n        \"\"\"\n        self.ast = ast\n        self.reader = reader\n        self.reader_factory = reader_factory\n        self.source = source\n        self.backend = backend\n        self.raw_sql = raw_sql\n\n        # Select executor based on backend\n        self._select_backend()\n\n    def _select_backend(self):\n        \"\"\"Select appropriate backend based on configuration\"\"\"\n\n        # Determine effective backend to use\n        target_backend = self.backend\n\n        if target_backend == \"auto\":\n            # Smart backend selection logic\n\n            # Case 1: AST already present\n            # This means custom parser already succeeded\n            if self.ast:\n                if PANDAS_AVAILABLE:\n                    target_backend = \"pandas\"\n                else:\n                    target_backend = \"python\"\n\n            # Case 2: No AST, analyze raw SQL\n            elif self.raw_sql:\n                if _can_parse_with_custom_parser(self.raw_sql):\n                    # Simple query - prefer pandas &gt; python\n                    if PANDAS_AVAILABLE:\n                        target_backend = \"pandas\"\n                    else:\n                        target_backend = \"python\"\n                elif DUCKDB_AVAILABLE:\n                    # Complex query - use DuckDB\n                    target_backend = \"duckdb\"\n                else:\n                    # Complex query but no DuckDB\n                    raise ImportError(\n                        \"This query requires advanced SQL features not supported by the basic parser. \"\n                        \"Install `sqlstream[duckdb]`\"\n                    )\n\n            # Case 3: Fallback (shouldn't happen in normal usage)\n            else:\n                if PANDAS_AVAILABLE:\n                    target_backend = \"pandas\"\n                elif DUCKDB_AVAILABLE:\n                    target_backend = \"duckdb\"\n                else:\n                    target_backend = \"python\"\n\n        # Configure executor based on target_backend\n        if target_backend == \"duckdb\":\n            # Force DuckDB backend\n            if not DUCKDB_AVAILABLE:\n                raise ImportError(\n                    \"DuckDB backend requested but duckdb is not installed. \"\n                    \"Install `sqlstream[duckdb]`\"\n                )\n            self.executor = DuckDBExecutor()\n            self.use_duckdb = True\n            self.use_pandas = False\n\n        elif target_backend == \"pandas\":\n            # Force pandas backend\n            if not PANDAS_AVAILABLE:\n                raise ImportError(\n                    \"Pandas backend requested but pandas is not installed. \"\n                    \"Install `sqlstream[pandas]`\"\n                )\n\n            # Ensure AST is parsed if not already present\n            if not self.ast and self.raw_sql:\n                try:\n                    self.ast = parse(self.raw_sql)\n                except Exception as e:\n                    # If auto selected pandas but parsing failed, try fallback to DuckDB\n                    if self.backend == \"auto\" and DUCKDB_AVAILABLE:\n                        self.executor = DuckDBExecutor()\n                        self.use_duckdb = True\n                        self.use_pandas = False\n                        return\n\n                    raise ValueError(\n                        f\"Failed to parse SQL query: {e}. \"\n                        \"Consider installing DuckDB for full SQL support: pip install sqlstream[duckdb]\"\n                    ) from e\n\n            self.executor = PandasExecutor()\n            self.use_pandas = True\n            self.use_duckdb = False\n\n        elif target_backend == \"python\":\n            # Force pure Python backend\n\n            # Ensure AST is parsed if not already present\n            if not self.ast and self.raw_sql:\n                try:\n                    self.ast = parse(self.raw_sql)\n                except Exception as e:\n                    # If auto selected python but parsing failed, try fallback to DuckDB\n                    if self.backend == \"auto\" and DUCKDB_AVAILABLE:\n                        self.executor = DuckDBExecutor()\n                        self.use_duckdb = True\n                        self.use_pandas = False\n                        return\n\n                    raise ValueError(f\"Failed to parse SQL query: {e}\") from e\n\n            self.executor = Executor()\n            self.use_pandas = False\n            self.use_duckdb = False\n\n    def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Execute query and yield results lazily\n\n        Yields:\n            Result rows as dictionaries\n        \"\"\"\n        if self.use_duckdb:\n            # DuckDB executor - pass raw SQL and discover tables\n            if not self.raw_sql:\n                raise ValueError(\"DuckDB backend requires raw SQL query\")\n\n            sources = self._discover_sources()\n\n            # Use reader factory to create DataFrames efficiently\n            yield from self.executor.execute_raw(\n                self.raw_sql,\n                sources,\n                reader_factory=self.reader_factory\n            )\n        elif self.use_pandas:\n            # Pandas executor takes file path directly\n            right_source = self.ast.join.right_source if self.ast.join else None\n            yield from self.executor.execute(self.ast, self.source or self.ast.source, right_source)\n        else:\n            # Python executor uses reader objects\n            # If no reader exists (sourceless query), create one from AST\n            reader = self.reader\n            if not reader and self.ast and self.ast.source:\n                reader = self.reader_factory(self.ast.source)\n            yield from self.executor.execute(self.ast, reader, self.reader_factory)\n\n    @staticmethod\n    def _get_sanitized_name_and_table_hint(source: str) -&gt; Tuple[str, Optional[int]]:\n        # Parse fragment if present to get base path\n        clean_path, format_hint, table_hint = parse_source_fragment(source)\n\n        # Generate a table name from the file path\n        # Extract base filename (without extension or parent directories)\n        base_name = os.path.splitext(os.path.basename(clean_path))[0]\n\n        # Clean up the name to be SQL-safe (only alphanumeric and underscore)\n        sanitized_name = re.sub(r'[^a-zA-Z0-9_]', '_', base_name)\n        return sanitized_name, table_hint\n\n    @staticmethod\n    def _get_table_name(source: str) -&gt; str:\n        sanitized_name, table_hint = QueryResult._get_sanitized_name_and_table_hint(source)\n\n        # Make the table name unique if multiple tables from same file\n        # This handles cases like complex.html#html:0, complex.html#html:1, etc.\n        if table_hint is not None:\n            # Include table/fragment index in the name\n            table_name = f\"{sanitized_name}_{table_hint}\"\n        else:\n            table_name = sanitized_name\n        return table_name\n\n    def _discover_sources(self) -&gt; Dict[str, str]:\n        \"\"\"\n        Discover all table sources from raw SQL or AST\n\n        For DuckDB backend, this extracts all file paths from the SQL query.\n        Handles multiple files in JOINs, subqueries, CTEs, etc.\n        Properly handles URL fragments like #html:0\n        \"\"\"\n        sources: Dict[str, str] = {}\n\n        if self.raw_sql:\n            # Extract file paths from raw SQL for DuckDB\n            # Pattern matches quoted file paths/URLs (including those with fragments)\n            # Matches: FROM 'file.csv' or FROM \"file.csv\" or JOIN 'url#format:table'\n            # The key is to NOT stop at # - we need to capture the full fragment\n\n            # Pattern for quoted file paths - captures everything inside quotes\n            # This handles fragments like #html:3, #csv:0, etc.\n            quoted_pattern = r\"(?:FROM|JOIN)\\s+(['\\\"])([^\\1]+?)\\1\"\n            matches = re.findall(quoted_pattern, self.raw_sql, re.IGNORECASE)\n\n            # Track table name usage to avoid conflicts\n            name_counter = {}\n\n            for _quote_char, file_path in matches:\n                sanitized_name, _ = self._get_sanitized_name_and_table_hint(file_path)\n                table_name = self._get_table_name(file_path)\n\n                # Ensure uniqueness by adding counter if needed\n                if table_name in sources:\n                    counter = name_counter.get(sanitized_name, 0) + 1\n                    name_counter[sanitized_name] = counter\n                    table_name = f\"{table_name}_{counter}\"\n\n                # Store the mapping: table_name -&gt; original_file_path\n                sources[table_name] = file_path\n\n            # Also check for unquoted file paths (e.g., from f-strings in tests)\n            # Pattern: FROM /path/to/file.ext or FROM file.ext\n            # This is tricky because we need to stop at keywords or whitespace\n            unquoted_pattern = r\"(?:FROM|JOIN)\\s+([/\\w.#:-]+?)(?:\\s+(?:ON|WHERE|GROUP|ORDER|LIMIT|INNER|LEFT|RIGHT|JOIN|,|\\))|$)\"\n            unquoted_matches: List[str] = re.findall(unquoted_pattern, self.raw_sql, re.IGNORECASE)\n\n            for file_path in unquoted_matches:\n                # Skip if already found as quoted\n                if file_path in set(sources.values()):\n                    continue\n\n                # Skip SQL keywords\n                if file_path.upper() in ['INNER', 'LEFT', 'RIGHT', 'OUTER', 'CROSS']:\n                    continue\n\n                # Only process if it looks like a file path\n                if '/' in file_path or '.' in file_path or '#' in file_path:\n                    sanitized_name, _ = self._get_sanitized_name_and_table_hint(file_path)\n                    table_name = self._get_table_name(file_path)\n\n                    # Ensure uniqueness\n                    if table_name in sources:\n                        counter = name_counter.get(sanitized_name, 0) + 1\n                        name_counter[sanitized_name] = counter\n                        table_name = f\"{table_name}_{counter}\"\n\n                    sources[table_name] = file_path\n\n            # If no sources found from SQL, use the main source\n            if not sources and self.source:\n                table_name = self._get_table_name(self.source)\n                sources[table_name] = self.source\n\n        elif self.ast:\n            table_name = self._get_table_name(self.ast.source)\n\n            # Extract from AST for Python/Pandas backends\n            # Main table\n            if hasattr(self.ast, 'table') and self.ast.table:\n                sources[self.ast.table] = self.ast.source\n            else:\n                # If no explicit table name, use 'data' as default\n                sources[table_name] = self.ast.source\n\n            # JOIN table\n            if self.ast.join:\n                join_table = self.ast.join.right_source\n                join_table_name = self._get_table_name(join_table)\n                sources[join_table_name] = join_table\n\n        return sources\n\n    def to_list(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Materialize all results into a list\n\n        Returns:\n            List of all result rows\n\n        Example:\n            &gt;&gt;&gt; results = query(\"data.csv\").sql(\"SELECT *\").to_list()\n            &gt;&gt;&gt; print(len(results))\n            100\n        \"\"\"\n        return list(self)\n\n    def explain(self) -&gt; str:\n        \"\"\"\n        Get query execution plan\n\n        Returns:\n            Human-readable execution plan\n\n        Example:\n            &gt;&gt;&gt; plan = query(\"data.csv\").sql(\"SELECT * WHERE age &gt; 25\").explain()\n            &gt;&gt;&gt; print(plan)\n            Limit(10)\n              Project(name, age)\n                Filter(age &gt; 25)\n                  Scan(CSVReader)\n        \"\"\"\n        if self.use_duckdb:\n            # DuckDB executor explain\n            if not self.raw_sql:\n                raise ValueError(\"DuckDB backend requires raw SQL query\")\n            sources = self._discover_sources()\n            return self.executor.explain(self.raw_sql, sources)\n        elif self.use_pandas:\n            # Pandas executor explain\n            return self.executor.explain(self.ast, self.source)\n        else:\n            # Python executor explain\n            return self.executor.explain(self.ast, self.reader, self.reader_factory)\n</code></pre>"},{"location":"api/reference/query/#sqlstream.core.query.QueryResult.__init__","title":"__init__","text":"<pre><code>__init__(ast, reader: BaseReader, reader_factory: Callable[[str], BaseReader], source: str, backend: str = 'auto', raw_sql: str = None)\n</code></pre> <p>Initialize query result</p> <p>Parameters:</p> Name Type Description Default <code>ast</code> <p>Parsed SQL AST (None for DuckDB backend)</p> required <code>reader</code> <code>BaseReader</code> <p>Data source reader</p> required <code>reader_factory</code> <code>Callable[[str], BaseReader]</code> <p>Factory function to create readers for JOIN tables</p> required <code>source</code> <code>str</code> <p>Path to data source file</p> required <code>backend</code> <code>str</code> <p>Execution backend (\"auto\", \"pandas\", \"python\", or \"duckdb\")</p> <code>'auto'</code> <code>raw_sql</code> <code>str</code> <p>Original SQL query string (required for DuckDB)</p> <code>None</code> Source code in <code>sqlstream/core/query.py</code> <pre><code>def __init__(\n    self,\n    ast,\n    reader: BaseReader,\n    reader_factory: Callable[[str], BaseReader],\n    source: str,\n    backend: str = \"auto\",\n    raw_sql: str = None,\n):\n    \"\"\"\n    Initialize query result\n\n    Args:\n        ast: Parsed SQL AST (None for DuckDB backend)\n        reader: Data source reader\n        reader_factory: Factory function to create readers for JOIN tables\n        source: Path to data source file\n        backend: Execution backend (\"auto\", \"pandas\", \"python\", or \"duckdb\")\n        raw_sql: Original SQL query string (required for DuckDB)\n    \"\"\"\n    self.ast = ast\n    self.reader = reader\n    self.reader_factory = reader_factory\n    self.source = source\n    self.backend = backend\n    self.raw_sql = raw_sql\n\n    # Select executor based on backend\n    self._select_backend()\n</code></pre>"},{"location":"api/reference/query/#sqlstream.core.query.QueryResult.__iter__","title":"__iter__","text":"<pre><code>__iter__() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Execute query and yield results lazily</p> <p>Yields:</p> Type Description <code>Dict[str, Any]</code> <p>Result rows as dictionaries</p> Source code in <code>sqlstream/core/query.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Execute query and yield results lazily\n\n    Yields:\n        Result rows as dictionaries\n    \"\"\"\n    if self.use_duckdb:\n        # DuckDB executor - pass raw SQL and discover tables\n        if not self.raw_sql:\n            raise ValueError(\"DuckDB backend requires raw SQL query\")\n\n        sources = self._discover_sources()\n\n        # Use reader factory to create DataFrames efficiently\n        yield from self.executor.execute_raw(\n            self.raw_sql,\n            sources,\n            reader_factory=self.reader_factory\n        )\n    elif self.use_pandas:\n        # Pandas executor takes file path directly\n        right_source = self.ast.join.right_source if self.ast.join else None\n        yield from self.executor.execute(self.ast, self.source or self.ast.source, right_source)\n    else:\n        # Python executor uses reader objects\n        # If no reader exists (sourceless query), create one from AST\n        reader = self.reader\n        if not reader and self.ast and self.ast.source:\n            reader = self.reader_factory(self.ast.source)\n        yield from self.executor.execute(self.ast, reader, self.reader_factory)\n</code></pre>"},{"location":"api/reference/query/#sqlstream.core.query.QueryResult.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Materialize all results into a list</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of all result rows</p> Example <p>results = query(\"data.csv\").sql(\"SELECT *\").to_list() print(len(results)) 100</p> Source code in <code>sqlstream/core/query.py</code> <pre><code>def to_list(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Materialize all results into a list\n\n    Returns:\n        List of all result rows\n\n    Example:\n        &gt;&gt;&gt; results = query(\"data.csv\").sql(\"SELECT *\").to_list()\n        &gt;&gt;&gt; print(len(results))\n        100\n    \"\"\"\n    return list(self)\n</code></pre>"},{"location":"api/reference/query/#sqlstream.core.query.QueryResult.explain","title":"explain","text":"<pre><code>explain() -&gt; str\n</code></pre> <p>Get query execution plan</p> <p>Returns:</p> Type Description <code>str</code> <p>Human-readable execution plan</p> Example <p>plan = query(\"data.csv\").sql(\"SELECT * WHERE age &gt; 25\").explain() print(plan) Limit(10)   Project(name, age)     Filter(age &gt; 25)       Scan(CSVReader)</p> Source code in <code>sqlstream/core/query.py</code> <pre><code>def explain(self) -&gt; str:\n    \"\"\"\n    Get query execution plan\n\n    Returns:\n        Human-readable execution plan\n\n    Example:\n        &gt;&gt;&gt; plan = query(\"data.csv\").sql(\"SELECT * WHERE age &gt; 25\").explain()\n        &gt;&gt;&gt; print(plan)\n        Limit(10)\n          Project(name, age)\n            Filter(age &gt; 25)\n              Scan(CSVReader)\n    \"\"\"\n    if self.use_duckdb:\n        # DuckDB executor explain\n        if not self.raw_sql:\n            raise ValueError(\"DuckDB backend requires raw SQL query\")\n        sources = self._discover_sources()\n        return self.executor.explain(self.raw_sql, sources)\n    elif self.use_pandas:\n        # Pandas executor explain\n        return self.executor.explain(self.ast, self.source)\n    else:\n        # Python executor explain\n        return self.executor.explain(self.ast, self.reader, self.reader_factory)\n</code></pre>"},{"location":"api/reference/query/#query_1","title":"query","text":""},{"location":"api/reference/query/#sqlstream.core.query.query","title":"query","text":"<pre><code>query(source: Optional[str] = None) -&gt; Query\n</code></pre> <p>Create a query for a data source</p> <p>This is the main entry point for the SQLStream API.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Optional[str]</code> <p>Optional path to data file or URL. If not provided,     sources will be extracted from the SQL query.</p> <code>None</code> <p>Returns:</p> Type Description <code>Query</code> <p>Query object</p> Example <p>from sqlstream import query</p> Source code in <code>sqlstream/core/query.py</code> <pre><code>def query(source: Optional[str] = None) -&gt; Query:\n    \"\"\"\n    Create a query for a data source\n\n    This is the main entry point for the SQLStream API.\n\n    Args:\n        source: Optional path to data file or URL. If not provided,\n                sources will be extracted from the SQL query.\n\n    Returns:\n        Query object\n\n    Example:\n        &gt;&gt;&gt; from sqlstream import query\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # With explicit source\n        &gt;&gt;&gt; results = query(\"data.csv\").sql(\"SELECT * WHERE age &gt; 25\")\n        &gt;&gt;&gt; for row in results:\n        ...     print(row)\n        &gt;&gt;&gt;\n        &gt;&gt;&gt; # Without source - extracted from SQL\n        &gt;&gt;&gt; results = query().sql(\"SELECT * FROM 'data.csv' WHERE age &gt; 25\")\n        &gt;&gt;&gt; for row in results:\n        ...     print(row)\n    \"\"\"\n    return Query(source)\n</code></pre>"},{"location":"api/reference/query/#sqlstream.core.query.query--with-explicit-source","title":"With explicit source","text":"<p>results = query(\"data.csv\").sql(\"SELECT * WHERE age &gt; 25\") for row in results: ...     print(row)</p>"},{"location":"api/reference/query/#sqlstream.core.query.query--without-source-extracted-from-sql","title":"Without source - extracted from SQL","text":"<p>results = query().sql(\"SELECT * FROM 'data.csv' WHERE age &gt; 25\") for row in results: ...     print(row)</p>"},{"location":"api/reference/readers/","title":"Readers API Reference","text":"<p>Data source readers for various formats.</p>"},{"location":"api/reference/readers/#basereader","title":"BaseReader","text":""},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader","title":"BaseReader","text":"<p>Base class for all data source readers</p> <p>Readers are responsible for: 1. Reading data from a source (file, URL, database, etc.) 2. Yielding rows as dictionaries (lazy evaluation) 3. Optionally supporting predicate pushdown 4. Optionally supporting column pruning</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>class BaseReader:\n    \"\"\"\n    Base class for all data source readers\n\n    Readers are responsible for:\n    1. Reading data from a source (file, URL, database, etc.)\n    2. Yielding rows as dictionaries (lazy evaluation)\n    3. Optionally supporting predicate pushdown\n    4. Optionally supporting column pruning\n    \"\"\"\n\n    def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Yield rows as dictionaries\n\n        This is the core method that all readers must implement.\n        It should yield one row at a time (lazy evaluation) rather than\n        loading all data into memory.\n\n        Yields:\n            Dictionary representing one row of data\n\n        Example:\n            {'name': 'Alice', 'age': 30, 'city': 'NYC'}\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement read_lazy()\")\n\n    def supports_pushdown(self) -&gt; bool:\n        \"\"\"\n        Does this reader support predicate pushdown?\n\n        If True, the query optimizer can call set_filter() to push\n        WHERE conditions down to the reader for more efficient execution.\n\n        Returns:\n            True if predicate pushdown is supported\n        \"\"\"\n        return False\n\n    def set_filter(self, conditions: List[Condition]) -&gt; None:\n        \"\"\"\n        Set filter conditions for predicate pushdown\n\n        Args:\n            conditions: List of WHERE conditions to apply during read\n\n        Note:\n            Only called if supports_pushdown() returns True\n        \"\"\"\n        pass\n\n    def supports_column_selection(self) -&gt; bool:\n        \"\"\"\n        Does this reader support column pruning?\n\n        If True, the query optimizer can call set_columns() to specify\n        which columns are needed, allowing the reader to skip reading\n        unnecessary columns.\n\n        Returns:\n            True if column selection is supported\n        \"\"\"\n        return False\n\n    def set_columns(self, columns: List[str]) -&gt; None:\n        \"\"\"\n        Set which columns to read (column pruning)\n\n        Args:\n            columns: List of column names to read\n\n        Note:\n            Only called if supports_column_selection() returns True\n        \"\"\"\n        pass\n\n    def supports_limit(self) -&gt; bool:\n        \"\"\"\n        Does this reader support early termination with LIMIT?\n\n        If True, the query optimizer can call set_limit() to specify\n        the maximum number of rows to read, allowing early termination.\n\n        Returns:\n            True if limit pushdown is supported\n        \"\"\"\n        return False\n\n    def set_limit(self, limit: int) -&gt; None:\n        \"\"\"\n        Set maximum number of rows to read (limit pushdown)\n\n        Args:\n            limit: Maximum number of rows to yield\n\n        Note:\n            Only called if supports_limit() returns True\n            Reader should stop yielding rows after 'limit' rows\n        \"\"\"\n        pass\n\n    def supports_partition_pruning(self) -&gt; bool:\n        \"\"\"\n        Does this reader support partition pruning?\n\n        If True, the query optimizer can call set_partition_filters() to specify\n        which partitions to read based on filter conditions.\n\n        Returns:\n            True if partition pruning is supported\n        \"\"\"\n        return False\n\n    def get_partition_columns(self) -&gt; set:\n        \"\"\"\n        Get partition column names for Hive-style partitioning\n\n        Returns:\n            Set of partition column names (e.g., {'year', 'month', 'day'})\n            Empty set if not partitioned\n\n        Example:\n            For path: s3://bucket/data/year=2024/month=01/data.parquet\n            Returns: {'year', 'month'}\n        \"\"\"\n        return set()\n\n    def set_partition_filters(self, conditions: List[Condition]) -&gt; None:\n        \"\"\"\n        Set filter conditions for partition pruning\n\n        Args:\n            conditions: List of WHERE conditions on partition columns\n\n        Note:\n            Only called if supports_partition_pruning() returns True\n            Reader should skip partitions that don't match these conditions\n        \"\"\"\n        pass\n\n    def get_schema(self) -&gt; Optional[Schema]:\n        \"\"\"\n        Get schema information (column names and types)\n\n        Returns:\n            Schema object with inferred types, or None if schema cannot be inferred\n\n        Note:\n            Optional method. Returns None by default.\n            Readers should override this to provide schema inference.\n        \"\"\"\n        return None\n\n    def __iter__(self):\n        \"\"\"Allow readers to be used directly in for loops\"\"\"\n        return self.read_lazy()\n\n    def to_dataframe(self):\n        \"\"\"\n        Convert reader content to pandas DataFrame\n\n        Returns:\n            pandas.DataFrame containing all data\n\n        Note:\n            Default implementation iterates over read_lazy() and creates DataFrame.\n            Subclasses should override this for better performance (e.g. using read_csv/read_parquet).\n        \"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\"Pandas is required for to_dataframe()\")\n\n        # Default implementation: materialize iterator\n        return pd.DataFrame(list(self.read_lazy()))\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.read_lazy","title":"read_lazy","text":"<pre><code>read_lazy() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Yield rows as dictionaries</p> <p>This is the core method that all readers must implement. It should yield one row at a time (lazy evaluation) rather than loading all data into memory.</p> <p>Yields:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary representing one row of data</p> Example <p>{'name': 'Alice', 'age': 30, 'city': 'NYC'}</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Yield rows as dictionaries\n\n    This is the core method that all readers must implement.\n    It should yield one row at a time (lazy evaluation) rather than\n    loading all data into memory.\n\n    Yields:\n        Dictionary representing one row of data\n\n    Example:\n        {'name': 'Alice', 'age': 30, 'city': 'NYC'}\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement read_lazy()\")\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.supports_pushdown","title":"supports_pushdown","text":"<pre><code>supports_pushdown() -&gt; bool\n</code></pre> <p>Does this reader support predicate pushdown?</p> <p>If True, the query optimizer can call set_filter() to push WHERE conditions down to the reader for more efficient execution.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if predicate pushdown is supported</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def supports_pushdown(self) -&gt; bool:\n    \"\"\"\n    Does this reader support predicate pushdown?\n\n    If True, the query optimizer can call set_filter() to push\n    WHERE conditions down to the reader for more efficient execution.\n\n    Returns:\n        True if predicate pushdown is supported\n    \"\"\"\n    return False\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.set_filter","title":"set_filter","text":"<pre><code>set_filter(conditions: List[Condition]) -&gt; None\n</code></pre> <p>Set filter conditions for predicate pushdown</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>List[Condition]</code> <p>List of WHERE conditions to apply during read</p> required Note <p>Only called if supports_pushdown() returns True</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def set_filter(self, conditions: List[Condition]) -&gt; None:\n    \"\"\"\n    Set filter conditions for predicate pushdown\n\n    Args:\n        conditions: List of WHERE conditions to apply during read\n\n    Note:\n        Only called if supports_pushdown() returns True\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.supports_column_selection","title":"supports_column_selection","text":"<pre><code>supports_column_selection() -&gt; bool\n</code></pre> <p>Does this reader support column pruning?</p> <p>If True, the query optimizer can call set_columns() to specify which columns are needed, allowing the reader to skip reading unnecessary columns.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if column selection is supported</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def supports_column_selection(self) -&gt; bool:\n    \"\"\"\n    Does this reader support column pruning?\n\n    If True, the query optimizer can call set_columns() to specify\n    which columns are needed, allowing the reader to skip reading\n    unnecessary columns.\n\n    Returns:\n        True if column selection is supported\n    \"\"\"\n    return False\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.set_columns","title":"set_columns","text":"<pre><code>set_columns(columns: List[str]) -&gt; None\n</code></pre> <p>Set which columns to read (column pruning)</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[str]</code> <p>List of column names to read</p> required Note <p>Only called if supports_column_selection() returns True</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def set_columns(self, columns: List[str]) -&gt; None:\n    \"\"\"\n    Set which columns to read (column pruning)\n\n    Args:\n        columns: List of column names to read\n\n    Note:\n        Only called if supports_column_selection() returns True\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.supports_limit","title":"supports_limit","text":"<pre><code>supports_limit() -&gt; bool\n</code></pre> <p>Does this reader support early termination with LIMIT?</p> <p>If True, the query optimizer can call set_limit() to specify the maximum number of rows to read, allowing early termination.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if limit pushdown is supported</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def supports_limit(self) -&gt; bool:\n    \"\"\"\n    Does this reader support early termination with LIMIT?\n\n    If True, the query optimizer can call set_limit() to specify\n    the maximum number of rows to read, allowing early termination.\n\n    Returns:\n        True if limit pushdown is supported\n    \"\"\"\n    return False\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.set_limit","title":"set_limit","text":"<pre><code>set_limit(limit: int) -&gt; None\n</code></pre> <p>Set maximum number of rows to read (limit pushdown)</p> <p>Parameters:</p> Name Type Description Default <code>limit</code> <code>int</code> <p>Maximum number of rows to yield</p> required Note <p>Only called if supports_limit() returns True Reader should stop yielding rows after 'limit' rows</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def set_limit(self, limit: int) -&gt; None:\n    \"\"\"\n    Set maximum number of rows to read (limit pushdown)\n\n    Args:\n        limit: Maximum number of rows to yield\n\n    Note:\n        Only called if supports_limit() returns True\n        Reader should stop yielding rows after 'limit' rows\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.supports_partition_pruning","title":"supports_partition_pruning","text":"<pre><code>supports_partition_pruning() -&gt; bool\n</code></pre> <p>Does this reader support partition pruning?</p> <p>If True, the query optimizer can call set_partition_filters() to specify which partitions to read based on filter conditions.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if partition pruning is supported</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def supports_partition_pruning(self) -&gt; bool:\n    \"\"\"\n    Does this reader support partition pruning?\n\n    If True, the query optimizer can call set_partition_filters() to specify\n    which partitions to read based on filter conditions.\n\n    Returns:\n        True if partition pruning is supported\n    \"\"\"\n    return False\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.get_partition_columns","title":"get_partition_columns","text":"<pre><code>get_partition_columns() -&gt; set\n</code></pre> <p>Get partition column names for Hive-style partitioning</p> <p>Returns:</p> Type Description <code>set</code> <p>Set of partition column names (e.g., {'year', 'month', 'day'})</p> <code>set</code> <p>Empty set if not partitioned</p> Example <p>For path: s3://bucket/data/year=2024/month=01/data.parquet Returns: {'year', 'month'}</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def get_partition_columns(self) -&gt; set:\n    \"\"\"\n    Get partition column names for Hive-style partitioning\n\n    Returns:\n        Set of partition column names (e.g., {'year', 'month', 'day'})\n        Empty set if not partitioned\n\n    Example:\n        For path: s3://bucket/data/year=2024/month=01/data.parquet\n        Returns: {'year', 'month'}\n    \"\"\"\n    return set()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.set_partition_filters","title":"set_partition_filters","text":"<pre><code>set_partition_filters(conditions: List[Condition]) -&gt; None\n</code></pre> <p>Set filter conditions for partition pruning</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>List[Condition]</code> <p>List of WHERE conditions on partition columns</p> required Note <p>Only called if supports_partition_pruning() returns True Reader should skip partitions that don't match these conditions</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def set_partition_filters(self, conditions: List[Condition]) -&gt; None:\n    \"\"\"\n    Set filter conditions for partition pruning\n\n    Args:\n        conditions: List of WHERE conditions on partition columns\n\n    Note:\n        Only called if supports_partition_pruning() returns True\n        Reader should skip partitions that don't match these conditions\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.get_schema","title":"get_schema","text":"<pre><code>get_schema() -&gt; Optional[Schema]\n</code></pre> <p>Get schema information (column names and types)</p> <p>Returns:</p> Type Description <code>Optional[Schema]</code> <p>Schema object with inferred types, or None if schema cannot be inferred</p> Note <p>Optional method. Returns None by default. Readers should override this to provide schema inference.</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def get_schema(self) -&gt; Optional[Schema]:\n    \"\"\"\n    Get schema information (column names and types)\n\n    Returns:\n        Schema object with inferred types, or None if schema cannot be inferred\n\n    Note:\n        Optional method. Returns None by default.\n        Readers should override this to provide schema inference.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Allow readers to be used directly in for loops</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def __iter__(self):\n    \"\"\"Allow readers to be used directly in for loops\"\"\"\n    return self.read_lazy()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.base.BaseReader.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> <p>Convert reader content to pandas DataFrame</p> <p>Returns:</p> Type Description <p>pandas.DataFrame containing all data</p> Note <p>Default implementation iterates over read_lazy() and creates DataFrame. Subclasses should override this for better performance (e.g. using read_csv/read_parquet).</p> Source code in <code>sqlstream/readers/base.py</code> <pre><code>def to_dataframe(self):\n    \"\"\"\n    Convert reader content to pandas DataFrame\n\n    Returns:\n        pandas.DataFrame containing all data\n\n    Note:\n        Default implementation iterates over read_lazy() and creates DataFrame.\n        Subclasses should override this for better performance (e.g. using read_csv/read_parquet).\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"Pandas is required for to_dataframe()\")\n\n    # Default implementation: materialize iterator\n    return pd.DataFrame(list(self.read_lazy()))\n</code></pre>"},{"location":"api/reference/readers/#csvreader","title":"CSVReader","text":""},{"location":"api/reference/readers/#sqlstream.readers.csv_reader.CSVReader","title":"CSVReader","text":"<p>               Bases: <code>BaseReader</code></p> <p>Lazy CSV reader with basic type inference</p> <p>Features: - Lazy iteration (doesn't load entire file into memory) - Automatic type inference (int, float, string) - Predicate pushdown support - Column pruning support</p> Source code in <code>sqlstream/readers/csv_reader.py</code> <pre><code>class CSVReader(BaseReader):\n    \"\"\"\n    Lazy CSV reader with basic type inference\n\n    Features:\n    - Lazy iteration (doesn't load entire file into memory)\n    - Automatic type inference (int, float, string)\n    - Predicate pushdown support\n    - Column pruning support\n    \"\"\"\n\n    def __init__(self, path: str, encoding: str = \"utf-8\", delimiter: str = \",\"):\n        \"\"\"\n        Initialize CSV reader\n\n        Args:\n            path: Path to CSV file (local or s3://)\n            encoding: File encoding (default: utf-8)\n            delimiter: CSV delimiter (default: comma)\n        \"\"\"\n        self.path_str = path\n        self.is_s3 = path.startswith(\"s3://\")\n        if not self.is_s3:\n            self.path = Path(path)\n        else:\n            self.path = None  # type: ignore\n\n        self.encoding = encoding\n        self.delimiter = delimiter\n\n        # For optimization (set by query optimizer)\n        self.filter_conditions: List[Condition] = []\n        self.required_columns: List[str] = []\n        self.limit: Optional[int] = None\n\n        if not self.is_s3 and not self.path.exists():\n            raise FileNotFoundError(f\"CSV file not found: {path}\")\n\n    def supports_pushdown(self) -&gt; bool:\n        \"\"\"CSV reader supports predicate pushdown\"\"\"\n        return True\n\n    def supports_column_selection(self) -&gt; bool:\n        \"\"\"CSV reader supports column pruning\"\"\"\n        return True\n\n    def supports_limit(self) -&gt; bool:\n        \"\"\"CSV reader supports limit pushdown\"\"\"\n        return True\n\n    def set_filter(self, conditions: List[Condition]) -&gt; None:\n        \"\"\"Set filter conditions for pushdown\"\"\"\n        self.filter_conditions = conditions\n\n    def set_columns(self, columns: List[str]) -&gt; None:\n        \"\"\"Set required columns for pruning\"\"\"\n        self.required_columns = columns\n\n    def set_limit(self, limit: int) -&gt; None:\n        \"\"\"Set maximum rows to read for early termination\"\"\"\n        self.limit = limit\n\n    def _get_file_handle(self):\n        \"\"\"Get file handle for reading (local or S3).\"\"\"\n        if self.is_s3:\n            try:\n                import s3fs\n                fs = s3fs.S3FileSystem(anon=False)\n                return fs.open(self.path_str, mode=\"r\", encoding=self.encoding)\n            except ImportError:\n                raise ImportError(\"s3fs is required for S3 support. Install with: pip install sqlstream[s3]\")\n        else:\n            return open(self.path, encoding=self.encoding, newline=\"\")\n\n    def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Lazy iterator over CSV rows\n\n        Yields rows as dictionaries with type inference applied.\n        If filters are set, applies them during iteration.\n        If columns are set, only yields those columns.\n        If limit is set, stops after yielding that many rows.\n        \"\"\"\n        with self._get_file_handle() as f:\n            reader = csv.DictReader(f, delimiter=self.delimiter)\n            rows_yielded = 0\n\n            for row_num, raw_row in enumerate(reader, start=2):  # Start at 2 (after header)\n                try:\n                    # Apply type inference\n                    row = self._infer_types(raw_row)\n\n                    # Apply filters if set (predicate pushdown)\n                    if self.filter_conditions:\n                        if not self._matches_filter(row):\n                            continue\n\n                    # Apply column selection if set (column pruning)\n                    if self.required_columns:\n                        row = {k: v for k, v in row.items() if k in self.required_columns}\n\n                    yield row\n                    rows_yielded += 1\n\n                    # Early termination if limit reached (limit pushdown)\n                    if self.limit is not None and rows_yielded &gt;= self.limit:\n                        break\n\n                except Exception as e:\n                    # Handle malformed rows gracefully\n                    warnings.warn(\n                        f\"Skipping malformed row {row_num} in {self.path}: {e}\",\n                        UserWarning,\n                    )\n                    continue\n\n    def _infer_types(self, row: Dict[str, str]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Infer types for all values in a row\n\n        Tries to convert strings to int, then float, otherwise keeps as string.\n\n        Args:\n            row: Dictionary with string values\n\n        Returns:\n            Dictionary with inferred types\n        \"\"\"\n        typed_row = {}\n\n        for key, value in row.items():\n            # Handle empty strings as None\n            if value == \"\" or value is None:\n                typed_row[key] = None\n                continue\n\n            typed_row[key] = self._infer_value_type(value)\n\n        return typed_row\n\n    def _infer_value_type(self, value: str) -&gt; Any:\n        \"\"\"\n        Infer type of a single value\n\n        Args:\n            value: String value from CSV\n\n        Returns:\n            Value converted to int, float, or kept as string\n        \"\"\"\n        value = value.strip()\n\n        # Try integer\n        try:\n            return int(value)\n        except ValueError:\n            pass\n\n        # Try float\n        try:\n            return float(value)\n        except ValueError:\n            pass\n\n        # Keep as string\n        return value\n\n    def _matches_filter(self, row: Dict[str, Any]) -&gt; bool:\n        \"\"\"\n        Check if row matches all filter conditions\n\n        Args:\n            row: Row to check\n\n        Returns:\n            True if row matches all conditions (AND logic)\n        \"\"\"\n        for condition in self.filter_conditions:\n            if not self._evaluate_condition(row, condition):\n                return False\n        return True\n\n    def _evaluate_condition(self, row: Dict[str, Any], condition: Condition) -&gt; bool:\n        \"\"\"\n        Evaluate a single condition against a row\n\n        Args:\n            row: Row to check\n            condition: Condition to evaluate\n\n        Returns:\n            True if condition is satisfied\n        \"\"\"\n        # Get column value\n        if condition.column not in row:\n            return False\n\n        value = row[condition.column]\n\n        # Handle NULL values\n        if value is None:\n            return False\n\n        # Evaluate operator\n        op = condition.operator\n        expected = condition.value\n\n        try:\n            if op == \"=\":\n                return value == expected\n            elif op == \"&gt;\":\n                return value &gt; expected\n            elif op == \"&lt;\":\n                return value &lt; expected\n            elif op == \"&gt;=\":\n                return value &gt;= expected\n            elif op == \"&lt;=\":\n                return value &lt;= expected\n            elif op == \"!=\":\n                return value != expected\n            else:\n                # Unknown operator, skip this condition\n                warnings.warn(f\"Unknown operator: {op}\", UserWarning)\n                return True\n\n        except TypeError:\n            # Type mismatch (e.g., comparing string to int)\n            # This is fine - row just doesn't match\n            return False\n\n    def get_schema(self, sample_size: int = 100) -&gt; Optional[Schema]:\n        \"\"\"\n        Infer schema by sampling rows from the CSV file\n\n        Args:\n            sample_size: Number of rows to sample for type inference (default: 100)\n\n        Returns:\n            Schema object with inferred types, or None if file is empty\n        \"\"\"\n        sample_rows = []\n\n        with self._get_file_handle() as f:\n            reader = csv.DictReader(f, delimiter=self.delimiter)\n\n            # Read sample rows to infer types\n            try:\n                for i, raw_row in enumerate(reader):\n                    if i &gt;= sample_size:\n                        break\n                    typed_row = self._infer_types(raw_row)\n                    sample_rows.append(typed_row)\n\n            except StopIteration:\n                # Empty file or fewer rows than sample_size\n                pass\n\n        if not sample_rows:\n            return None\n\n        return Schema.from_rows(sample_rows)\n\n    def to_dataframe(self):\n        \"\"\"\n        Convert to pandas DataFrame efficiently\n        \"\"\"\n        import pandas as pd\n\n        # Use pandas read_csv for performance\n        if self.is_s3:\n            # For S3, use s3fs via storage_options or direct s3 path if supported\n            # Since we already handle s3fs import in _get_file_handle, we can rely on pandas s3 support\n            # which uses s3fs under the hood\n            return pd.read_csv(\n                self.path_str,\n                encoding=self.encoding,\n                delimiter=self.delimiter,\n                storage_options={\"anon\": False}\n            )\n        else:\n            return pd.read_csv(\n                self.path,\n                encoding=self.encoding,\n                delimiter=self.delimiter\n            )\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.csv_reader.CSVReader.__init__","title":"__init__","text":"<pre><code>__init__(path: str, encoding: str = 'utf-8', delimiter: str = ',')\n</code></pre> <p>Initialize CSV reader</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to CSV file (local or s3://)</p> required <code>encoding</code> <code>str</code> <p>File encoding (default: utf-8)</p> <code>'utf-8'</code> <code>delimiter</code> <code>str</code> <p>CSV delimiter (default: comma)</p> <code>','</code> Source code in <code>sqlstream/readers/csv_reader.py</code> <pre><code>def __init__(self, path: str, encoding: str = \"utf-8\", delimiter: str = \",\"):\n    \"\"\"\n    Initialize CSV reader\n\n    Args:\n        path: Path to CSV file (local or s3://)\n        encoding: File encoding (default: utf-8)\n        delimiter: CSV delimiter (default: comma)\n    \"\"\"\n    self.path_str = path\n    self.is_s3 = path.startswith(\"s3://\")\n    if not self.is_s3:\n        self.path = Path(path)\n    else:\n        self.path = None  # type: ignore\n\n    self.encoding = encoding\n    self.delimiter = delimiter\n\n    # For optimization (set by query optimizer)\n    self.filter_conditions: List[Condition] = []\n    self.required_columns: List[str] = []\n    self.limit: Optional[int] = None\n\n    if not self.is_s3 and not self.path.exists():\n        raise FileNotFoundError(f\"CSV file not found: {path}\")\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.csv_reader.CSVReader.supports_pushdown","title":"supports_pushdown","text":"<pre><code>supports_pushdown() -&gt; bool\n</code></pre> <p>CSV reader supports predicate pushdown</p> Source code in <code>sqlstream/readers/csv_reader.py</code> <pre><code>def supports_pushdown(self) -&gt; bool:\n    \"\"\"CSV reader supports predicate pushdown\"\"\"\n    return True\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.csv_reader.CSVReader.supports_column_selection","title":"supports_column_selection","text":"<pre><code>supports_column_selection() -&gt; bool\n</code></pre> <p>CSV reader supports column pruning</p> Source code in <code>sqlstream/readers/csv_reader.py</code> <pre><code>def supports_column_selection(self) -&gt; bool:\n    \"\"\"CSV reader supports column pruning\"\"\"\n    return True\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.csv_reader.CSVReader.supports_limit","title":"supports_limit","text":"<pre><code>supports_limit() -&gt; bool\n</code></pre> <p>CSV reader supports limit pushdown</p> Source code in <code>sqlstream/readers/csv_reader.py</code> <pre><code>def supports_limit(self) -&gt; bool:\n    \"\"\"CSV reader supports limit pushdown\"\"\"\n    return True\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.csv_reader.CSVReader.set_filter","title":"set_filter","text":"<pre><code>set_filter(conditions: List[Condition]) -&gt; None\n</code></pre> <p>Set filter conditions for pushdown</p> Source code in <code>sqlstream/readers/csv_reader.py</code> <pre><code>def set_filter(self, conditions: List[Condition]) -&gt; None:\n    \"\"\"Set filter conditions for pushdown\"\"\"\n    self.filter_conditions = conditions\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.csv_reader.CSVReader.set_columns","title":"set_columns","text":"<pre><code>set_columns(columns: List[str]) -&gt; None\n</code></pre> <p>Set required columns for pruning</p> Source code in <code>sqlstream/readers/csv_reader.py</code> <pre><code>def set_columns(self, columns: List[str]) -&gt; None:\n    \"\"\"Set required columns for pruning\"\"\"\n    self.required_columns = columns\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.csv_reader.CSVReader.set_limit","title":"set_limit","text":"<pre><code>set_limit(limit: int) -&gt; None\n</code></pre> <p>Set maximum rows to read for early termination</p> Source code in <code>sqlstream/readers/csv_reader.py</code> <pre><code>def set_limit(self, limit: int) -&gt; None:\n    \"\"\"Set maximum rows to read for early termination\"\"\"\n    self.limit = limit\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.csv_reader.CSVReader.read_lazy","title":"read_lazy","text":"<pre><code>read_lazy() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Lazy iterator over CSV rows</p> <p>Yields rows as dictionaries with type inference applied. If filters are set, applies them during iteration. If columns are set, only yields those columns. If limit is set, stops after yielding that many rows.</p> Source code in <code>sqlstream/readers/csv_reader.py</code> <pre><code>def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Lazy iterator over CSV rows\n\n    Yields rows as dictionaries with type inference applied.\n    If filters are set, applies them during iteration.\n    If columns are set, only yields those columns.\n    If limit is set, stops after yielding that many rows.\n    \"\"\"\n    with self._get_file_handle() as f:\n        reader = csv.DictReader(f, delimiter=self.delimiter)\n        rows_yielded = 0\n\n        for row_num, raw_row in enumerate(reader, start=2):  # Start at 2 (after header)\n            try:\n                # Apply type inference\n                row = self._infer_types(raw_row)\n\n                # Apply filters if set (predicate pushdown)\n                if self.filter_conditions:\n                    if not self._matches_filter(row):\n                        continue\n\n                # Apply column selection if set (column pruning)\n                if self.required_columns:\n                    row = {k: v for k, v in row.items() if k in self.required_columns}\n\n                yield row\n                rows_yielded += 1\n\n                # Early termination if limit reached (limit pushdown)\n                if self.limit is not None and rows_yielded &gt;= self.limit:\n                    break\n\n            except Exception as e:\n                # Handle malformed rows gracefully\n                warnings.warn(\n                    f\"Skipping malformed row {row_num} in {self.path}: {e}\",\n                    UserWarning,\n                )\n                continue\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.csv_reader.CSVReader.get_schema","title":"get_schema","text":"<pre><code>get_schema(sample_size: int = 100) -&gt; Optional[Schema]\n</code></pre> <p>Infer schema by sampling rows from the CSV file</p> <p>Parameters:</p> Name Type Description Default <code>sample_size</code> <code>int</code> <p>Number of rows to sample for type inference (default: 100)</p> <code>100</code> <p>Returns:</p> Type Description <code>Optional[Schema]</code> <p>Schema object with inferred types, or None if file is empty</p> Source code in <code>sqlstream/readers/csv_reader.py</code> <pre><code>def get_schema(self, sample_size: int = 100) -&gt; Optional[Schema]:\n    \"\"\"\n    Infer schema by sampling rows from the CSV file\n\n    Args:\n        sample_size: Number of rows to sample for type inference (default: 100)\n\n    Returns:\n        Schema object with inferred types, or None if file is empty\n    \"\"\"\n    sample_rows = []\n\n    with self._get_file_handle() as f:\n        reader = csv.DictReader(f, delimiter=self.delimiter)\n\n        # Read sample rows to infer types\n        try:\n            for i, raw_row in enumerate(reader):\n                if i &gt;= sample_size:\n                    break\n                typed_row = self._infer_types(raw_row)\n                sample_rows.append(typed_row)\n\n        except StopIteration:\n            # Empty file or fewer rows than sample_size\n            pass\n\n    if not sample_rows:\n        return None\n\n    return Schema.from_rows(sample_rows)\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.csv_reader.CSVReader.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> <p>Convert to pandas DataFrame efficiently</p> Source code in <code>sqlstream/readers/csv_reader.py</code> <pre><code>def to_dataframe(self):\n    \"\"\"\n    Convert to pandas DataFrame efficiently\n    \"\"\"\n    import pandas as pd\n\n    # Use pandas read_csv for performance\n    if self.is_s3:\n        # For S3, use s3fs via storage_options or direct s3 path if supported\n        # Since we already handle s3fs import in _get_file_handle, we can rely on pandas s3 support\n        # which uses s3fs under the hood\n        return pd.read_csv(\n            self.path_str,\n            encoding=self.encoding,\n            delimiter=self.delimiter,\n            storage_options={\"anon\": False}\n        )\n    else:\n        return pd.read_csv(\n            self.path,\n            encoding=self.encoding,\n            delimiter=self.delimiter\n        )\n</code></pre>"},{"location":"api/reference/readers/#htmlreader","title":"HTMLReader","text":""},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader","title":"HTMLReader","text":"<p>               Bases: <code>BaseReader</code></p> <p>Read tables from HTML files or URLs</p> <p>Extracts all tables from HTML and allows querying them. If multiple tables exist, you can select which one to query.</p> Example Source code in <code>sqlstream/readers/html_reader.py</code> <pre><code>class HTMLReader(BaseReader):\n    \"\"\"\n    Read tables from HTML files or URLs\n\n    Extracts all tables from HTML and allows querying them.\n    If multiple tables exist, you can select which one to query.\n\n    Example:\n        # Query first table in HTML\n        reader = HTMLReader(\"data.html\")\n\n        # Query specific table (0-indexed)\n        reader = HTMLReader(\"data.html\", table_index=1)\n\n        # Query table by matching text\n        reader = HTMLReader(\"data.html\", match=\"Sales Data\")\n    \"\"\"\n\n    def __init__(\n        self,\n        source: str,\n        table: int = 0,\n        match: Optional[str] = None,\n        **kwargs\n    ):\n        \"\"\"\n        Initialize HTML reader\n\n        Args:\n            source: Path to HTML file or URL\n            table: Which table to read (0-indexed, default: 0)\n            match: Text to match in table (tries to find table containing this text)\n            **kwargs: Additional arguments passed to pandas read_html\n        \"\"\"\n        if not PANDAS_AVAILABLE:\n            raise ImportError(\n                \"HTML reader requires pandas library. \"\n                \"Install `sqlstream[pandas]`\"\n            )\n\n        self.source = source\n        self.table = table\n        self.match = match\n        self.kwargs = kwargs\n\n        # Load tables from HTML\n        self._load_tables()\n\n        # Filter conditions and columns\n        self.filter_conditions: List[Condition] = []\n        self.required_columns: List[str] = []\n\n    def _load_tables(self) -&gt; None:\n        \"\"\"Load all tables from HTML source\"\"\"\n        try:\n            # read_html returns a list of DataFrames\n            match_pattern = self.match if self.match else \".+\"\n            self.tables = pd.read_html(\n                self.source,\n                match=match_pattern,\n                **self.kwargs\n            )\n\n            if not self.tables:\n                raise ValueError(f\"No tables found in HTML: {self.source}\")\n\n            # Select the table to work with\n            if self.table &gt;= len(self.tables):\n                raise ValueError(\n                    f\"Table index {self.table} out of range. \"\n                    f\"HTML contains {len(self.tables)} table(s).\"\n                )\n\n            self.df = self.tables[self.table]\n\n            # Clean column names (convert to strings, handle duplicates)\n            self.df.columns = [str(col) for col in self.df.columns]\n\n        except ValueError:\n            # Re-raise ValueError for validation errors\n            raise\n        except Exception as e:\n            # Only wrap actual I/O errors\n            raise IOError(f\"Failed to read HTML tables from {self.source}: {e}\")\n\n    def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"Read data lazily from the selected table\"\"\"\n        df = self.df\n\n        # Apply filters if any\n        if self.filter_conditions:\n            df = self._apply_filters(df)\n\n        # Apply column selection if any\n        if self.required_columns:\n            available_cols = [c for c in self.required_columns if c in df.columns]\n            if available_cols:\n                df = df[available_cols]\n\n        # Yield rows as dictionaries\n        yield from df.to_dict('records')\n\n    def _apply_filters(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Apply filter conditions to DataFrame\"\"\"\n        mask = pd.Series([True] * len(df), index=df.index)\n\n        for condition in self.filter_conditions:\n            col = condition.column\n            op = condition.operator\n            value = condition.value\n\n            if col not in df.columns:\n                continue\n\n            # Build condition mask\n            if op == \"=\":\n                mask &amp;= df[col] == value\n            elif op == \"&gt;\":\n                mask &amp;= df[col] &gt; value\n            elif op == \"&lt;\":\n                mask &amp;= df[col] &lt; value\n            elif op == \"&gt;=\":\n                mask &amp;= df[col] &gt;= value\n            elif op == \"&lt;=\":\n                mask &amp;= df[col] &lt;= value\n            elif op == \"!=\":\n                mask &amp;= df[col] != value\n\n        return df[mask]\n\n    def get_schema(self) -&gt; Schema:\n        \"\"\"Get schema from the selected table\"\"\"\n\n        schema = {}\n        for col in self.df.columns:\n            dtype = str(self.df[col].dtype)\n            # Map pandas dtypes to SQL-like types\n            if dtype.startswith('int'):\n                schema[col] = DataType.INTEGER\n            elif dtype.startswith('float'):\n                schema[col] = DataType.FLOAT\n            elif dtype == 'bool':\n                schema[col] = DataType.BOOLEAN\n            else:\n                schema[col] = DataType.STRING\n        return Schema(schema)\n\n    def supports_pushdown(self) -&gt; bool:\n        \"\"\"HTML reader supports filter pushdown\"\"\"\n        return True\n\n    def supports_column_selection(self) -&gt; bool:\n        \"\"\"HTML reader supports column selection\"\"\"\n        return True\n\n    def set_filter(self, conditions: List[Condition]) -&gt; None:\n        \"\"\"Set filter conditions\"\"\"\n        self.filter_conditions = conditions\n\n    def set_columns(self, columns: List[str]) -&gt; None:\n        \"\"\"Set required columns\"\"\"\n        self.required_columns = columns\n\n    def list_tables(self) -&gt; List[str]:\n        \"\"\"\n        List all tables found in the HTML\n\n        Returns:\n            List of table descriptions (first few column names)\n        \"\"\"\n        descriptions = []\n        for i, table in enumerate(self.tables):\n            cols = list(table.columns)[:3]  # First 3 columns\n            col_str = \", \".join(str(c) for c in cols)\n            if len(table.columns) &gt; 3:\n                col_str += \", ...\"\n            descriptions.append(f\"Table {i}: {col_str} ({len(table)} rows)\")\n        return descriptions\n\n    def to_dataframe(self):\n        \"\"\"\n        Convert to pandas DataFrame efficiently\n        \"\"\"\n        # HTMLReader already holds data as a DataFrame\n        return self.df\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader--query-first-table-in-html","title":"Query first table in HTML","text":"<p>reader = HTMLReader(\"data.html\")</p>"},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader--query-specific-table-0-indexed","title":"Query specific table (0-indexed)","text":"<p>reader = HTMLReader(\"data.html\", table_index=1)</p>"},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader--query-table-by-matching-text","title":"Query table by matching text","text":"<p>reader = HTMLReader(\"data.html\", match=\"Sales Data\")</p>"},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader.__init__","title":"__init__","text":"<pre><code>__init__(source: str, table: int = 0, match: Optional[str] = None, **kwargs)\n</code></pre> <p>Initialize HTML reader</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Path to HTML file or URL</p> required <code>table</code> <code>int</code> <p>Which table to read (0-indexed, default: 0)</p> <code>0</code> <code>match</code> <code>Optional[str]</code> <p>Text to match in table (tries to find table containing this text)</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to pandas read_html</p> <code>{}</code> Source code in <code>sqlstream/readers/html_reader.py</code> <pre><code>def __init__(\n    self,\n    source: str,\n    table: int = 0,\n    match: Optional[str] = None,\n    **kwargs\n):\n    \"\"\"\n    Initialize HTML reader\n\n    Args:\n        source: Path to HTML file or URL\n        table: Which table to read (0-indexed, default: 0)\n        match: Text to match in table (tries to find table containing this text)\n        **kwargs: Additional arguments passed to pandas read_html\n    \"\"\"\n    if not PANDAS_AVAILABLE:\n        raise ImportError(\n            \"HTML reader requires pandas library. \"\n            \"Install `sqlstream[pandas]`\"\n        )\n\n    self.source = source\n    self.table = table\n    self.match = match\n    self.kwargs = kwargs\n\n    # Load tables from HTML\n    self._load_tables()\n\n    # Filter conditions and columns\n    self.filter_conditions: List[Condition] = []\n    self.required_columns: List[str] = []\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader.read_lazy","title":"read_lazy","text":"<pre><code>read_lazy() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Read data lazily from the selected table</p> Source code in <code>sqlstream/readers/html_reader.py</code> <pre><code>def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"Read data lazily from the selected table\"\"\"\n    df = self.df\n\n    # Apply filters if any\n    if self.filter_conditions:\n        df = self._apply_filters(df)\n\n    # Apply column selection if any\n    if self.required_columns:\n        available_cols = [c for c in self.required_columns if c in df.columns]\n        if available_cols:\n            df = df[available_cols]\n\n    # Yield rows as dictionaries\n    yield from df.to_dict('records')\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader.get_schema","title":"get_schema","text":"<pre><code>get_schema() -&gt; Schema\n</code></pre> <p>Get schema from the selected table</p> Source code in <code>sqlstream/readers/html_reader.py</code> <pre><code>def get_schema(self) -&gt; Schema:\n    \"\"\"Get schema from the selected table\"\"\"\n\n    schema = {}\n    for col in self.df.columns:\n        dtype = str(self.df[col].dtype)\n        # Map pandas dtypes to SQL-like types\n        if dtype.startswith('int'):\n            schema[col] = DataType.INTEGER\n        elif dtype.startswith('float'):\n            schema[col] = DataType.FLOAT\n        elif dtype == 'bool':\n            schema[col] = DataType.BOOLEAN\n        else:\n            schema[col] = DataType.STRING\n    return Schema(schema)\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader.supports_pushdown","title":"supports_pushdown","text":"<pre><code>supports_pushdown() -&gt; bool\n</code></pre> <p>HTML reader supports filter pushdown</p> Source code in <code>sqlstream/readers/html_reader.py</code> <pre><code>def supports_pushdown(self) -&gt; bool:\n    \"\"\"HTML reader supports filter pushdown\"\"\"\n    return True\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader.supports_column_selection","title":"supports_column_selection","text":"<pre><code>supports_column_selection() -&gt; bool\n</code></pre> <p>HTML reader supports column selection</p> Source code in <code>sqlstream/readers/html_reader.py</code> <pre><code>def supports_column_selection(self) -&gt; bool:\n    \"\"\"HTML reader supports column selection\"\"\"\n    return True\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader.set_filter","title":"set_filter","text":"<pre><code>set_filter(conditions: List[Condition]) -&gt; None\n</code></pre> <p>Set filter conditions</p> Source code in <code>sqlstream/readers/html_reader.py</code> <pre><code>def set_filter(self, conditions: List[Condition]) -&gt; None:\n    \"\"\"Set filter conditions\"\"\"\n    self.filter_conditions = conditions\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader.set_columns","title":"set_columns","text":"<pre><code>set_columns(columns: List[str]) -&gt; None\n</code></pre> <p>Set required columns</p> Source code in <code>sqlstream/readers/html_reader.py</code> <pre><code>def set_columns(self, columns: List[str]) -&gt; None:\n    \"\"\"Set required columns\"\"\"\n    self.required_columns = columns\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader.list_tables","title":"list_tables","text":"<pre><code>list_tables() -&gt; List[str]\n</code></pre> <p>List all tables found in the HTML</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of table descriptions (first few column names)</p> Source code in <code>sqlstream/readers/html_reader.py</code> <pre><code>def list_tables(self) -&gt; List[str]:\n    \"\"\"\n    List all tables found in the HTML\n\n    Returns:\n        List of table descriptions (first few column names)\n    \"\"\"\n    descriptions = []\n    for i, table in enumerate(self.tables):\n        cols = list(table.columns)[:3]  # First 3 columns\n        col_str = \", \".join(str(c) for c in cols)\n        if len(table.columns) &gt; 3:\n            col_str += \", ...\"\n        descriptions.append(f\"Table {i}: {col_str} ({len(table)} rows)\")\n    return descriptions\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.html_reader.HTMLReader.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> <p>Convert to pandas DataFrame efficiently</p> Source code in <code>sqlstream/readers/html_reader.py</code> <pre><code>def to_dataframe(self):\n    \"\"\"\n    Convert to pandas DataFrame efficiently\n    \"\"\"\n    # HTMLReader already holds data as a DataFrame\n    return self.df\n</code></pre>"},{"location":"api/reference/readers/#httpreader","title":"HTTPReader","text":""},{"location":"api/reference/readers/#sqlstream.readers.http_reader.HTTPReader","title":"HTTPReader","text":"<p>               Bases: <code>BaseReader</code></p> <p>Read data from HTTP/HTTPS URLs with intelligent caching</p> <p>Automatically detects file format (CSV or Parquet) and delegates to appropriate reader. Caches downloaded files to avoid re-downloads.</p> Example <p>reader = HTTPReader(\"https://example.com/data.csv\") for row in reader.read_lazy():     print(row)</p> Source code in <code>sqlstream/readers/http_reader.py</code> <pre><code>class HTTPReader(BaseReader):\n    \"\"\"\n    Read data from HTTP/HTTPS URLs with intelligent caching\n\n    Automatically detects file format (CSV or Parquet) and delegates\n    to appropriate reader. Caches downloaded files to avoid re-downloads.\n\n    Example:\n        reader = HTTPReader(\"https://example.com/data.csv\")\n        for row in reader.read_lazy():\n            print(row)\n    \"\"\"\n\n    def __init__(\n        self,\n        url: str,\n        cache_dir: Optional[str] = None,\n        force_download: bool = False,\n        format: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initialize HTTP reader\n\n        Args:\n            url: HTTP/HTTPS URL to data file\n            cache_dir: Directory to cache downloaded files (default: system temp)\n            force_download: If True, re-download even if cached\n            format: Explicit format specification (csv, parquet, html, markdown).\n                   If not provided, will auto-detect from URL extension or content.\n            **kwargs: Additional arguments passed to the delegate reader\n        \"\"\"\n        if not HTTPX_AVAILABLE:\n            raise ImportError(\n                \"HTTP reader requires httpx library. \"\n                \"Install `sqlstream[http]`\"\n            )\n\n        self.url = url\n        self.cache_dir = Path(cache_dir) if cache_dir else Path(tempfile.gettempdir()) / \"sqlstream_cache\"\n        self.force_download = force_download\n        self.explicit_format = format\n        self.reader_kwargs = kwargs\n\n        # Ensure cache directory exists\n        self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n        # Download or get cached file\n        self.local_path = self._get_or_download()\n\n        # Detect format and create appropriate reader\n        self.delegate_reader = self._create_delegate_reader()\n\n        # Delegate filter conditions and column selection\n        self.filter_conditions: List[Condition] = []\n        self.required_columns: List[str] = []\n\n    def _get_cache_path(self) -&gt; Path:\n        \"\"\"Generate cache file path based on URL hash\"\"\"\n        # Create hash of URL for cache key\n        url_hash = hashlib.md5(self.url.encode()).hexdigest()\n\n        # Extract filename from URL\n        parsed = urlparse(self.url)\n        filename = Path(parsed.path).name or \"data\"\n\n        # Cache path: cache_dir/url_hash_filename\n        return self.cache_dir / f\"{url_hash}_{filename}\"\n\n    def _get_or_download(self) -&gt; Path:\n        \"\"\"Get cached file or download if not cached\"\"\"\n        cache_path = self._get_cache_path()\n\n        # Return cached file if it exists and we're not forcing download\n        if cache_path.exists() and not self.force_download:\n            return cache_path\n\n        # Download file\n        return self._download_file(cache_path)\n\n    def _download_file(self, target_path: Path) -&gt; Path:\n        \"\"\"Download file from URL to target path\"\"\"\n        try:\n            with httpx.stream(\"GET\", self.url, follow_redirects=True) as response:\n                response.raise_for_status()\n\n                # Write to temporary file first, then move to target\n                temp_path = target_path.with_suffix(\".tmp\")\n\n                with open(temp_path, \"wb\") as f:\n                    for chunk in response.iter_bytes(chunk_size=8192):\n                        f.write(chunk)\n\n                # Move temp file to final location\n                temp_path.rename(target_path)\n\n                return target_path\n\n        except Exception as e:\n            raise IOError(f\"Failed to download {self.url}: {e}\")\n\n    def _create_delegate_reader(self) -&gt; BaseReader:\n        \"\"\"Create appropriate reader based on file format\"\"\"\n        format_to_use = self.explicit_format\n\n        # If no explicit format, try to detect from URL extension\n        if not format_to_use:\n            path_lower = str(self.local_path).lower()\n\n            if path_lower.endswith(\".parquet\"):\n                format_to_use = \"parquet\"\n            elif path_lower.endswith(\".csv\"):\n                format_to_use = \"csv\"\n            elif path_lower.endswith((\".html\", \".htm\")):\n                format_to_use = \"html\"\n            elif path_lower.endswith((\".md\", \".markdown\")):\n                format_to_use = \"markdown\"\n            else:\n                # Try to detect from content\n                format_to_use = self._detect_format_from_content()\n\n        # Create appropriate reader based on detected/specified format\n        if format_to_use == \"parquet\":\n            if not PARQUET_AVAILABLE:\n                raise ImportError(\n                    \"Parquet files require pyarrow. \"\n                    \"Install `sqlstream[parquet]`\"\n                )\n            return ParquetReader(str(self.local_path))\n\n        elif format_to_use == \"html\":\n            try:\n                from sqlstream.readers.html_reader import HTMLReader\n                return HTMLReader(str(self.local_path), **self.reader_kwargs)\n            except ImportError:\n                raise ImportError(\n                    \"HTML reader requires pandas library. \"\n                    \"Install `sqlstream[pandas]`\"\n                )\n\n        elif format_to_use == \"markdown\":\n            from sqlstream.readers.markdown_reader import MarkdownReader\n            return MarkdownReader(str(self.local_path), **self.reader_kwargs)\n\n        else:  # csv or unknown - default to CSV\n            try:\n                return CSVReader(str(self.local_path))\n            except Exception:\n                raise ValueError(f\"Unknown file format: {format_to_use}\")\n\n    def _detect_format_from_content(self) -&gt; str:\n        \"\"\"Try to detect format by peeking at file content\"\"\"\n        try:\n            with open(self.local_path, 'rb') as f:\n                # Read first few bytes\n                header = f.read(512)\n\n            # Check for HTML\n            if b'&lt;html' in header.lower() or b'&lt;!doctype html' in header.lower() or b'&lt;table' in header.lower():\n                return \"html\"\n\n            # Check for Markdown table (simple heuristic)\n            if b'|' in header and b'---' in header:\n                return \"markdown\"\n\n            # Check for Parquet magic number\n            if header.startswith(b'PAR1'):\n                return \"parquet\"\n\n            # Default to CSV\n            return \"csv\"\n\n        except Exception:\n            # If detection fails, default to CSV\n            return \"csv\"\n\n    def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"Read data lazily, delegating to underlying reader\"\"\"\n        # Apply filter conditions to delegate\n        if self.filter_conditions:\n            self.delegate_reader.set_filter(self.filter_conditions)\n\n        # Apply column selection to delegate\n        if self.required_columns:\n            self.delegate_reader.set_columns(self.required_columns)\n\n        # Delegate to underlying reader\n        yield from self.delegate_reader.read_lazy()\n\n    def get_schema(self) -&gt; Dict[str, str]:\n        \"\"\"Get schema from delegate reader\"\"\"\n        return self.delegate_reader.get_schema()\n\n    def supports_pushdown(self) -&gt; bool:\n        \"\"\"HTTP reader supports pushdown via delegation\"\"\"\n        return self.delegate_reader.supports_pushdown()\n\n    def supports_column_selection(self) -&gt; bool:\n        \"\"\"HTTP reader supports column selection via delegation\"\"\"\n        return self.delegate_reader.supports_column_selection()\n\n    def set_filter(self, conditions: List[Condition]) -&gt; None:\n        \"\"\"Set filter conditions (will be pushed to delegate)\"\"\"\n        self.filter_conditions = conditions\n\n    def set_columns(self, columns: List[str]) -&gt; None:\n        \"\"\"Set required columns (will be pushed to delegate)\"\"\"\n        self.required_columns = columns\n\n    def clear_cache(self) -&gt; None:\n        \"\"\"Remove cached file for this URL\"\"\"\n        cache_path = self._get_cache_path()\n        if cache_path.exists():\n            cache_path.unlink()\n\n    @staticmethod\n    def clear_all_cache(cache_dir: Optional[str] = None) -&gt; int:\n        \"\"\"\n        Clear all cached files\n\n        Args:\n            cache_dir: Cache directory to clear (default: system temp)\n\n        Returns:\n            Number of files deleted\n        \"\"\"\n        cache_path = Path(cache_dir) if cache_dir else Path(tempfile.gettempdir()) / \"sqlstream_cache\"\n\n        if not cache_path.exists():\n            return 0\n\n        count = 0\n        for file in cache_path.glob(\"*\"):\n            if file.is_file():\n                file.unlink()\n                count += 1\n\n        return count\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.http_reader.HTTPReader.__init__","title":"__init__","text":"<pre><code>__init__(url: str, cache_dir: Optional[str] = None, force_download: bool = False, format: Optional[str] = None, **kwargs)\n</code></pre> <p>Initialize HTTP reader</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>HTTP/HTTPS URL to data file</p> required <code>cache_dir</code> <code>Optional[str]</code> <p>Directory to cache downloaded files (default: system temp)</p> <code>None</code> <code>force_download</code> <code>bool</code> <p>If True, re-download even if cached</p> <code>False</code> <code>format</code> <code>Optional[str]</code> <p>Explicit format specification (csv, parquet, html, markdown).    If not provided, will auto-detect from URL extension or content.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the delegate reader</p> <code>{}</code> Source code in <code>sqlstream/readers/http_reader.py</code> <pre><code>def __init__(\n    self,\n    url: str,\n    cache_dir: Optional[str] = None,\n    force_download: bool = False,\n    format: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initialize HTTP reader\n\n    Args:\n        url: HTTP/HTTPS URL to data file\n        cache_dir: Directory to cache downloaded files (default: system temp)\n        force_download: If True, re-download even if cached\n        format: Explicit format specification (csv, parquet, html, markdown).\n               If not provided, will auto-detect from URL extension or content.\n        **kwargs: Additional arguments passed to the delegate reader\n    \"\"\"\n    if not HTTPX_AVAILABLE:\n        raise ImportError(\n            \"HTTP reader requires httpx library. \"\n            \"Install `sqlstream[http]`\"\n        )\n\n    self.url = url\n    self.cache_dir = Path(cache_dir) if cache_dir else Path(tempfile.gettempdir()) / \"sqlstream_cache\"\n    self.force_download = force_download\n    self.explicit_format = format\n    self.reader_kwargs = kwargs\n\n    # Ensure cache directory exists\n    self.cache_dir.mkdir(parents=True, exist_ok=True)\n\n    # Download or get cached file\n    self.local_path = self._get_or_download()\n\n    # Detect format and create appropriate reader\n    self.delegate_reader = self._create_delegate_reader()\n\n    # Delegate filter conditions and column selection\n    self.filter_conditions: List[Condition] = []\n    self.required_columns: List[str] = []\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.http_reader.HTTPReader.read_lazy","title":"read_lazy","text":"<pre><code>read_lazy() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Read data lazily, delegating to underlying reader</p> Source code in <code>sqlstream/readers/http_reader.py</code> <pre><code>def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"Read data lazily, delegating to underlying reader\"\"\"\n    # Apply filter conditions to delegate\n    if self.filter_conditions:\n        self.delegate_reader.set_filter(self.filter_conditions)\n\n    # Apply column selection to delegate\n    if self.required_columns:\n        self.delegate_reader.set_columns(self.required_columns)\n\n    # Delegate to underlying reader\n    yield from self.delegate_reader.read_lazy()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.http_reader.HTTPReader.get_schema","title":"get_schema","text":"<pre><code>get_schema() -&gt; Dict[str, str]\n</code></pre> <p>Get schema from delegate reader</p> Source code in <code>sqlstream/readers/http_reader.py</code> <pre><code>def get_schema(self) -&gt; Dict[str, str]:\n    \"\"\"Get schema from delegate reader\"\"\"\n    return self.delegate_reader.get_schema()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.http_reader.HTTPReader.supports_pushdown","title":"supports_pushdown","text":"<pre><code>supports_pushdown() -&gt; bool\n</code></pre> <p>HTTP reader supports pushdown via delegation</p> Source code in <code>sqlstream/readers/http_reader.py</code> <pre><code>def supports_pushdown(self) -&gt; bool:\n    \"\"\"HTTP reader supports pushdown via delegation\"\"\"\n    return self.delegate_reader.supports_pushdown()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.http_reader.HTTPReader.supports_column_selection","title":"supports_column_selection","text":"<pre><code>supports_column_selection() -&gt; bool\n</code></pre> <p>HTTP reader supports column selection via delegation</p> Source code in <code>sqlstream/readers/http_reader.py</code> <pre><code>def supports_column_selection(self) -&gt; bool:\n    \"\"\"HTTP reader supports column selection via delegation\"\"\"\n    return self.delegate_reader.supports_column_selection()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.http_reader.HTTPReader.set_filter","title":"set_filter","text":"<pre><code>set_filter(conditions: List[Condition]) -&gt; None\n</code></pre> <p>Set filter conditions (will be pushed to delegate)</p> Source code in <code>sqlstream/readers/http_reader.py</code> <pre><code>def set_filter(self, conditions: List[Condition]) -&gt; None:\n    \"\"\"Set filter conditions (will be pushed to delegate)\"\"\"\n    self.filter_conditions = conditions\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.http_reader.HTTPReader.set_columns","title":"set_columns","text":"<pre><code>set_columns(columns: List[str]) -&gt; None\n</code></pre> <p>Set required columns (will be pushed to delegate)</p> Source code in <code>sqlstream/readers/http_reader.py</code> <pre><code>def set_columns(self, columns: List[str]) -&gt; None:\n    \"\"\"Set required columns (will be pushed to delegate)\"\"\"\n    self.required_columns = columns\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.http_reader.HTTPReader.clear_cache","title":"clear_cache","text":"<pre><code>clear_cache() -&gt; None\n</code></pre> <p>Remove cached file for this URL</p> Source code in <code>sqlstream/readers/http_reader.py</code> <pre><code>def clear_cache(self) -&gt; None:\n    \"\"\"Remove cached file for this URL\"\"\"\n    cache_path = self._get_cache_path()\n    if cache_path.exists():\n        cache_path.unlink()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.http_reader.HTTPReader.clear_all_cache","title":"clear_all_cache  <code>staticmethod</code>","text":"<pre><code>clear_all_cache(cache_dir: Optional[str] = None) -&gt; int\n</code></pre> <p>Clear all cached files</p> <p>Parameters:</p> Name Type Description Default <code>cache_dir</code> <code>Optional[str]</code> <p>Cache directory to clear (default: system temp)</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of files deleted</p> Source code in <code>sqlstream/readers/http_reader.py</code> <pre><code>@staticmethod\ndef clear_all_cache(cache_dir: Optional[str] = None) -&gt; int:\n    \"\"\"\n    Clear all cached files\n\n    Args:\n        cache_dir: Cache directory to clear (default: system temp)\n\n    Returns:\n        Number of files deleted\n    \"\"\"\n    cache_path = Path(cache_dir) if cache_dir else Path(tempfile.gettempdir()) / \"sqlstream_cache\"\n\n    if not cache_path.exists():\n        return 0\n\n    count = 0\n    for file in cache_path.glob(\"*\"):\n        if file.is_file():\n            file.unlink()\n            count += 1\n\n    return count\n</code></pre>"},{"location":"api/reference/readers/#markdownreader","title":"MarkdownReader","text":""},{"location":"api/reference/readers/#sqlstream.readers.markdown_reader.MarkdownReader","title":"MarkdownReader","text":"<p>               Bases: <code>BaseReader</code></p> <p>Read tables from Markdown files</p> <p>Parses Markdown tables (GFM format) and allows querying them. Supports files with multiple tables.</p> Example Markdown table Name Age City Alice 30 New York Bob 25 San Francisco Example <p>reader = MarkdownReader(\"data.md\") for row in reader.read_lazy():     print(row)</p> Source code in <code>sqlstream/readers/markdown_reader.py</code> <pre><code>class MarkdownReader(BaseReader):\n    \"\"\"\n    Read tables from Markdown files\n\n    Parses Markdown tables (GFM format) and allows querying them.\n    Supports files with multiple tables.\n\n    Example Markdown table:\n        | Name    | Age | City          |\n        |:--------|----:|--------------:|\n        | Alice   | 30  | New York      |\n        | Bob     | 25  | San Francisco |\n\n    Example:\n        reader = MarkdownReader(\"data.md\")\n        for row in reader.read_lazy():\n            print(row)\n    \"\"\"\n\n    def __init__(\n        self,\n        source: str,\n        table: int = 0,\n    ):\n        \"\"\"\n        Initialize Markdown reader\n\n        Args:\n            source: Path to Markdown file\n            table: Which table to read if multiple tables exist (0-indexed)\n        \"\"\"\n        self.source = source\n        self.table = table\n\n        # Parse tables from Markdown\n        self._parse_markdown()\n\n        # Filter conditions and columns\n        self.filter_conditions: List[Condition] = []\n        self.required_columns: List[str] = []\n\n    def _parse_markdown(self) -&gt; None:\n        \"\"\"Parse all tables from Markdown file\"\"\"\n        # Read file content\n        with open(self.source, 'r', encoding='utf-8') as f:\n            content = f.read()\n\n        # Find all tables\n        self.tables = self._extract_tables(content)\n\n        if not self.tables:\n            raise ValueError(f\"No tables found in Markdown file: {self.source}\")\n\n        if self.table &gt;= len(self.tables):\n            raise ValueError(\n                f\"Table index {self.table} out of range. \"\n                f\"Markdown contains {len(self.tables)} table(s).\"\n            )\n\n        # Select the table to work with\n        self.data = self.tables[self.table]\n        self.columns = self.data['columns']\n        self.rows = self.data['rows']\n\n    def _extract_tables(self, content: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Extract all tables from Markdown content\n\n        Returns:\n            List of table dicts with 'columns' and 'rows' keys\n        \"\"\"\n        tables = []\n        lines = content.split('\\n')\n\n        i = 0\n        while i &lt; len(lines):\n            # Check if this line looks like a table header\n            line = lines[i].strip()\n\n            if line.startswith('|') and '|' in line[1:]:\n                # Potential table start\n                # Check if next line is separator\n                if i + 1 &lt; len(lines):\n                    next_line = lines[i + 1].strip()\n                    if self._is_separator_line(next_line):\n                        # Found a table!\n                        table = self._parse_table(lines, i)\n                        if table:\n                            tables.append(table)\n                        # Skip past this table\n                        i += table.get('line_count', 2)\n                        continue\n\n            i += 1\n\n        return tables\n\n    def _is_separator_line(self, line: str) -&gt; bool:\n        \"\"\"Check if line is a table separator (e.g., |:---|---:|)\"\"\"\n        if not line.startswith('|'):\n            return False\n\n        # Remove outer pipes and split\n        parts = line.strip('|').split('|')\n\n        # Check if all parts match separator pattern\n        # Separators can be: ---, :---, ---:, :---:\n        separator_pattern = re.compile(r'^:?-+:?$')\n\n        return all(separator_pattern.match(p.strip()) for p in parts if p.strip())\n\n    def _parse_table(self, lines: List[str], start_idx: int) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Parse a single table starting at the given index\"\"\"\n        # Parse header\n        header_line = lines[start_idx].strip()\n        columns = self._parse_row(header_line, infer_types=False)\n\n        if not columns:\n            return None\n\n        # Skip separator line\n        rows = []\n        i = start_idx + 2  # Skip header and separator\n\n        # Parse data rows\n        while i &lt; len(lines):\n            line = lines[i].strip()\n\n            # Stop if we hit an empty line or non-table content\n            if not line or not line.startswith('|'):\n                break\n\n            # Skip if it's another separator (shouldn't happen in valid markdown)\n            if self._is_separator_line(line):\n                i += 1\n                continue\n\n            # Parse row\n            row_values = self._parse_row(line)\n            if row_values:\n                # Ensure row has same number of columns as header\n                # Pad with None if needed\n                while len(row_values) &lt; len(columns):\n                    row_values.append(None)\n\n                # Create row dict\n                row_dict = {columns[j]: row_values[j] for j in range(len(columns))}\n                rows.append(row_dict)\n\n            i += 1\n\n        return {\n            'columns': columns,\n            'rows': rows,\n            'line_count': i - start_idx\n        }\n\n    def _parse_row(self, line: str, infer_types: bool = True) -&gt; List[Any]:\n        \"\"\"Parse a single table row\"\"\"\n        # Remove leading/trailing pipes and whitespace\n        line = line.strip('|').strip()\n\n        # Split by pipes, handling escaped pipes\n        parts = []\n        current = \"\"\n        escaped = False\n\n        for char in line:\n            if char == '\\\\' and not escaped:\n                escaped = True\n                continue\n            elif char == '|' and not escaped:\n                parts.append(current.strip())\n                current = \"\"\n            else:\n                if escaped:\n                    current += '\\\\'\n                    escaped = False\n                current += char\n\n        # Add last part\n        if current or line.endswith('|'):\n            parts.append(current.strip())\n\n        # Clean up values\n        cleaned = []\n        for part in parts:\n            # Convert empty strings and common null representations to None\n            if not part or part.lower() in ('null', 'none', 'n/a', '-'):\n                cleaned.append(None)\n            else:\n                # Try to infer types if requested\n                if infer_types:\n                    cleaned.append(self._infer_type(part))\n                else:\n                    cleaned.append(part)\n\n        return cleaned\n\n    def _infer_type(self, value: str) -&gt; Any:\n        \"\"\"Infer and convert value to appropriate type\"\"\"\n        # Try integer\n        try:\n            return int(value)\n        except ValueError:\n            pass\n\n        # Try float\n        try:\n            return float(value)\n        except ValueError:\n            pass\n\n        # Try boolean\n        if value.lower() in ('true', 'yes'):\n            return True\n        elif value.lower() in ('false', 'no'):\n            return False\n\n       # Return as string\n        return value\n\n    def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"Read data lazily from the selected table\"\"\"\n        for row in self.rows:\n            # Apply filters if any\n            if self.filter_conditions:\n                if not self._matches_filters(row):\n                    continue\n\n            # Apply column selection if any\n            if self.required_columns:\n                filtered_row = {\n                    k: v for k, v in row.items()\n                    if k in self.required_columns\n                }\n                yield filtered_row\n            else:\n                yield row\n\n    def _matches_filters(self, row: Dict[str, Any]) -&gt; bool:\n        \"\"\"Check if row matches all filter conditions\"\"\"\n        for condition in self.filter_conditions:\n            col = condition.column\n            op = condition.operator\n            value = condition.value\n\n            if col not in row:\n                return False\n\n            row_value = row[col]\n\n            # Handle None values\n            if row_value is None:\n                return False\n\n            # Apply operator\n            if op == \"=\":\n                if row_value != value:\n                    return False\n            elif op == \"&gt;\":\n                if row_value &lt;= value:\n                    return False\n            elif op == \"&lt;\":\n                if row_value &gt;= value:\n                    return False\n            elif op == \"&gt;=\":\n                if row_value &lt; value:\n                    return False\n            elif op == \"&lt;=\":\n                if row_value &gt; value:\n                    return False\n            elif op == \"!=\":\n                if row_value == value:\n                    return False\n\n        return True\n\n    def get_schema(self) -&gt; Schema:\n        \"\"\"Get schema by inferring types from first few rows\"\"\"\n        schema = {}\n\n        # Sample first few rows to infer types\n        sample_size = min(10, len(self.rows))\n\n        for col in self.columns:\n            # Collect non-None values\n            values = [\n                row[col] for row in self.rows[:sample_size]\n                if row.get(col) is not None\n            ]\n\n            if not values:\n                schema[col] = DataType.STRING\n                continue\n\n            # Infer type from values\n            if all(isinstance(v, bool) for v in values):\n                schema[col] = DataType.BOOLEAN\n            elif all(isinstance(v, int) for v in values):\n                schema[col] = DataType.INTEGER\n            elif all(isinstance(v, (int, float)) for v in values):\n                schema[col] = DataType.FLOAT\n            else:\n                schema[col] = DataType.STRING\n\n        return Schema(schema)\n\n    def supports_pushdown(self) -&gt; bool:\n        \"\"\"Markdown reader supports filter pushdown\"\"\"\n        return True\n\n    def supports_column_selection(self) -&gt; bool:\n        \"\"\"Markdown reader supports column selection\"\"\"\n        return True\n\n    def set_filter(self, conditions: List[Condition]) -&gt; None:\n        \"\"\"Set filter conditions\"\"\"\n        self.filter_conditions = conditions\n\n    def set_columns(self, columns: List[str]) -&gt; None:\n        \"\"\"Set required columns\"\"\"\n        self.required_columns = columns\n\n    def list_tables(self) -&gt; List[str]:\n        \"\"\"\n        List all tables found in the Markdown file\n\n        Returns:\n            List of table descriptions\n        \"\"\"\n        descriptions = []\n        for i, table in enumerate(self.tables):\n            cols = table['columns'][:3]\n            col_str = \", \".join(cols)\n            if len(table['columns']) &gt; 3:\n                col_str += \", ...\"\n            row_count = len(table['rows'])\n            descriptions.append(f\"Table {i}: {col_str} ({row_count} rows)\")\n        return descriptions\n\n    def to_dataframe(self):\n        \"\"\"\n        Convert to pandas DataFrame\n        \"\"\"\n        try:\n            import pandas as pd\n        except ImportError:\n            raise ImportError(\"Pandas is required for to_dataframe()\")\n\n        return pd.DataFrame(self.rows)\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.markdown_reader.MarkdownReader.__init__","title":"__init__","text":"<pre><code>__init__(source: str, table: int = 0)\n</code></pre> <p>Initialize Markdown reader</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>Path to Markdown file</p> required <code>table</code> <code>int</code> <p>Which table to read if multiple tables exist (0-indexed)</p> <code>0</code> Source code in <code>sqlstream/readers/markdown_reader.py</code> <pre><code>def __init__(\n    self,\n    source: str,\n    table: int = 0,\n):\n    \"\"\"\n    Initialize Markdown reader\n\n    Args:\n        source: Path to Markdown file\n        table: Which table to read if multiple tables exist (0-indexed)\n    \"\"\"\n    self.source = source\n    self.table = table\n\n    # Parse tables from Markdown\n    self._parse_markdown()\n\n    # Filter conditions and columns\n    self.filter_conditions: List[Condition] = []\n    self.required_columns: List[str] = []\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.markdown_reader.MarkdownReader.read_lazy","title":"read_lazy","text":"<pre><code>read_lazy() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Read data lazily from the selected table</p> Source code in <code>sqlstream/readers/markdown_reader.py</code> <pre><code>def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"Read data lazily from the selected table\"\"\"\n    for row in self.rows:\n        # Apply filters if any\n        if self.filter_conditions:\n            if not self._matches_filters(row):\n                continue\n\n        # Apply column selection if any\n        if self.required_columns:\n            filtered_row = {\n                k: v for k, v in row.items()\n                if k in self.required_columns\n            }\n            yield filtered_row\n        else:\n            yield row\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.markdown_reader.MarkdownReader.get_schema","title":"get_schema","text":"<pre><code>get_schema() -&gt; Schema\n</code></pre> <p>Get schema by inferring types from first few rows</p> Source code in <code>sqlstream/readers/markdown_reader.py</code> <pre><code>def get_schema(self) -&gt; Schema:\n    \"\"\"Get schema by inferring types from first few rows\"\"\"\n    schema = {}\n\n    # Sample first few rows to infer types\n    sample_size = min(10, len(self.rows))\n\n    for col in self.columns:\n        # Collect non-None values\n        values = [\n            row[col] for row in self.rows[:sample_size]\n            if row.get(col) is not None\n        ]\n\n        if not values:\n            schema[col] = DataType.STRING\n            continue\n\n        # Infer type from values\n        if all(isinstance(v, bool) for v in values):\n            schema[col] = DataType.BOOLEAN\n        elif all(isinstance(v, int) for v in values):\n            schema[col] = DataType.INTEGER\n        elif all(isinstance(v, (int, float)) for v in values):\n            schema[col] = DataType.FLOAT\n        else:\n            schema[col] = DataType.STRING\n\n    return Schema(schema)\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.markdown_reader.MarkdownReader.supports_pushdown","title":"supports_pushdown","text":"<pre><code>supports_pushdown() -&gt; bool\n</code></pre> <p>Markdown reader supports filter pushdown</p> Source code in <code>sqlstream/readers/markdown_reader.py</code> <pre><code>def supports_pushdown(self) -&gt; bool:\n    \"\"\"Markdown reader supports filter pushdown\"\"\"\n    return True\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.markdown_reader.MarkdownReader.supports_column_selection","title":"supports_column_selection","text":"<pre><code>supports_column_selection() -&gt; bool\n</code></pre> <p>Markdown reader supports column selection</p> Source code in <code>sqlstream/readers/markdown_reader.py</code> <pre><code>def supports_column_selection(self) -&gt; bool:\n    \"\"\"Markdown reader supports column selection\"\"\"\n    return True\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.markdown_reader.MarkdownReader.set_filter","title":"set_filter","text":"<pre><code>set_filter(conditions: List[Condition]) -&gt; None\n</code></pre> <p>Set filter conditions</p> Source code in <code>sqlstream/readers/markdown_reader.py</code> <pre><code>def set_filter(self, conditions: List[Condition]) -&gt; None:\n    \"\"\"Set filter conditions\"\"\"\n    self.filter_conditions = conditions\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.markdown_reader.MarkdownReader.set_columns","title":"set_columns","text":"<pre><code>set_columns(columns: List[str]) -&gt; None\n</code></pre> <p>Set required columns</p> Source code in <code>sqlstream/readers/markdown_reader.py</code> <pre><code>def set_columns(self, columns: List[str]) -&gt; None:\n    \"\"\"Set required columns\"\"\"\n    self.required_columns = columns\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.markdown_reader.MarkdownReader.list_tables","title":"list_tables","text":"<pre><code>list_tables() -&gt; List[str]\n</code></pre> <p>List all tables found in the Markdown file</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of table descriptions</p> Source code in <code>sqlstream/readers/markdown_reader.py</code> <pre><code>def list_tables(self) -&gt; List[str]:\n    \"\"\"\n    List all tables found in the Markdown file\n\n    Returns:\n        List of table descriptions\n    \"\"\"\n    descriptions = []\n    for i, table in enumerate(self.tables):\n        cols = table['columns'][:3]\n        col_str = \", \".join(cols)\n        if len(table['columns']) &gt; 3:\n            col_str += \", ...\"\n        row_count = len(table['rows'])\n        descriptions.append(f\"Table {i}: {col_str} ({row_count} rows)\")\n    return descriptions\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.markdown_reader.MarkdownReader.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> <p>Convert to pandas DataFrame</p> Source code in <code>sqlstream/readers/markdown_reader.py</code> <pre><code>def to_dataframe(self):\n    \"\"\"\n    Convert to pandas DataFrame\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"Pandas is required for to_dataframe()\")\n\n    return pd.DataFrame(self.rows)\n</code></pre>"},{"location":"api/reference/readers/#parallelcsvreader","title":"ParallelCSVReader","text":""},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.ParallelCSVReader","title":"ParallelCSVReader","text":"<p>Parallel CSV reader using chunked reading</p> Note <p>This is a placeholder for true parallel CSV reading. Implementing this correctly requires: - Chunk boundary detection (find newlines) - Header parsing and schema inference - Correct line splitting across chunks - Order preservation</p> <p>For now, this is just a wrapper around ParallelReader.</p> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>class ParallelCSVReader:\n    \"\"\"\n    Parallel CSV reader using chunked reading\n\n    Note:\n        This is a placeholder for true parallel CSV reading.\n        Implementing this correctly requires:\n        - Chunk boundary detection (find newlines)\n        - Header parsing and schema inference\n        - Correct line splitting across chunks\n        - Order preservation\n\n    For now, this is just a wrapper around ParallelReader.\n    \"\"\"\n\n    def __init__(self, path: str, num_threads: int = 4, chunk_size: int = 1024 * 1024):\n        \"\"\"\n        Initialize parallel CSV reader\n\n        Args:\n            path: Path to CSV file\n            num_threads: Number of worker threads\n            chunk_size: Chunk size in bytes\n        \"\"\"\n        from sqlstream.readers.csv_reader import CSVReader\n\n        self.reader = CSVReader(path)\n        self.parallel_reader = ParallelReader(self.reader, num_threads=num_threads)\n\n    def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"Yield rows\"\"\"\n        return self.parallel_reader.read_lazy()\n\n    def __iter__(self):\n        \"\"\"Allow iteration\"\"\"\n        return self.read_lazy()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.ParallelCSVReader.__init__","title":"__init__","text":"<pre><code>__init__(path: str, num_threads: int = 4, chunk_size: int = 1024 * 1024)\n</code></pre> <p>Initialize parallel CSV reader</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to CSV file</p> required <code>num_threads</code> <code>int</code> <p>Number of worker threads</p> <code>4</code> <code>chunk_size</code> <code>int</code> <p>Chunk size in bytes</p> <code>1024 * 1024</code> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>def __init__(self, path: str, num_threads: int = 4, chunk_size: int = 1024 * 1024):\n    \"\"\"\n    Initialize parallel CSV reader\n\n    Args:\n        path: Path to CSV file\n        num_threads: Number of worker threads\n        chunk_size: Chunk size in bytes\n    \"\"\"\n    from sqlstream.readers.csv_reader import CSVReader\n\n    self.reader = CSVReader(path)\n    self.parallel_reader = ParallelReader(self.reader, num_threads=num_threads)\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.ParallelCSVReader.read_lazy","title":"read_lazy","text":"<pre><code>read_lazy() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Yield rows</p> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"Yield rows\"\"\"\n    return self.parallel_reader.read_lazy()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.ParallelCSVReader.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Allow iteration</p> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>def __iter__(self):\n    \"\"\"Allow iteration\"\"\"\n    return self.read_lazy()\n</code></pre>"},{"location":"api/reference/readers/#parallelparquetreader","title":"ParallelParquetReader","text":""},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.ParallelParquetReader","title":"ParallelParquetReader","text":"<p>Parallel Parquet reader using row group parallelism</p> <p>Parquet files are naturally parallelizable because: - Data is split into row groups - Each row group can be read independently - PyArrow supports parallel reading natively</p> Note <p>This is a placeholder. PyArrow already supports parallel reading via threads parameter in read_table().</p> <p>For true parallel execution in SQLStream, we would: 1. Read row groups in parallel 2. Apply filters in parallel 3. Merge results in order</p> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>class ParallelParquetReader:\n    \"\"\"\n    Parallel Parquet reader using row group parallelism\n\n    Parquet files are naturally parallelizable because:\n    - Data is split into row groups\n    - Each row group can be read independently\n    - PyArrow supports parallel reading natively\n\n    Note:\n        This is a placeholder. PyArrow already supports parallel\n        reading via threads parameter in read_table().\n\n        For true parallel execution in SQLStream, we would:\n        1. Read row groups in parallel\n        2. Apply filters in parallel\n        3. Merge results in order\n    \"\"\"\n\n    def __init__(self, path: str, num_threads: int = 4):\n        \"\"\"\n        Initialize parallel Parquet reader\n\n        Args:\n            path: Path to Parquet file\n            num_threads: Number of worker threads\n        \"\"\"\n        from sqlstream.readers.parquet_reader import ParquetReader\n\n        self.reader = ParquetReader(path)\n        self.parallel_reader = ParallelReader(self.reader, num_threads=num_threads)\n\n    def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"Yield rows\"\"\"\n        return self.parallel_reader.read_lazy()\n\n    def __iter__(self):\n        \"\"\"Allow iteration\"\"\"\n        return self.read_lazy()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.ParallelParquetReader.__init__","title":"__init__","text":"<pre><code>__init__(path: str, num_threads: int = 4)\n</code></pre> <p>Initialize parallel Parquet reader</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Parquet file</p> required <code>num_threads</code> <code>int</code> <p>Number of worker threads</p> <code>4</code> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>def __init__(self, path: str, num_threads: int = 4):\n    \"\"\"\n    Initialize parallel Parquet reader\n\n    Args:\n        path: Path to Parquet file\n        num_threads: Number of worker threads\n    \"\"\"\n    from sqlstream.readers.parquet_reader import ParquetReader\n\n    self.reader = ParquetReader(path)\n    self.parallel_reader = ParallelReader(self.reader, num_threads=num_threads)\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.ParallelParquetReader.read_lazy","title":"read_lazy","text":"<pre><code>read_lazy() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Yield rows</p> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"Yield rows\"\"\"\n    return self.parallel_reader.read_lazy()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.ParallelParquetReader.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Allow iteration</p> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>def __iter__(self):\n    \"\"\"Allow iteration\"\"\"\n    return self.read_lazy()\n</code></pre>"},{"location":"api/reference/readers/#parallelreader","title":"ParallelReader","text":""},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.ParallelReader","title":"ParallelReader","text":"<p>Parallel wrapper for data readers</p> <p>Wraps any BaseReader and reads data in parallel using a thread pool.</p> Usage <pre><code>reader = CSVReader(\"large_file.csv\")\nparallel_reader = ParallelReader(reader, num_threads=4)\n\nfor row in parallel_reader:\n    process(row)\n</code></pre> <p>How it works: - Producer threads read chunks of data - Consumer (main thread) yields rows in order - Queue-based coordination - Graceful shutdown on completion or error</p> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>class ParallelReader:\n    \"\"\"\n    Parallel wrapper for data readers\n\n    Wraps any BaseReader and reads data in parallel using a thread pool.\n\n    Usage:\n        ```python\n        reader = CSVReader(\"large_file.csv\")\n        parallel_reader = ParallelReader(reader, num_threads=4)\n\n        for row in parallel_reader:\n            process(row)\n        ```\n\n    How it works:\n    - Producer threads read chunks of data\n    - Consumer (main thread) yields rows in order\n    - Queue-based coordination\n    - Graceful shutdown on completion or error\n    \"\"\"\n\n    def __init__(\n        self,\n        reader: BaseReader,\n        num_threads: int = 4,\n        queue_size: int = 100,\n    ):\n        \"\"\"\n        Initialize parallel reader\n\n        Args:\n            reader: Underlying reader to wrap\n            num_threads: Number of worker threads\n            queue_size: Maximum items in queue (backpressure)\n        \"\"\"\n        self.reader = reader\n        self.num_threads = num_threads\n        self.queue_size = queue_size\n\n        # Queue for passing rows between threads\n        self.row_queue: queue.Queue = queue.Queue(maxsize=queue_size)\n\n        # Coordination\n        self.stop_event = threading.Event()\n        self.error: Optional[Exception] = None\n        self.workers: list[threading.Thread] = []\n\n    def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Yield rows from parallel reader\n\n        Yields:\n            Dictionary representing one row\n        \"\"\"\n        # Start worker threads\n        self._start_workers()\n\n        try:\n            # Yield rows from queue until done\n            while True:\n                try:\n                    # Get row from queue (with timeout to check for errors)\n                    row = self.row_queue.get(timeout=0.1)\n\n                    # Sentinel value indicates completion\n                    if row is None:\n                        break\n\n                    yield row\n\n                except queue.Empty:\n                    # Check if workers encountered error\n                    if self.error:\n                        raise self.error\n\n                    # Check if all workers finished\n                    if not any(w.is_alive() for w in self.workers):\n                        # Workers done but no sentinel? Something went wrong\n                        if self.row_queue.empty():\n                            break\n\n        finally:\n            # Cleanup: stop workers and join threads\n            self._stop_workers()\n\n    def _start_workers(self) -&gt; None:\n        \"\"\"Start worker threads\"\"\"\n        # For now, use single-threaded mode\n        # Multi-threading in Python is tricky due to GIL\n        # and iterator protocol doesn't work well with threads\n\n        # Simple implementation: just delegate to underlying reader\n        # Future: Implement true parallelism with chunking\n\n        worker = threading.Thread(target=self._worker_function, daemon=True)\n        worker.start()\n        self.workers.append(worker)\n\n    def _worker_function(self) -&gt; None:\n        \"\"\"\n        Worker thread function\n\n        Reads rows from underlying reader and puts them in queue\n        \"\"\"\n        try:\n            for row in self.reader.read_lazy():\n                # Check if we should stop\n                if self.stop_event.is_set():\n                    break\n\n                # Put row in queue (blocks if queue full)\n                self.row_queue.put(row)\n\n            # Signal completion with sentinel\n            self.row_queue.put(None)\n\n        except Exception as e:\n            # Capture error for main thread\n            self.error = e\n            # Put sentinel to unblock main thread\n            self.row_queue.put(None)\n\n    def _stop_workers(self) -&gt; None:\n        \"\"\"Stop worker threads and clean up\"\"\"\n        # Signal workers to stop\n        self.stop_event.set()\n\n        # Wait for workers to finish (with timeout)\n        for worker in self.workers:\n            worker.join(timeout=1.0)\n\n        # Clear queue\n        while not self.row_queue.empty():\n            try:\n                self.row_queue.get_nowait()\n            except queue.Empty:\n                break\n\n    def __iter__(self):\n        \"\"\"Allow iteration\"\"\"\n        return self.read_lazy()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.ParallelReader.__init__","title":"__init__","text":"<pre><code>__init__(reader: BaseReader, num_threads: int = 4, queue_size: int = 100)\n</code></pre> <p>Initialize parallel reader</p> <p>Parameters:</p> Name Type Description Default <code>reader</code> <code>BaseReader</code> <p>Underlying reader to wrap</p> required <code>num_threads</code> <code>int</code> <p>Number of worker threads</p> <code>4</code> <code>queue_size</code> <code>int</code> <p>Maximum items in queue (backpressure)</p> <code>100</code> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>def __init__(\n    self,\n    reader: BaseReader,\n    num_threads: int = 4,\n    queue_size: int = 100,\n):\n    \"\"\"\n    Initialize parallel reader\n\n    Args:\n        reader: Underlying reader to wrap\n        num_threads: Number of worker threads\n        queue_size: Maximum items in queue (backpressure)\n    \"\"\"\n    self.reader = reader\n    self.num_threads = num_threads\n    self.queue_size = queue_size\n\n    # Queue for passing rows between threads\n    self.row_queue: queue.Queue = queue.Queue(maxsize=queue_size)\n\n    # Coordination\n    self.stop_event = threading.Event()\n    self.error: Optional[Exception] = None\n    self.workers: list[threading.Thread] = []\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.ParallelReader.read_lazy","title":"read_lazy","text":"<pre><code>read_lazy() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Yield rows from parallel reader</p> <p>Yields:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary representing one row</p> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Yield rows from parallel reader\n\n    Yields:\n        Dictionary representing one row\n    \"\"\"\n    # Start worker threads\n    self._start_workers()\n\n    try:\n        # Yield rows from queue until done\n        while True:\n            try:\n                # Get row from queue (with timeout to check for errors)\n                row = self.row_queue.get(timeout=0.1)\n\n                # Sentinel value indicates completion\n                if row is None:\n                    break\n\n                yield row\n\n            except queue.Empty:\n                # Check if workers encountered error\n                if self.error:\n                    raise self.error\n\n                # Check if all workers finished\n                if not any(w.is_alive() for w in self.workers):\n                    # Workers done but no sentinel? Something went wrong\n                    if self.row_queue.empty():\n                        break\n\n    finally:\n        # Cleanup: stop workers and join threads\n        self._stop_workers()\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.ParallelReader.__iter__","title":"__iter__","text":"<pre><code>__iter__()\n</code></pre> <p>Allow iteration</p> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>def __iter__(self):\n    \"\"\"Allow iteration\"\"\"\n    return self.read_lazy()\n</code></pre>"},{"location":"api/reference/readers/#enable_parallel_reading","title":"enable_parallel_reading","text":""},{"location":"api/reference/readers/#sqlstream.readers.parallel_reader.enable_parallel_reading","title":"enable_parallel_reading","text":"<pre><code>enable_parallel_reading(reader: BaseReader, num_threads: int = 4) -&gt; ParallelReader\n</code></pre> <p>Enable parallel reading for any reader</p> <p>Parameters:</p> Name Type Description Default <code>reader</code> <code>BaseReader</code> <p>Reader to wrap</p> required <code>num_threads</code> <code>int</code> <p>Number of worker threads</p> <code>4</code> <p>Returns:</p> Type Description <code>ParallelReader</code> <p>Parallel reader wrapper</p> Example <pre><code>reader = CSVReader(\"large_file.csv\")\nparallel_reader = enable_parallel_reading(reader, num_threads=4)\n\nfor row in parallel_reader:\n    process(row)\n</code></pre> Source code in <code>sqlstream/readers/parallel_reader.py</code> <pre><code>def enable_parallel_reading(reader: BaseReader, num_threads: int = 4) -&gt; ParallelReader:\n    \"\"\"\n    Enable parallel reading for any reader\n\n    Args:\n        reader: Reader to wrap\n        num_threads: Number of worker threads\n\n    Returns:\n        Parallel reader wrapper\n\n    Example:\n        ```python\n        reader = CSVReader(\"large_file.csv\")\n        parallel_reader = enable_parallel_reading(reader, num_threads=4)\n\n        for row in parallel_reader:\n            process(row)\n        ```\n    \"\"\"\n    return ParallelReader(reader, num_threads=num_threads)\n</code></pre>"},{"location":"api/reference/readers/#parquetreader","title":"ParquetReader","text":""},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader","title":"ParquetReader","text":"<p>               Bases: <code>BaseReader</code></p> <p>Intelligent Parquet reader with statistics-based optimization</p> <p>Features: - Lazy iteration (doesn't load entire file) - Row group statistics-based pruning (HUGE performance win) - Column selection (only read needed columns) - Predicate pushdown with statistics</p> <p>The key insight: Parquet stores min/max for each column in each row group. We can skip entire row groups if their statistics don't match our filters!</p> Example <p>Row Group 1: age [18-30], city ['LA', 'NYC'] Row Group 2: age [31-45], city ['NYC', 'SF'] Row Group 3: age [46-90], city ['LA', 'SF']</p> <p>Query: WHERE age &gt; 60 \u2192 Skip RG1 (max=30), Skip RG2 (max=45), Read RG3 only!</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>class ParquetReader(BaseReader):\n    \"\"\"\n    Intelligent Parquet reader with statistics-based optimization\n\n    Features:\n    - Lazy iteration (doesn't load entire file)\n    - Row group statistics-based pruning (HUGE performance win)\n    - Column selection (only read needed columns)\n    - Predicate pushdown with statistics\n\n    The key insight: Parquet stores min/max for each column in each row group.\n    We can skip entire row groups if their statistics don't match our filters!\n\n    Example:\n        Row Group 1: age [18-30], city ['LA', 'NYC']\n        Row Group 2: age [31-45], city ['NYC', 'SF']\n        Row Group 3: age [46-90], city ['LA', 'SF']\n\n        Query: WHERE age &gt; 60\n        \u2192 Skip RG1 (max=30), Skip RG2 (max=45), Read RG3 only!\n    \"\"\"\n\n    def __init__(self, path: str):\n        \"\"\"\n        Initialize Parquet reader\n\n        Args:\n            path: Path to Parquet file (local or s3://)\n        \"\"\"\n        self.path_str = path\n        self.is_s3 = path.startswith(\"s3://\")\n\n        filesystem = None\n        path_to_open = path\n\n        if self.is_s3:\n            try:\n                import s3fs\n                filesystem = s3fs.S3FileSystem(anon=False)\n                # s3fs expects path without protocol when filesystem is provided\n                path_to_open = path.replace(\"s3://\", \"\")\n            except ImportError:\n                raise ImportError(\"s3fs is required for S3 support. Install `sqlstream[s3]`\")\n        else:\n            self.path = Path(path)\n            path_to_open = str(self.path)\n            if not self.path.exists():\n                raise FileNotFoundError(f\"Parquet file not found: {path}\")\n\n        self.parquet_file = pq.ParquetFile(path_to_open, filesystem=filesystem)\n\n        # Optimization state (set by planner)\n        self.filter_conditions: List[Condition] = []\n        self.required_columns: List[str] = []\n        self.limit: Optional[int] = None\n        self.partition_filters: List[Condition] = []\n\n        # Parse partition information from path\n        self.partition_columns: set = set()\n        self.partition_values: Dict[str, Any] = {}\n        self._parse_partition_info()\n\n        # Check if file should be skipped based on partition filters\n        self.partition_pruned = False\n\n        # Statistics tracking\n        self.total_row_groups = self.parquet_file.num_row_groups\n        self.row_groups_scanned = 0\n\n    def supports_pushdown(self) -&gt; bool:\n        \"\"\"Parquet reader supports predicate pushdown\"\"\"\n        return True\n\n    def supports_column_selection(self) -&gt; bool:\n        \"\"\"Parquet reader supports column pruning\"\"\"\n        return True\n\n    def supports_limit(self) -&gt; bool:\n        \"\"\"Parquet reader supports limit pushdown\"\"\"\n        return True\n\n    def set_filter(self, conditions: List[Condition]) -&gt; None:\n        \"\"\"Set filter conditions for pushdown\"\"\"\n        self.filter_conditions = conditions\n\n    def set_columns(self, columns: List[str]) -&gt; None:\n        \"\"\"Set required columns for pruning\"\"\"\n        self.required_columns = columns\n\n    def set_limit(self, limit: int) -&gt; None:\n        \"\"\"Set maximum rows to read for early termination\"\"\"\n        self.limit = limit\n\n    def supports_partition_pruning(self) -&gt; bool:\n        \"\"\"Parquet reader supports partition pruning for Hive-style partitioning\"\"\"\n        return True\n\n    def get_partition_columns(self) -&gt; set:\n        \"\"\"Get partition column names detected from file path\"\"\"\n        return self.partition_columns\n\n    def set_partition_filters(self, conditions: List[Condition]) -&gt; None:\n        \"\"\"\n        Set partition filters and check if this file should be skipped\n\n        Args:\n            conditions: List of WHERE conditions on partition columns\n        \"\"\"\n        self.partition_filters = conditions\n\n        # Check if this file's partitions match the filters\n        # If not, mark it as pruned so we skip reading it\n        if not self._partition_matches_filters():\n            self.partition_pruned = True\n\n    def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Lazy iterator over Parquet rows with intelligent row group pruning\n\n        This is where the magic happens:\n        1. Check partition pruning (skip entire file if needed!)\n        2. Select row groups using statistics (skip irrelevant ones!)\n        3. Read only selected row groups\n        4. Read only required columns\n        5. Yield rows as dictionaries\n        6. Early termination if limit is reached\n        \"\"\"\n        # Step 0: Partition pruning - skip entire file if partition doesn't match\n        if self.partition_pruned:\n            # File has been pruned based on partition filters\n            # Don't read any data!\n            return\n\n        # Step 1: Intelligent row group selection\n        selected_row_groups = self._select_row_groups_with_statistics()\n\n        # Track how many we're actually reading\n        self.row_groups_scanned = len(selected_row_groups)\n\n        # Step 2: Read only selected row groups\n        rows_yielded = 0\n        for rg_idx in selected_row_groups:\n            # Read this row group (with column selection)\n            for row in self._read_row_group(rg_idx):\n                # Add partition columns to the row\n                # These are \"virtual\" columns from the directory structure\n                for col, value in self.partition_values.items():\n                    row[col] = value\n\n                yield row\n                rows_yielded += 1\n\n                # Early termination if limit reached\n                if self.limit is not None and rows_yielded &gt;= self.limit:\n                    return\n\n    def _select_row_groups_with_statistics(self) -&gt; List[int]:\n        \"\"\"\n        Use row group statistics to select which ones to read\n\n        This is THE key optimization for Parquet!\n\n        Returns:\n            List of row group indices to read\n        \"\"\"\n        if not self.filter_conditions:\n            # No filters, read all row groups\n            return list(range(self.total_row_groups))\n\n        selected = []\n        metadata = self.parquet_file.metadata\n\n        for rg_idx in range(self.total_row_groups):\n            rg_metadata = metadata.row_group(rg_idx)\n\n            # Check if this row group's statistics match our filters\n            if self._row_group_matches_filters(rg_metadata):\n                selected.append(rg_idx)\n\n        return selected\n\n    def _row_group_matches_filters(self, rg_metadata) -&gt; bool:\n        \"\"\"\n        Check if row group statistics overlap with filter conditions\n\n        Uses min/max statistics to determine if a row group could\n        possibly contain matching rows.\n\n        Args:\n            rg_metadata: Row group metadata from Parquet file\n\n        Returns:\n            True if row group might contain matching rows\n            False if we can definitively skip it\n\n        Example:\n            Filter: age &gt; 60\n            Row Group: age [18-55]\n            Result: False (max &lt; 60, so no rows can match)\n        \"\"\"\n        for condition in self.filter_conditions:\n            column_name = condition.column\n\n            # Find column index\n            try:\n                # Get schema to find column index\n                schema = self.parquet_file.schema_arrow\n                column_idx = schema.get_field_index(column_name)\n            except Exception:\n                # Column not found or no index, can't use statistics\n                continue\n\n            # Get column metadata\n            try:\n                col_metadata = rg_metadata.column(column_idx)\n\n                # Check if statistics are available\n                if not col_metadata.is_stats_set:\n                    continue\n\n                stats = col_metadata.statistics\n\n                # Get min/max values\n                min_val = stats.min\n                max_val = stats.max\n\n                # Check if filter can eliminate this row group\n                if not self._statistics_match_condition(\n                    min_val, max_val, condition\n                ):\n                    return False  # Skip this row group!\n\n            except Exception:\n                # No statistics or error, conservatively keep row group\n                continue\n\n        return True  # Row group might contain matches\n\n    def _statistics_match_condition(\n        self, min_val: Any, max_val: Any, condition: Condition\n    ) -&gt; bool:\n        \"\"\"\n        Check if min/max statistics overlap with a condition\n\n        Args:\n            min_val: Minimum value in row group\n            max_val: Maximum value in row group\n            condition: Filter condition to check\n\n        Returns:\n            True if row group might contain matches\n            False if we can skip it\n\n        Logic:\n            age &gt; 60: Skip if max_val &lt;= 60\n            age &lt; 30: Skip if min_val &gt;= 30\n            age = 25: Skip if 25 &lt; min_val or 25 &gt; max_val\n            age &gt;= 50: Skip if max_val &lt; 50\n            age &lt;= 40: Skip if min_val &gt; 40\n        \"\"\"\n        op = condition.operator\n        value = condition.value\n\n        try:\n            if op == \"&gt;\":\n                # Skip if max_val &lt;= value (all rows too small)\n                return max_val &gt; value\n\n            elif op == \"&gt;=\":\n                # Skip if max_val &lt; value\n                return max_val &gt;= value\n\n            elif op == \"&lt;\":\n                # Skip if min_val &gt;= value (all rows too large)\n                return min_val &lt; value\n\n            elif op == \"&lt;=\":\n                # Skip if min_val &gt; value\n                return min_val &lt;= value\n\n            elif op == \"=\":\n                # Skip if value outside [min_val, max_val]\n                return min_val &lt;= value &lt;= max_val\n\n            elif op == \"!=\":\n                # Can only skip if min_val == max_val == value\n                # (entire row group is the excluded value)\n                if min_val == max_val == value:\n                    return False\n                return True\n\n            else:\n                # Unknown operator, conservatively keep row group\n                return True\n\n        except (TypeError, ValueError):\n            # Comparison failed (type mismatch), keep row group\n            return True\n\n    def _read_row_group(self, rg_idx: int) -&gt; Iterator[Dict[str, Any]]:\n        \"\"\"\n        Read a specific row group\n\n        Args:\n            rg_idx: Row group index to read\n\n        Yields:\n            Rows as dictionaries\n        \"\"\"\n        # Determine which columns to read\n        # If we have filters, we need to read those columns even if not in required_columns\n        columns_to_read = set()\n\n        if self.required_columns:\n            columns_to_read.update(self.required_columns)\n\n        # Add columns needed for filtering\n        if self.filter_conditions:\n            for condition in self.filter_conditions:\n                columns_to_read.add(condition.column)\n\n        # Convert to list, or None to read all columns\n        columns = list(columns_to_read) if columns_to_read else None\n\n        # Read row group with column selection\n        table = self.parquet_file.read_row_group(rg_idx, columns=columns)\n\n        # Convert to row-oriented format and yield\n        # PyArrow returns columnar data, we need rows\n        num_rows = table.num_rows\n\n        for i in range(num_rows):\n            row = {}\n            for col_name in table.column_names:\n                col_data = table.column(col_name)\n                # Get value at index i\n                value = col_data[i].as_py()  # Convert to Python type\n                row[col_name] = value\n\n            # Apply filter conditions if set\n            # Note: Row group statistics only help us skip entire groups,\n            # but we still need to filter individual rows within selected groups\n            if self.filter_conditions:\n                if not self._matches_filter(row):\n                    continue\n\n            # Apply column selection to output\n            # (we may have read extra columns for filtering)\n            if self.required_columns:\n                row = {k: v for k, v in row.items() if k in self.required_columns}\n\n            yield row\n\n    def _matches_filter(self, row: Dict[str, Any]) -&gt; bool:\n        \"\"\"\n        Check if row matches all filter conditions\n\n        Args:\n            row: Row to check\n\n        Returns:\n            True if row matches all conditions (AND logic)\n        \"\"\"\n        for condition in self.filter_conditions:\n            if not self._evaluate_condition(row, condition):\n                return False\n        return True\n\n    def _evaluate_condition(self, row: Dict[str, Any], condition: Condition) -&gt; bool:\n        \"\"\"\n        Evaluate a single condition against a row\n\n        Args:\n            row: Row to check\n            condition: Condition to evaluate\n\n        Returns:\n            True if condition is satisfied\n        \"\"\"\n        # Get column value\n        if condition.column not in row:\n            return False\n\n        value = row[condition.column]\n\n        # Handle NULL values\n        if value is None:\n            return False\n\n        # Evaluate operator\n        op = condition.operator\n        expected = condition.value\n\n        try:\n            if op == \"=\":\n                return value == expected\n            elif op == \"&gt;\":\n                return value &gt; expected\n            elif op == \"&lt;\":\n                return value &lt; expected\n            elif op == \"&gt;=\":\n                return value &gt;= expected\n            elif op == \"&lt;=\":\n                return value &lt;= expected\n            elif op == \"!=\":\n                return value != expected\n            else:\n                # Unknown operator, conservatively keep row\n                return True\n\n        except TypeError:\n            # Type mismatch (e.g., comparing string to int)\n            # This is fine - row just doesn't match\n            return False\n\n    def get_schema(self) -&gt; Schema:\n        \"\"\"\n        Get schema from Parquet metadata\n\n        Returns:\n            Dictionary mapping column names to types\n        \"\"\"\n        schema: Dict[str, DataType] = {}\n        arrow_schema = self.parquet_file.schema_arrow\n\n        for i in range(len(arrow_schema)):\n            field = arrow_schema.field(i)\n            # Map Arrow types to simple type names\n            schema[field.name] = self._arrow_type_to_dtype(field.type)\n\n        return Schema(schema)\n\n    def _arrow_type_to_string(self, arrow_type) -&gt; str:\n        \"\"\"\n        Convert PyArrow type to simple string\n\n        Args:\n            arrow_type: PyArrow data type\n\n        Returns:\n            Simple type name (int, float, string, etc.)\n        \"\"\"\n        type_str = str(arrow_type)\n\n        if \"int\" in type_str.lower():\n            return \"int\"\n        elif \"float\" in type_str.lower() or \"double\" in type_str.lower() or \"decimal\" in type_str.lower():\n            return \"float\"\n        elif \"string\" in type_str.lower() or \"utf8\" in type_str.lower():\n            return \"string\"\n        elif \"bool\" in type_str.lower():\n            return \"bool\"\n        elif \"date\" in type_str.lower():\n            return \"date\"\n        elif \"timestamp\" in type_str.lower():\n            return \"datetime\"\n        else:\n            return type_str\n\n    def _arrow_type_to_dtype(self, arrow_type) -&gt; DataType:\n        \"\"\"\n        Convert PyArrow type to SQLStream data type\n\n        Args:\n            arrow_type: PyArrow data type\n\n        Returns:\n            SQLStream data type\n        \"\"\"\n        simple_type = self._arrow_type_to_string(arrow_type)\n        if simple_type == \"int\":\n            return DataType.INTEGER\n        elif simple_type == \"float\":\n            return DataType.FLOAT\n        elif simple_type == \"string\":\n            return DataType.STRING\n        elif simple_type == \"bool\":\n            return DataType.BOOLEAN\n        elif simple_type in [\"date\", \"datetime\"]:\n            return DataType.DATE\n        else:\n            return DataType.NULL\n\n    def _parse_partition_info(self) -&gt; None:\n        \"\"\"\n        Parse partition information from Hive-style partitioned path\n\n        Detects partition columns and values from path structure:\n        - s3://bucket/data/year=2024/month=01/data.parquet\n        - /path/to/data/country=USA/state=CA/data.parquet\n\n        Populates:\n        - self.partition_columns: {'year', 'month'} or {'country', 'state'}\n        - self.partition_values: {'year': 2024, 'month': 1} or {'country': 'USA', 'state': 'CA'}\n        \"\"\"\n        import re\n\n        # Parse the path string for partition key=value patterns\n        # Match pattern: name=value in directory structure\n        partition_pattern = re.compile(r'([^/=]+)=([^/]+)')\n\n        matches = partition_pattern.findall(self.path_str)\n\n        for key, value in matches:\n            self.partition_columns.add(key)\n\n            # Try to infer type of partition value\n            # Common patterns: year=2024 (int), month=01 (int), country=USA (str)\n            typed_value = self._infer_partition_value_type(value)\n            self.partition_values[key] = typed_value\n\n    def _infer_partition_value_type(self, value: str) -&gt; Any:\n        \"\"\"\n        Infer the type of a partition value string\n\n        Args:\n            value: String value from partition path (e.g., \"2024\", \"01\", \"USA\")\n\n        Returns:\n            Typed value (int, float, or str)\n        \"\"\"\n        # Try int first\n        try:\n            return int(value)\n        except ValueError:\n            pass\n\n        # Try float\n        try:\n            return float(value)\n        except ValueError:\n            pass\n\n        # Default to string\n        return value\n\n    def _partition_matches_filters(self) -&gt; bool:\n        \"\"\"\n        Check if this file's partition values match the partition filters\n\n        Returns:\n            True if partition matches (file should be read)\n            False if partition doesn't match (file should be skipped)\n\n        Example:\n            File path: s3://bucket/data/year=2024/month=01/data.parquet\n            Partition values: {'year': 2024, 'month': 1}\n\n            Filter: year &gt; 2023 AND month = 1\n            Result: True (matches)\n\n            Filter: year = 2025\n            Result: False (doesn't match, skip file!)\n        \"\"\"\n        if not self.partition_filters:\n            # No filters, all partitions match\n            return True\n\n        # Check each partition filter condition\n        for condition in self.partition_filters:\n            column = condition.column\n            operator = condition.operator\n            expected = condition.value\n\n            # Get partition value for this column\n            if column not in self.partition_values:\n                # Filter references a partition column that doesn't exist in path\n                # Conservatively assume match (don't skip)\n                continue\n\n            actual = self.partition_values[column]\n\n            # Evaluate condition\n            if not self._evaluate_partition_condition(actual, operator, expected):\n                # Condition failed, skip this file!\n                return False\n\n        # All conditions passed\n        return True\n\n    def _evaluate_partition_condition(\n        self, actual: Any, operator: str, expected: Any\n    ) -&gt; bool:\n        \"\"\"\n        Evaluate a partition filter condition\n\n        Args:\n            actual: Actual partition value from path\n            operator: Comparison operator (=, &gt;, &lt;, etc.)\n            expected: Expected value from WHERE clause\n\n        Returns:\n            True if condition is satisfied, False otherwise\n        \"\"\"\n        try:\n            if operator == \"=\":\n                return actual == expected\n            elif operator == \"&gt;\":\n                return actual &gt; expected\n            elif operator == \"&lt;\":\n                return actual &lt; expected\n            elif operator == \"&gt;=\":\n                return actual &gt;= expected\n            elif operator == \"&lt;=\":\n                return actual &lt;= expected\n            elif operator == \"!=\":\n                return actual != expected\n            else:\n                # Unknown operator, conservatively match\n                return True\n\n        except (TypeError, ValueError):\n            # Type mismatch, conservatively match\n            return True\n\n    def get_statistics(self) -&gt; Dict[str, Any]:\n        \"\"\"\n        Get statistics about row group pruning\n\n        Returns:\n            Dictionary with pruning statistics\n        \"\"\"\n        return {\n            \"total_row_groups\": self.total_row_groups,\n            \"row_groups_scanned\": self.row_groups_scanned,\n            \"row_groups_skipped\": self.total_row_groups - self.row_groups_scanned,\n            \"pruning_ratio\": (\n                (self.total_row_groups - self.row_groups_scanned)\n                / self.total_row_groups\n                if self.total_row_groups &gt; 0\n                else 0\n            ),\n            \"partition_pruned\": self.partition_pruned,\n            \"partition_columns\": list(self.partition_columns),\n            \"partition_values\": self.partition_values,\n        }\n\n    def to_dataframe(self):\n        \"\"\"\n        Convert to pandas DataFrame efficiently\n        \"\"\"\n        import pandas as pd\n\n        # Use pandas read_parquet for performance\n        if self.is_s3:\n            return pd.read_parquet(\n                self.path_str,\n                storage_options={\"anon\": False}\n            )\n        else:\n            return pd.read_parquet(self.path)\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.__init__","title":"__init__","text":"<pre><code>__init__(path: str)\n</code></pre> <p>Initialize Parquet reader</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Parquet file (local or s3://)</p> required Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def __init__(self, path: str):\n    \"\"\"\n    Initialize Parquet reader\n\n    Args:\n        path: Path to Parquet file (local or s3://)\n    \"\"\"\n    self.path_str = path\n    self.is_s3 = path.startswith(\"s3://\")\n\n    filesystem = None\n    path_to_open = path\n\n    if self.is_s3:\n        try:\n            import s3fs\n            filesystem = s3fs.S3FileSystem(anon=False)\n            # s3fs expects path without protocol when filesystem is provided\n            path_to_open = path.replace(\"s3://\", \"\")\n        except ImportError:\n            raise ImportError(\"s3fs is required for S3 support. Install `sqlstream[s3]`\")\n    else:\n        self.path = Path(path)\n        path_to_open = str(self.path)\n        if not self.path.exists():\n            raise FileNotFoundError(f\"Parquet file not found: {path}\")\n\n    self.parquet_file = pq.ParquetFile(path_to_open, filesystem=filesystem)\n\n    # Optimization state (set by planner)\n    self.filter_conditions: List[Condition] = []\n    self.required_columns: List[str] = []\n    self.limit: Optional[int] = None\n    self.partition_filters: List[Condition] = []\n\n    # Parse partition information from path\n    self.partition_columns: set = set()\n    self.partition_values: Dict[str, Any] = {}\n    self._parse_partition_info()\n\n    # Check if file should be skipped based on partition filters\n    self.partition_pruned = False\n\n    # Statistics tracking\n    self.total_row_groups = self.parquet_file.num_row_groups\n    self.row_groups_scanned = 0\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.supports_pushdown","title":"supports_pushdown","text":"<pre><code>supports_pushdown() -&gt; bool\n</code></pre> <p>Parquet reader supports predicate pushdown</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def supports_pushdown(self) -&gt; bool:\n    \"\"\"Parquet reader supports predicate pushdown\"\"\"\n    return True\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.supports_column_selection","title":"supports_column_selection","text":"<pre><code>supports_column_selection() -&gt; bool\n</code></pre> <p>Parquet reader supports column pruning</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def supports_column_selection(self) -&gt; bool:\n    \"\"\"Parquet reader supports column pruning\"\"\"\n    return True\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.supports_limit","title":"supports_limit","text":"<pre><code>supports_limit() -&gt; bool\n</code></pre> <p>Parquet reader supports limit pushdown</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def supports_limit(self) -&gt; bool:\n    \"\"\"Parquet reader supports limit pushdown\"\"\"\n    return True\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.set_filter","title":"set_filter","text":"<pre><code>set_filter(conditions: List[Condition]) -&gt; None\n</code></pre> <p>Set filter conditions for pushdown</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def set_filter(self, conditions: List[Condition]) -&gt; None:\n    \"\"\"Set filter conditions for pushdown\"\"\"\n    self.filter_conditions = conditions\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.set_columns","title":"set_columns","text":"<pre><code>set_columns(columns: List[str]) -&gt; None\n</code></pre> <p>Set required columns for pruning</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def set_columns(self, columns: List[str]) -&gt; None:\n    \"\"\"Set required columns for pruning\"\"\"\n    self.required_columns = columns\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.set_limit","title":"set_limit","text":"<pre><code>set_limit(limit: int) -&gt; None\n</code></pre> <p>Set maximum rows to read for early termination</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def set_limit(self, limit: int) -&gt; None:\n    \"\"\"Set maximum rows to read for early termination\"\"\"\n    self.limit = limit\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.supports_partition_pruning","title":"supports_partition_pruning","text":"<pre><code>supports_partition_pruning() -&gt; bool\n</code></pre> <p>Parquet reader supports partition pruning for Hive-style partitioning</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def supports_partition_pruning(self) -&gt; bool:\n    \"\"\"Parquet reader supports partition pruning for Hive-style partitioning\"\"\"\n    return True\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.get_partition_columns","title":"get_partition_columns","text":"<pre><code>get_partition_columns() -&gt; set\n</code></pre> <p>Get partition column names detected from file path</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def get_partition_columns(self) -&gt; set:\n    \"\"\"Get partition column names detected from file path\"\"\"\n    return self.partition_columns\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.set_partition_filters","title":"set_partition_filters","text":"<pre><code>set_partition_filters(conditions: List[Condition]) -&gt; None\n</code></pre> <p>Set partition filters and check if this file should be skipped</p> <p>Parameters:</p> Name Type Description Default <code>conditions</code> <code>List[Condition]</code> <p>List of WHERE conditions on partition columns</p> required Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def set_partition_filters(self, conditions: List[Condition]) -&gt; None:\n    \"\"\"\n    Set partition filters and check if this file should be skipped\n\n    Args:\n        conditions: List of WHERE conditions on partition columns\n    \"\"\"\n    self.partition_filters = conditions\n\n    # Check if this file's partitions match the filters\n    # If not, mark it as pruned so we skip reading it\n    if not self._partition_matches_filters():\n        self.partition_pruned = True\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.read_lazy","title":"read_lazy","text":"<pre><code>read_lazy() -&gt; Iterator[Dict[str, Any]]\n</code></pre> <p>Lazy iterator over Parquet rows with intelligent row group pruning</p> <p>This is where the magic happens: 1. Check partition pruning (skip entire file if needed!) 2. Select row groups using statistics (skip irrelevant ones!) 3. Read only selected row groups 4. Read only required columns 5. Yield rows as dictionaries 6. Early termination if limit is reached</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def read_lazy(self) -&gt; Iterator[Dict[str, Any]]:\n    \"\"\"\n    Lazy iterator over Parquet rows with intelligent row group pruning\n\n    This is where the magic happens:\n    1. Check partition pruning (skip entire file if needed!)\n    2. Select row groups using statistics (skip irrelevant ones!)\n    3. Read only selected row groups\n    4. Read only required columns\n    5. Yield rows as dictionaries\n    6. Early termination if limit is reached\n    \"\"\"\n    # Step 0: Partition pruning - skip entire file if partition doesn't match\n    if self.partition_pruned:\n        # File has been pruned based on partition filters\n        # Don't read any data!\n        return\n\n    # Step 1: Intelligent row group selection\n    selected_row_groups = self._select_row_groups_with_statistics()\n\n    # Track how many we're actually reading\n    self.row_groups_scanned = len(selected_row_groups)\n\n    # Step 2: Read only selected row groups\n    rows_yielded = 0\n    for rg_idx in selected_row_groups:\n        # Read this row group (with column selection)\n        for row in self._read_row_group(rg_idx):\n            # Add partition columns to the row\n            # These are \"virtual\" columns from the directory structure\n            for col, value in self.partition_values.items():\n                row[col] = value\n\n            yield row\n            rows_yielded += 1\n\n            # Early termination if limit reached\n            if self.limit is not None and rows_yielded &gt;= self.limit:\n                return\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.get_schema","title":"get_schema","text":"<pre><code>get_schema() -&gt; Schema\n</code></pre> <p>Get schema from Parquet metadata</p> <p>Returns:</p> Type Description <code>Schema</code> <p>Dictionary mapping column names to types</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def get_schema(self) -&gt; Schema:\n    \"\"\"\n    Get schema from Parquet metadata\n\n    Returns:\n        Dictionary mapping column names to types\n    \"\"\"\n    schema: Dict[str, DataType] = {}\n    arrow_schema = self.parquet_file.schema_arrow\n\n    for i in range(len(arrow_schema)):\n        field = arrow_schema.field(i)\n        # Map Arrow types to simple type names\n        schema[field.name] = self._arrow_type_to_dtype(field.type)\n\n    return Schema(schema)\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.get_statistics","title":"get_statistics","text":"<pre><code>get_statistics() -&gt; Dict[str, Any]\n</code></pre> <p>Get statistics about row group pruning</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with pruning statistics</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def get_statistics(self) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get statistics about row group pruning\n\n    Returns:\n        Dictionary with pruning statistics\n    \"\"\"\n    return {\n        \"total_row_groups\": self.total_row_groups,\n        \"row_groups_scanned\": self.row_groups_scanned,\n        \"row_groups_skipped\": self.total_row_groups - self.row_groups_scanned,\n        \"pruning_ratio\": (\n            (self.total_row_groups - self.row_groups_scanned)\n            / self.total_row_groups\n            if self.total_row_groups &gt; 0\n            else 0\n        ),\n        \"partition_pruned\": self.partition_pruned,\n        \"partition_columns\": list(self.partition_columns),\n        \"partition_values\": self.partition_values,\n    }\n</code></pre>"},{"location":"api/reference/readers/#sqlstream.readers.parquet_reader.ParquetReader.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> <p>Convert to pandas DataFrame efficiently</p> Source code in <code>sqlstream/readers/parquet_reader.py</code> <pre><code>def to_dataframe(self):\n    \"\"\"\n    Convert to pandas DataFrame efficiently\n    \"\"\"\n    import pandas as pd\n\n    # Use pandas read_parquet for performance\n    if self.is_s3:\n        return pd.read_parquet(\n            self.path_str,\n            storage_options={\"anon\": False}\n        )\n    else:\n        return pd.read_parquet(self.path)\n</code></pre>"},{"location":"api/reference/sql/","title":"SQL Parser Reference","text":"<p>SQL parsing and AST generation.</p>"},{"location":"api/reference/sql/#aggregatefunction","title":"AggregateFunction","text":""},{"location":"api/reference/sql/#sqlstream.sql.ast_nodes.AggregateFunction","title":"AggregateFunction  <code>dataclass</code>","text":"<p>Represents an aggregate function in SELECT clause</p> <p>Examples:</p> <p>COUNT(*), COUNT(id), SUM(amount), AVG(price), MIN(age), MAX(age)</p> Source code in <code>sqlstream/sql/ast_nodes.py</code> <pre><code>@dataclass\nclass AggregateFunction:\n    \"\"\"\n    Represents an aggregate function in SELECT clause\n\n    Examples:\n        COUNT(*), COUNT(id), SUM(amount), AVG(price), MIN(age), MAX(age)\n    \"\"\"\n\n    function: str  # 'COUNT', 'SUM', 'AVG', 'MIN', 'MAX'\n    column: str  # Column name, or '*' for COUNT(*)\n    alias: Optional[str] = None  # AS alias\n\n    def __repr__(self) -&gt; str:\n        result = f\"{self.function}({self.column})\"\n        if self.alias:\n            result += f\" AS {self.alias}\"\n        return result\n</code></pre>"},{"location":"api/reference/sql/#condition","title":"Condition","text":""},{"location":"api/reference/sql/#sqlstream.sql.ast_nodes.Condition","title":"Condition  <code>dataclass</code>","text":"<p>A single WHERE condition: column operator value</p> Source code in <code>sqlstream/sql/ast_nodes.py</code> <pre><code>@dataclass\nclass Condition:\n    \"\"\"A single WHERE condition: column operator value\"\"\"\n\n    column: str\n    operator: str  # '=', '&gt;', '&lt;', '&gt;=', '&lt;=', '!=', 'IN'\n    value: Any\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.column} {self.operator} {self.value}\"\n</code></pre>"},{"location":"api/reference/sql/#joinclause","title":"JoinClause","text":""},{"location":"api/reference/sql/#sqlstream.sql.ast_nodes.JoinClause","title":"JoinClause  <code>dataclass</code>","text":"<p>Represents a JOIN clause</p> <p>Examples:</p> <p>INNER JOIN orders ON customers.id = orders.customer_id LEFT JOIN products ON orders.product_id = products.id</p> Source code in <code>sqlstream/sql/ast_nodes.py</code> <pre><code>@dataclass\nclass JoinClause:\n    \"\"\"\n    Represents a JOIN clause\n\n    Examples:\n        INNER JOIN orders ON customers.id = orders.customer_id\n        LEFT JOIN products ON orders.product_id = products.id\n    \"\"\"\n\n    right_source: str  # Right table/file name\n    join_type: str  # 'INNER', 'LEFT', 'RIGHT'\n    on_left: str  # Left column in join condition\n    on_right: str  # Right column in join condition\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.join_type} JOIN {self.right_source} ON {self.on_left} = {self.on_right}\"\n</code></pre>"},{"location":"api/reference/sql/#orderbycolumn","title":"OrderByColumn","text":""},{"location":"api/reference/sql/#sqlstream.sql.ast_nodes.OrderByColumn","title":"OrderByColumn  <code>dataclass</code>","text":"<p>Represents a column in ORDER BY clause</p> <p>Examples:</p> <p>name ASC, age DESC</p> Source code in <code>sqlstream/sql/ast_nodes.py</code> <pre><code>@dataclass\nclass OrderByColumn:\n    \"\"\"\n    Represents a column in ORDER BY clause\n\n    Examples:\n        name ASC, age DESC\n    \"\"\"\n\n    column: str\n    direction: str = \"ASC\"  # 'ASC' or 'DESC', default ASC\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.column} {self.direction}\"\n</code></pre>"},{"location":"api/reference/sql/#selectstatement","title":"SelectStatement","text":""},{"location":"api/reference/sql/#sqlstream.sql.ast_nodes.SelectStatement","title":"SelectStatement  <code>dataclass</code>","text":"<p>Represents a complete SELECT statement</p> <p>Examples:</p> <p>SELECT * FROM data SELECT name, age FROM data WHERE age &gt; 25 SELECT * FROM data WHERE age &gt; 25 LIMIT 10 SELECT city, COUNT(*) FROM data GROUP BY city SELECT * FROM data ORDER BY age DESC LIMIT 10 SELECT * FROM customers INNER JOIN orders ON customers.id = orders.customer_id</p> Source code in <code>sqlstream/sql/ast_nodes.py</code> <pre><code>@dataclass\nclass SelectStatement:\n    \"\"\"\n    Represents a complete SELECT statement\n\n    Examples:\n        SELECT * FROM data\n        SELECT name, age FROM data WHERE age &gt; 25\n        SELECT * FROM data WHERE age &gt; 25 LIMIT 10\n        SELECT city, COUNT(*) FROM data GROUP BY city\n        SELECT * FROM data ORDER BY age DESC LIMIT 10\n        SELECT * FROM customers INNER JOIN orders ON customers.id = orders.customer_id\n    \"\"\"\n\n    columns: List[str]  # ['*'] for all columns, or specific column names\n    source: str  # Table/file name (FROM clause)\n    where: Optional[WhereClause] = None\n    group_by: Optional[List[str]] = None\n    order_by: Optional[List[OrderByColumn]] = None\n    limit: Optional[int] = None\n    aggregates: Optional[List[AggregateFunction]] = None  # Aggregate functions in SELECT\n    join: Optional[JoinClause] = None  # JOIN clause\n\n    def __repr__(self) -&gt; str:\n        parts = [f\"SELECT {', '.join(self.columns)}\"]\n        parts.append(f\"FROM {self.source}\")\n        if self.where:\n            parts.append(f\"WHERE {self.where}\")\n        if self.group_by:\n            parts.append(f\"GROUP BY {', '.join(self.group_by)}\")\n        if self.order_by:\n            parts.append(f\"ORDER BY {', '.join(str(col) for col in self.order_by)}\")\n        if self.join:\n            parts.append(str(self.join))\n        if self.limit:\n            parts.append(f\"LIMIT {self.limit}\")\n        return \" \".join(parts)\n</code></pre>"},{"location":"api/reference/sql/#whereclause","title":"WhereClause","text":""},{"location":"api/reference/sql/#sqlstream.sql.ast_nodes.WhereClause","title":"WhereClause  <code>dataclass</code>","text":"<p>WHERE clause containing multiple conditions</p> Source code in <code>sqlstream/sql/ast_nodes.py</code> <pre><code>@dataclass\nclass WhereClause:\n    \"\"\"WHERE clause containing multiple conditions\"\"\"\n\n    conditions: List[Condition]\n\n    def __repr__(self) -&gt; str:\n        return \" AND \".join(str(c) for c in self.conditions)\n</code></pre>"},{"location":"api/reference/sql/#parseerror","title":"ParseError","text":""},{"location":"api/reference/sql/#sqlstream.sql.parser.ParseError","title":"ParseError","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when SQL parsing fails</p> Source code in <code>sqlstream/sql/parser.py</code> <pre><code>class ParseError(Exception):\n    \"\"\"Raised when SQL parsing fails\"\"\"\n\n    pass\n</code></pre>"},{"location":"api/reference/sql/#sqlparser","title":"SQLParser","text":""},{"location":"api/reference/sql/#sqlstream.sql.parser.SQLParser","title":"SQLParser","text":"<p>Simple recursive descent parser for SQL</p> <p>Grammar (simplified):     SELECT_STMT := SELECT columns FROM source [WHERE conditions][LIMIT n]     columns     := * | column_name [, column_name]     conditions  := condition [AND condition]     condition   := column_name operator value     operator    := = | &gt; | &lt; | &gt;= | &lt;= | !=</p> Source code in <code>sqlstream/sql/parser.py</code> <pre><code>class SQLParser:\n    \"\"\"\n    Simple recursive descent parser for SQL\n\n    Grammar (simplified):\n        SELECT_STMT := SELECT columns FROM source [WHERE conditions] [LIMIT n]\n        columns     := * | column_name [, column_name]*\n        conditions  := condition [AND condition]*\n        condition   := column_name operator value\n        operator    := = | &gt; | &lt; | &gt;= | &lt;= | !=\n    \"\"\"\n\n    def __init__(self, sql: str):\n        self.sql = sql.strip()\n        self.tokens = self._tokenize(sql)\n        self.pos = 0\n\n    def _tokenize(self, sql: str) -&gt; List[str]:\n        \"\"\"\n        Simple tokenization by splitting on whitespace and special characters\n\n        This is intentionally simple. A production parser would use a proper lexer.\n        \"\"\"\n        # Replace commas and parens with spaces around them\n        sql = re.sub(r\"([,()])\", r\" \\1 \", sql)\n\n        # Don't split dots - we'll handle table.column qualification in the parser\n        # This allows file paths like \"/path/to/file.csv\" to remain intact\n\n        # Split on whitespace\n        tokens = sql.split()\n\n        return tokens\n\n    def current(self) -&gt; Optional[str]:\n        \"\"\"Get current token without advancing\"\"\"\n        if self.pos &lt; len(self.tokens):\n            return self.tokens[self.pos]\n        return None\n\n    def peek(self, offset: int = 1) -&gt; Optional[str]:\n        \"\"\"Look ahead at token\"\"\"\n        pos = self.pos + offset\n        if pos &lt; len(self.tokens):\n            return self.tokens[pos]\n        return None\n\n    def consume(self, expected: Optional[str] = None) -&gt; str:\n        \"\"\"\n        Consume and return current token, optionally checking it matches expected\n\n        Args:\n            expected: If provided, raises ParseError if current token doesn't match\n\n        Returns:\n            The consumed token\n\n        Raises:\n            ParseError: If expected token doesn't match or no more tokens\n        \"\"\"\n        if self.pos &gt;= len(self.tokens):\n            raise ParseError(f\"Unexpected end of query. Expected: {expected}\")\n\n        token = self.tokens[self.pos]\n\n        if expected and token.upper() != expected.upper():\n            raise ParseError(\n                f\"Expected '{expected}' but got '{token}' at position {self.pos}\"\n            )\n\n        self.pos += 1\n        return token\n\n    def parse(self) -&gt; SelectStatement:\n        \"\"\"Parse SQL query into AST\"\"\"\n        return self._parse_select()\n\n    def _parse_select(self) -&gt; SelectStatement:\n        \"\"\"Parse SELECT statement\"\"\"\n        self.consume(\"SELECT\")\n\n        # Parse columns (may include aggregates)\n        columns, aggregates = self._parse_columns()\n\n        # Parse FROM\n        self.consume(\"FROM\")\n        source = self._parse_table_name()\n\n        # Skip optional table alias (e.g., FROM 'file.csv' AS t or FROM 'file.csv' t)\n        if self.current() and self.current().upper() == \"AS\":\n            self.consume(\"AS\")\n            self.consume()  # Skip alias name\n        elif self.current() and self.current().upper() not in (\n            \"WHERE\",\n            \"GROUP\",\n            \"ORDER\",\n            \"LIMIT\",\n            \"JOIN\",\n            \"INNER\",\n            \"LEFT\",\n            \"RIGHT\",\n        ):\n            # If next token is not a keyword, it's likely an alias\n            self.consume()  # Skip alias name\n\n        # Optional JOIN clause\n        join = None\n        if self.current() and self.current().upper() in (\"INNER\", \"LEFT\", \"RIGHT\", \"JOIN\"):\n            join = self._parse_join()\n\n        # Optional WHERE clause\n        where = None\n        if self.current() and self.current().upper() == \"WHERE\":\n            where = self._parse_where()\n\n        # Optional GROUP BY clause\n        group_by = None\n        if self.current() and self.current().upper() == \"GROUP\":\n            group_by = self._parse_group_by()\n\n        # Optional ORDER BY clause\n        order_by = None\n        if self.current() and self.current().upper() == \"ORDER\":\n            order_by = self._parse_order_by()\n\n        # Optional LIMIT clause\n        limit = None\n        if self.current() and self.current().upper() == \"LIMIT\":\n            limit = self._parse_limit()\n\n        return SelectStatement(\n            columns=columns,\n            source=source,\n            where=where,\n            group_by=group_by,\n            order_by=order_by,\n            limit=limit,\n            aggregates=aggregates,\n            join=join,\n        )\n\n    def _parse_columns(self):\n        \"\"\"\n        Parse column list, including aggregate functions\n\n        Examples:\n            *\n            name, age\n            COUNT(*), SUM(amount)\n            city, COUNT(*) AS count\n\n        Returns:\n            Tuple of (columns, aggregates)\n        \"\"\"\n        columns = []\n        aggregates = []\n\n        # Check for SELECT *\n        if self.current() == \"*\":\n            self.consume()\n            return [\"*\"], None\n\n        # Parse comma-separated columns/aggregates\n        while True:\n            # Check if this is an aggregate function\n            if self._is_aggregate_function():\n                agg = self._parse_aggregate()\n                aggregates.append(agg)\n                # Add placeholder column name for aggregate\n                col_name = agg.alias if agg.alias else f\"{agg.function.lower()}_{agg.column}\"\n                columns.append(col_name)\n            else:\n                # Regular column\n                column = self.consume()\n                columns.append(column)\n\n            # Check for comma (more columns)\n            if self.current() == \",\":\n                self.consume(\",\")\n            else:\n                break\n\n        return columns, aggregates if aggregates else None\n\n    def _is_aggregate_function(self) -&gt; bool:\n        \"\"\"Check if current token is start of aggregate function\"\"\"\n        current = self.current()\n        if not current:\n            return False\n        func = current.upper()\n        return func in (\"COUNT\", \"SUM\", \"AVG\", \"MIN\", \"MAX\") and self.peek() == \"(\"\n\n    def _parse_aggregate(self) -&gt; AggregateFunction:\n        \"\"\"\n        Parse aggregate function\n\n        Examples:\n            COUNT(*)\n            COUNT(id)\n            SUM(amount) AS total\n        \"\"\"\n        # Parse function name\n        function = self.consume().upper()\n\n        # Parse opening paren\n        self.consume(\"(\")\n\n        # Parse column (or *)\n        column = self.consume()\n\n        # Parse closing paren\n        self.consume(\")\")\n\n        # Optional AS alias\n        alias = None\n        if self.current() and self.current().upper() == \"AS\":\n            self.consume(\"AS\")\n            alias = self.consume()\n\n        return AggregateFunction(function=function, column=column, alias=alias)\n\n    def _parse_where(self) -&gt; WhereClause:\n        \"\"\"\n        Parse WHERE clause\n\n        Example: WHERE age &gt; 25 AND city = 'NYC'\n        \"\"\"\n        self.consume(\"WHERE\")\n\n        conditions = []\n\n        # Parse first condition\n        conditions.append(self._parse_condition())\n\n        # Parse additional AND conditions\n        while self.current() and self.current().upper() == \"AND\":\n            self.consume(\"AND\")\n            conditions.append(self._parse_condition())\n\n        return WhereClause(conditions=conditions)\n\n    def _parse_condition(self) -&gt; Condition:\n        \"\"\"\n        Parse a single condition: column operator value\n\n        Examples:\n            age &gt; 25\n            name = 'Alice'\n            city != 'NYC'\n        \"\"\"\n        column = self.consume()\n        operator = self.consume()\n\n        # Parse value (could be number, string, or identifier)\n        value_token = self.consume()\n        value = self._parse_value(value_token)\n\n        # Validate operator\n        valid_operators = {\"=\", \"&gt;\", \"&lt;\", \"&gt;=\", \"&lt;=\", \"!=\", \"&lt;&gt;\"}\n        if operator not in valid_operators:\n            raise ParseError(f\"Invalid operator: {operator}\")\n\n        # Normalize &lt;&gt; to !=\n        if operator == \"&lt;&gt;\":\n            operator = \"!=\"\n\n        return Condition(column=column, operator=operator, value=value)\n\n    def _parse_value(self, token: str):\n        \"\"\"\n        Parse a value token into appropriate Python type\n\n        Examples:\n            '123' -&gt; 123 (int)\n            '3.14' -&gt; 3.14 (float)\n            \"'Alice'\" -&gt; 'Alice' (string, quotes removed)\n            'Alice' -&gt; 'Alice' (string)\n        \"\"\"\n        # Remove quotes if present\n        if (token.startswith(\"'\") and token.endswith(\"'\")) or (\n            token.startswith('\"') and token.endswith('\"')\n        ):\n            return token[1:-1]\n\n        # Try parsing as number\n        try:\n            # Try int first\n            if \".\" not in token:\n                return int(token)\n            # Then float\n            return float(token)\n        except ValueError:\n            # Return as string\n            return token\n\n    def _parse_table_name(self) -&gt; str:\n        \"\"\"\n        Parse a table name (file path), handling quoted and unquoted forms\n\n        Examples:\n            'data.csv' -&gt; data.csv (quotes removed)\n            \"data.csv\" -&gt; data.csv (quotes removed)\n            '/path/to/file.csv' -&gt; /path/to/file.csv (quotes removed)\n            data.csv -&gt; data.csv (no quotes)\n            data -&gt; data (table alias without extension)\n\n        Returns:\n            Table name/file path with quotes removed if present\n        \"\"\"\n        token = self.consume()\n\n        # Remove quotes if present (for file paths with spaces or special chars)\n        if (token.startswith(\"'\") and token.endswith(\"'\")) or (\n            token.startswith('\"') and token.endswith('\"')\n        ):\n            return token[1:-1]\n\n        return token\n\n    def _parse_group_by(self) -&gt; List[str]:\n        \"\"\"\n        Parse GROUP BY clause\n\n        Example: GROUP BY city, country\n        \"\"\"\n        self.consume(\"GROUP\")\n        self.consume(\"BY\")\n\n        columns = []\n\n        # Parse comma-separated column names\n        while True:\n            column = self.consume()\n            columns.append(column)\n\n            # Check for comma (more columns)\n            if self.current() == \",\":\n                self.consume(\",\")\n            else:\n                break\n\n        return columns\n\n    def _parse_order_by(self) -&gt; List[OrderByColumn]:\n        \"\"\"\n        Parse ORDER BY clause\n\n        Examples:\n            ORDER BY name\n            ORDER BY age DESC\n            ORDER BY city ASC, age DESC\n        \"\"\"\n        self.consume(\"ORDER\")\n        self.consume(\"BY\")\n\n        order_columns = []\n\n        # Parse comma-separated column specifications\n        while True:\n            column = self.consume()\n\n            # Check for optional ASC/DESC\n            direction = \"ASC\"  # Default\n            if self.current() and self.current().upper() in (\"ASC\", \"DESC\"):\n                direction = self.consume().upper()\n\n            order_columns.append(OrderByColumn(column=column, direction=direction))\n\n            # Check for comma (more columns)\n            if self.current() == \",\":\n                self.consume(\",\")\n            else:\n                break\n\n        return order_columns\n\n    def _parse_limit(self) -&gt; int:\n        \"\"\"Parse LIMIT clause\"\"\"\n        self.consume(\"LIMIT\")\n        limit_str = self.consume()\n\n        try:\n            limit = int(limit_str)\n            if limit &lt; 0:\n                raise ParseError(f\"LIMIT must be non-negative, got {limit}\")\n            return limit\n        except ValueError:\n            raise ParseError(f\"LIMIT must be an integer, got '{limit_str}'\")\n\n    def _parse_join(self) -&gt; JoinClause:\n        \"\"\"\n        Parse JOIN clause\n\n        Examples:\n            JOIN orders ON customers.id = orders.customer_id\n            INNER JOIN orders ON customers.id = orders.customer_id\n            LEFT JOIN products ON orders.product_id = products.id\n            RIGHT JOIN users ON orders.user_id = users.id\n        \"\"\"\n        # Parse join type (INNER/LEFT/RIGHT or just JOIN)\n        current = self.current().upper()\n\n        if current in (\"INNER\", \"LEFT\", \"RIGHT\"):\n            join_type = self.consume().upper()\n            self.consume(\"JOIN\")\n        elif current == \"JOIN\":\n            self.consume(\"JOIN\")\n            join_type = \"INNER\"  # Default to INNER JOIN\n        else:\n            raise ParseError(f\"Expected JOIN keyword, got '{self.current()}'\")\n\n        # Parse right table name (handle quoted paths)\n        right_source = self._parse_table_name()\n\n        # Skip optional table alias (e.g., JOIN 'file.csv' AS t or JOIN 'file.csv' t)\n        if self.current() and self.current().upper() == \"AS\":\n            self.consume(\"AS\")\n            self.consume()  # Skip alias name\n        elif self.current() and self.current().upper() not in (\"ON\", \"WHERE\", \"GROUP\", \"ORDER\", \"LIMIT\"):\n            # If next token is not a keyword, it's likely an alias\n            self.consume()  # Skip alias name\n\n        # Parse ON keyword\n        self.consume(\"ON\")\n\n        # Parse join condition: left.column = right.column\n        # Support both qualified (table.column) and unqualified (column) names\n        left_col_token = self.consume()\n\n        # Check if it's qualified (contains a dot for table.column)\n        if \".\" in left_col_token:\n            # Split on last dot to handle paths like /tmp/file.csv correctly\n            # table.column -&gt; extract column\n            parts = left_col_token.rsplit(\".\", 1)\n            left_col = parts[1] if len(parts) == 2 else left_col_token\n        else:\n            left_col = left_col_token\n\n        # Parse equals operator\n        if self.current() != \"=\":\n            raise ParseError(f\"Expected '=' in JOIN condition, got '{self.current()}'\")\n        self.consume(\"=\")\n\n        # Parse right column\n        right_col_token = self.consume()\n\n        # Check if it's qualified (contains a dot for table.column)\n        if \".\" in right_col_token:\n            # Split on last dot to extract column name\n            parts = right_col_token.rsplit(\".\", 1)\n            right_col = parts[1] if len(parts) == 2 else right_col_token\n        else:\n            right_col = right_col_token\n\n        return JoinClause(\n            right_source=right_source,\n            join_type=join_type,\n            on_left=left_col,\n            on_right=right_col,\n        )\n</code></pre>"},{"location":"api/reference/sql/#sqlstream.sql.parser.SQLParser.current","title":"current","text":"<pre><code>current() -&gt; Optional[str]\n</code></pre> <p>Get current token without advancing</p> Source code in <code>sqlstream/sql/parser.py</code> <pre><code>def current(self) -&gt; Optional[str]:\n    \"\"\"Get current token without advancing\"\"\"\n    if self.pos &lt; len(self.tokens):\n        return self.tokens[self.pos]\n    return None\n</code></pre>"},{"location":"api/reference/sql/#sqlstream.sql.parser.SQLParser.peek","title":"peek","text":"<pre><code>peek(offset: int = 1) -&gt; Optional[str]\n</code></pre> <p>Look ahead at token</p> Source code in <code>sqlstream/sql/parser.py</code> <pre><code>def peek(self, offset: int = 1) -&gt; Optional[str]:\n    \"\"\"Look ahead at token\"\"\"\n    pos = self.pos + offset\n    if pos &lt; len(self.tokens):\n        return self.tokens[pos]\n    return None\n</code></pre>"},{"location":"api/reference/sql/#sqlstream.sql.parser.SQLParser.consume","title":"consume","text":"<pre><code>consume(expected: Optional[str] = None) -&gt; str\n</code></pre> <p>Consume and return current token, optionally checking it matches expected</p> <p>Parameters:</p> Name Type Description Default <code>expected</code> <code>Optional[str]</code> <p>If provided, raises ParseError if current token doesn't match</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The consumed token</p> <p>Raises:</p> Type Description <code>ParseError</code> <p>If expected token doesn't match or no more tokens</p> Source code in <code>sqlstream/sql/parser.py</code> <pre><code>def consume(self, expected: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Consume and return current token, optionally checking it matches expected\n\n    Args:\n        expected: If provided, raises ParseError if current token doesn't match\n\n    Returns:\n        The consumed token\n\n    Raises:\n        ParseError: If expected token doesn't match or no more tokens\n    \"\"\"\n    if self.pos &gt;= len(self.tokens):\n        raise ParseError(f\"Unexpected end of query. Expected: {expected}\")\n\n    token = self.tokens[self.pos]\n\n    if expected and token.upper() != expected.upper():\n        raise ParseError(\n            f\"Expected '{expected}' but got '{token}' at position {self.pos}\"\n        )\n\n    self.pos += 1\n    return token\n</code></pre>"},{"location":"api/reference/sql/#sqlstream.sql.parser.SQLParser.parse","title":"parse","text":"<pre><code>parse() -&gt; SelectStatement\n</code></pre> <p>Parse SQL query into AST</p> Source code in <code>sqlstream/sql/parser.py</code> <pre><code>def parse(self) -&gt; SelectStatement:\n    \"\"\"Parse SQL query into AST\"\"\"\n    return self._parse_select()\n</code></pre>"},{"location":"api/reference/sql/#parse","title":"parse","text":""},{"location":"api/reference/sql/#sqlstream.sql.parser.parse","title":"parse","text":"<pre><code>parse(sql: str) -&gt; SelectStatement\n</code></pre> <p>Convenience function to parse SQL query</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL query string</p> required <p>Returns:</p> Type Description <code>SelectStatement</code> <p>Parsed SelectStatement AST</p> <p>Raises:</p> Type Description <code>ParseError</code> <p>If query is invalid</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ast = parse(\"SELECT * FROM data\")\n&gt;&gt;&gt; ast = parse(\"SELECT name, age FROM users WHERE age &gt; 25 LIMIT 10\")\n</code></pre> Source code in <code>sqlstream/sql/parser.py</code> <pre><code>def parse(sql: str) -&gt; SelectStatement:\n    \"\"\"\n    Convenience function to parse SQL query\n\n    Args:\n        sql: SQL query string\n\n    Returns:\n        Parsed SelectStatement AST\n\n    Raises:\n        ParseError: If query is invalid\n\n    Examples:\n        &gt;&gt;&gt; ast = parse(\"SELECT * FROM data\")\n        &gt;&gt;&gt; ast = parse(\"SELECT name, age FROM users WHERE age &gt; 25 LIMIT 10\")\n    \"\"\"\n    parser = SQLParser(sql)\n    return parser.parse()\n</code></pre>"},{"location":"api/reference/types/","title":"Type System Reference","text":"<p>Type system and schema definitions.</p>"},{"location":"api/reference/types/#datatype","title":"DataType","text":""},{"location":"api/reference/types/#sqlstream.core.types.DataType","title":"DataType","text":"<p>               Bases: <code>Enum</code></p> <p>SQL data types supported by SQLStream.</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>class DataType(Enum):\n    \"\"\"SQL data types supported by SQLStream.\"\"\"\n\n    INTEGER = \"INTEGER\"\n    FLOAT = \"FLOAT\"\n    STRING = \"STRING\"\n    BOOLEAN = \"BOOLEAN\"\n    DATE = \"DATE\"\n    NULL = \"NULL\"\n\n    def __str__(self) -&gt; str:\n        return self.value\n\n    def is_numeric(self) -&gt; bool:\n        \"\"\"Check if type is numeric (INTEGER or FLOAT).\"\"\"\n        return self in (DataType.INTEGER, DataType.FLOAT)\n\n    def is_comparable(self, other: \"DataType\") -&gt; bool:\n        \"\"\"Check if this type can be compared with another type.\"\"\"\n        # NULL can be compared with anything\n        if self == DataType.NULL or other == DataType.NULL:\n            return True\n\n        # Same types are always comparable\n        if self == other:\n            return True\n\n        # Numeric types are comparable with each other\n        if self.is_numeric() and other.is_numeric():\n            return True\n\n        # Strings and dates are not comparable with numbers\n        return False\n\n    def coerce_to(self, other: \"DataType\") -&gt; \"DataType\":\n        \"\"\"Determine the result type when coercing this type to another.\n\n        Used for type promotion in expressions like INTEGER + FLOAT.\n        \"\"\"\n        # NULL coerces to any type\n        if self == DataType.NULL:\n            return other\n        if other == DataType.NULL:\n            return self\n\n        # Same type\n        if self == other:\n            return self\n\n        # Numeric coercion: INT + FLOAT = FLOAT\n        if self == DataType.INTEGER and other == DataType.FLOAT:\n            return DataType.FLOAT\n        if self == DataType.FLOAT and other == DataType.INTEGER:\n            return DataType.FLOAT\n\n        # Otherwise, no coercion possible - return STRING as fallback\n        return DataType.STRING\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.DataType.is_numeric","title":"is_numeric","text":"<pre><code>is_numeric() -&gt; bool\n</code></pre> <p>Check if type is numeric (INTEGER or FLOAT).</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>def is_numeric(self) -&gt; bool:\n    \"\"\"Check if type is numeric (INTEGER or FLOAT).\"\"\"\n    return self in (DataType.INTEGER, DataType.FLOAT)\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.DataType.is_comparable","title":"is_comparable","text":"<pre><code>is_comparable(other: DataType) -&gt; bool\n</code></pre> <p>Check if this type can be compared with another type.</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>def is_comparable(self, other: \"DataType\") -&gt; bool:\n    \"\"\"Check if this type can be compared with another type.\"\"\"\n    # NULL can be compared with anything\n    if self == DataType.NULL or other == DataType.NULL:\n        return True\n\n    # Same types are always comparable\n    if self == other:\n        return True\n\n    # Numeric types are comparable with each other\n    if self.is_numeric() and other.is_numeric():\n        return True\n\n    # Strings and dates are not comparable with numbers\n    return False\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.DataType.coerce_to","title":"coerce_to","text":"<pre><code>coerce_to(other: DataType) -&gt; DataType\n</code></pre> <p>Determine the result type when coercing this type to another.</p> <p>Used for type promotion in expressions like INTEGER + FLOAT.</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>def coerce_to(self, other: \"DataType\") -&gt; \"DataType\":\n    \"\"\"Determine the result type when coercing this type to another.\n\n    Used for type promotion in expressions like INTEGER + FLOAT.\n    \"\"\"\n    # NULL coerces to any type\n    if self == DataType.NULL:\n        return other\n    if other == DataType.NULL:\n        return self\n\n    # Same type\n    if self == other:\n        return self\n\n    # Numeric coercion: INT + FLOAT = FLOAT\n    if self == DataType.INTEGER and other == DataType.FLOAT:\n        return DataType.FLOAT\n    if self == DataType.FLOAT and other == DataType.INTEGER:\n        return DataType.FLOAT\n\n    # Otherwise, no coercion possible - return STRING as fallback\n    return DataType.STRING\n</code></pre>"},{"location":"api/reference/types/#schema","title":"Schema","text":""},{"location":"api/reference/types/#sqlstream.core.types.Schema","title":"Schema","text":"<p>Schema definition for a table or query result.</p> <p>Holds column names and their corresponding data types.</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>class Schema:\n    \"\"\"Schema definition for a table or query result.\n\n    Holds column names and their corresponding data types.\n    \"\"\"\n\n    def __init__(self, columns: Dict[str, DataType]):\n        \"\"\"Initialize schema.\n\n        Args:\n            columns: Dictionary mapping column names to data types\n        \"\"\"\n        self.columns = columns\n\n    def __getitem__(self, column: str) -&gt; DataType:\n        \"\"\"Get type of a column.\"\"\"\n        return self.columns[column]\n\n    def __contains__(self, column: str) -&gt; bool:\n        \"\"\"Check if column exists in schema.\"\"\"\n        return column in self.columns\n\n    def __len__(self) -&gt; int:\n        \"\"\"Get number of columns.\"\"\"\n        return len(self.columns)\n\n    def __repr__(self) -&gt; str:\n        cols = \", \".join(f\"{name}: {dtype}\" for name, dtype in self.columns.items())\n        return f\"Schema({cols})\"\n\n    def get_column_names(self) -&gt; List[str]:\n        \"\"\"Get list of column names.\"\"\"\n        return list(self.columns.keys())\n\n    def get_column_type(self, column: str) -&gt; Optional[DataType]:\n        \"\"\"Get type of a column, or None if column doesn't exist.\"\"\"\n        return self.columns.get(column)\n\n    def validate_column(self, column: str) -&gt; None:\n        \"\"\"Validate that a column exists in the schema.\n\n        Args:\n            column: Column name to validate\n\n        Raises:\n            ValueError: If column doesn't exist\n        \"\"\"\n        if column not in self.columns:\n            available = \", \".join(self.columns.keys())\n            raise ValueError(\n                f\"Column '{column}' not found in schema. Available columns: {available}\"\n            )\n\n    @staticmethod\n    def from_row(row: Dict[str, Any]) -&gt; \"Schema\":\n        \"\"\"Infer schema from a single row.\n\n        Args:\n            row: Dictionary representing a row\n\n        Returns:\n            Inferred Schema\n        \"\"\"\n        columns = {name: infer_type(value) for name, value in row.items()}\n        return Schema(columns)\n\n    @staticmethod\n    def from_rows(rows: List[Dict[str, Any]]) -&gt; \"Schema\":\n        \"\"\"Infer schema from multiple rows.\n\n        This provides more accurate type inference by looking at multiple values.\n\n        Args:\n            rows: List of dictionaries representing rows\n\n        Returns:\n            Inferred Schema\n        \"\"\"\n        if not rows:\n            return Schema({})\n\n        # Get all column names from first row\n        column_names = list(rows[0].keys())\n\n        # Collect values for each column\n        columns = {}\n        for col_name in column_names:\n            col_values = [row.get(col_name) for row in rows if col_name in row]\n            columns[col_name] = infer_common_type(col_values)\n\n        return Schema(columns)\n\n    def merge(self, other: \"Schema\") -&gt; \"Schema\":\n        \"\"\"Merge two schemas, coercing types where needed.\n\n        This is useful for operations like UNION or JOIN where schemas need to be compatible.\n\n        Args:\n            other: Another schema to merge with\n\n        Returns:\n            Merged schema with coerced types\n        \"\"\"\n        merged = {}\n\n        # Get all column names from both schemas\n        all_columns = set(self.columns.keys()) | set(other.columns.keys())\n\n        for col_name in all_columns:\n            if col_name in self.columns and col_name in other.columns:\n                # Column exists in both - coerce types\n                merged[col_name] = self.columns[col_name].coerce_to(\n                    other.columns[col_name]\n                )\n            elif col_name in self.columns:\n                # Column only in self\n                merged[col_name] = self.columns[col_name]\n            else:\n                # Column only in other\n                merged[col_name] = other.columns[col_name]\n\n        return Schema(merged)\n\n    def to_dict(self) -&gt; Dict[str, Any]:\n        \"\"\"Convert schema to dictionary.\"\"\"\n        return {name: dtype.value for name, dtype in self.columns.items()}\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.Schema.__init__","title":"__init__","text":"<pre><code>__init__(columns: Dict[str, DataType])\n</code></pre> <p>Initialize schema.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>Dict[str, DataType]</code> <p>Dictionary mapping column names to data types</p> required Source code in <code>sqlstream/core/types.py</code> <pre><code>def __init__(self, columns: Dict[str, DataType]):\n    \"\"\"Initialize schema.\n\n    Args:\n        columns: Dictionary mapping column names to data types\n    \"\"\"\n    self.columns = columns\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.Schema.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(column: str) -&gt; DataType\n</code></pre> <p>Get type of a column.</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>def __getitem__(self, column: str) -&gt; DataType:\n    \"\"\"Get type of a column.\"\"\"\n    return self.columns[column]\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.Schema.__contains__","title":"__contains__","text":"<pre><code>__contains__(column: str) -&gt; bool\n</code></pre> <p>Check if column exists in schema.</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>def __contains__(self, column: str) -&gt; bool:\n    \"\"\"Check if column exists in schema.\"\"\"\n    return column in self.columns\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.Schema.__len__","title":"__len__","text":"<pre><code>__len__() -&gt; int\n</code></pre> <p>Get number of columns.</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Get number of columns.\"\"\"\n    return len(self.columns)\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.Schema.get_column_names","title":"get_column_names","text":"<pre><code>get_column_names() -&gt; List[str]\n</code></pre> <p>Get list of column names.</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>def get_column_names(self) -&gt; List[str]:\n    \"\"\"Get list of column names.\"\"\"\n    return list(self.columns.keys())\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.Schema.get_column_type","title":"get_column_type","text":"<pre><code>get_column_type(column: str) -&gt; Optional[DataType]\n</code></pre> <p>Get type of a column, or None if column doesn't exist.</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>def get_column_type(self, column: str) -&gt; Optional[DataType]:\n    \"\"\"Get type of a column, or None if column doesn't exist.\"\"\"\n    return self.columns.get(column)\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.Schema.validate_column","title":"validate_column","text":"<pre><code>validate_column(column: str) -&gt; None\n</code></pre> <p>Validate that a column exists in the schema.</p> <p>Parameters:</p> Name Type Description Default <code>column</code> <code>str</code> <p>Column name to validate</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If column doesn't exist</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>def validate_column(self, column: str) -&gt; None:\n    \"\"\"Validate that a column exists in the schema.\n\n    Args:\n        column: Column name to validate\n\n    Raises:\n        ValueError: If column doesn't exist\n    \"\"\"\n    if column not in self.columns:\n        available = \", \".join(self.columns.keys())\n        raise ValueError(\n            f\"Column '{column}' not found in schema. Available columns: {available}\"\n        )\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.Schema.from_row","title":"from_row  <code>staticmethod</code>","text":"<pre><code>from_row(row: Dict[str, Any]) -&gt; Schema\n</code></pre> <p>Infer schema from a single row.</p> <p>Parameters:</p> Name Type Description Default <code>row</code> <code>Dict[str, Any]</code> <p>Dictionary representing a row</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>Inferred Schema</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>@staticmethod\ndef from_row(row: Dict[str, Any]) -&gt; \"Schema\":\n    \"\"\"Infer schema from a single row.\n\n    Args:\n        row: Dictionary representing a row\n\n    Returns:\n        Inferred Schema\n    \"\"\"\n    columns = {name: infer_type(value) for name, value in row.items()}\n    return Schema(columns)\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.Schema.from_rows","title":"from_rows  <code>staticmethod</code>","text":"<pre><code>from_rows(rows: List[Dict[str, Any]]) -&gt; Schema\n</code></pre> <p>Infer schema from multiple rows.</p> <p>This provides more accurate type inference by looking at multiple values.</p> <p>Parameters:</p> Name Type Description Default <code>rows</code> <code>List[Dict[str, Any]]</code> <p>List of dictionaries representing rows</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>Inferred Schema</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>@staticmethod\ndef from_rows(rows: List[Dict[str, Any]]) -&gt; \"Schema\":\n    \"\"\"Infer schema from multiple rows.\n\n    This provides more accurate type inference by looking at multiple values.\n\n    Args:\n        rows: List of dictionaries representing rows\n\n    Returns:\n        Inferred Schema\n    \"\"\"\n    if not rows:\n        return Schema({})\n\n    # Get all column names from first row\n    column_names = list(rows[0].keys())\n\n    # Collect values for each column\n    columns = {}\n    for col_name in column_names:\n        col_values = [row.get(col_name) for row in rows if col_name in row]\n        columns[col_name] = infer_common_type(col_values)\n\n    return Schema(columns)\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.Schema.merge","title":"merge","text":"<pre><code>merge(other: Schema) -&gt; Schema\n</code></pre> <p>Merge two schemas, coercing types where needed.</p> <p>This is useful for operations like UNION or JOIN where schemas need to be compatible.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Schema</code> <p>Another schema to merge with</p> required <p>Returns:</p> Type Description <code>Schema</code> <p>Merged schema with coerced types</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>def merge(self, other: \"Schema\") -&gt; \"Schema\":\n    \"\"\"Merge two schemas, coercing types where needed.\n\n    This is useful for operations like UNION or JOIN where schemas need to be compatible.\n\n    Args:\n        other: Another schema to merge with\n\n    Returns:\n        Merged schema with coerced types\n    \"\"\"\n    merged = {}\n\n    # Get all column names from both schemas\n    all_columns = set(self.columns.keys()) | set(other.columns.keys())\n\n    for col_name in all_columns:\n        if col_name in self.columns and col_name in other.columns:\n            # Column exists in both - coerce types\n            merged[col_name] = self.columns[col_name].coerce_to(\n                other.columns[col_name]\n            )\n        elif col_name in self.columns:\n            # Column only in self\n            merged[col_name] = self.columns[col_name]\n        else:\n            # Column only in other\n            merged[col_name] = other.columns[col_name]\n\n    return Schema(merged)\n</code></pre>"},{"location":"api/reference/types/#sqlstream.core.types.Schema.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict[str, Any]\n</code></pre> <p>Convert schema to dictionary.</p> Source code in <code>sqlstream/core/types.py</code> <pre><code>def to_dict(self) -&gt; Dict[str, Any]:\n    \"\"\"Convert schema to dictionary.\"\"\"\n    return {name: dtype.value for name, dtype in self.columns.items()}\n</code></pre>"},{"location":"api/reference/types/#infer_common_type","title":"infer_common_type","text":""},{"location":"api/reference/types/#sqlstream.core.types.infer_common_type","title":"infer_common_type","text":"<pre><code>infer_common_type(values: List[Any]) -&gt; DataType\n</code></pre> <p>Infer a common type from a list of values.</p> <p>This is useful for schema inference when reading data files.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>List[Any]</code> <p>List of values to infer type from</p> required <p>Returns:</p> Type Description <code>DataType</code> <p>Common DataType that can represent all values</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; infer_common_type([1, 2, 3])\nDataType.INTEGER\n&gt;&gt;&gt; infer_common_type([1, 2.5, 3])\nDataType.FLOAT\n&gt;&gt;&gt; infer_common_type([1, \"hello\", 3])\nDataType.STRING\n</code></pre> Source code in <code>sqlstream/core/types.py</code> <pre><code>def infer_common_type(values: List[Any]) -&gt; DataType:\n    \"\"\"Infer a common type from a list of values.\n\n    This is useful for schema inference when reading data files.\n\n    Args:\n        values: List of values to infer type from\n\n    Returns:\n        Common DataType that can represent all values\n\n    Examples:\n        &gt;&gt;&gt; infer_common_type([1, 2, 3])\n        DataType.INTEGER\n        &gt;&gt;&gt; infer_common_type([1, 2.5, 3])\n        DataType.FLOAT\n        &gt;&gt;&gt; infer_common_type([1, \"hello\", 3])\n        DataType.STRING\n    \"\"\"\n    if not values:\n        return DataType.NULL\n\n    # Filter out None values for type inference\n    non_null_values = [v for v in values if v is not None]\n\n    if not non_null_values:\n        return DataType.NULL\n\n    # Infer type of first value\n    common_type = infer_type(non_null_values[0])\n\n    # Check remaining values and coerce as needed\n    for value in non_null_values[1:]:\n        value_type = infer_type(value)\n        common_type = common_type.coerce_to(value_type)\n\n    return common_type\n</code></pre>"},{"location":"api/reference/types/#infer_type","title":"infer_type","text":""},{"location":"api/reference/types/#sqlstream.core.types.infer_type","title":"infer_type","text":"<pre><code>infer_type(value: Any) -&gt; DataType\n</code></pre> <p>Infer the data type from a Python value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>Python value to infer type from</p> required <p>Returns:</p> Type Description <code>DataType</code> <p>Inferred DataType</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; infer_type(42)\nDataType.INTEGER\n&gt;&gt;&gt; infer_type(3.14)\nDataType.FLOAT\n&gt;&gt;&gt; infer_type(\"hello\")\nDataType.STRING\n&gt;&gt;&gt; infer_type(None)\nDataType.NULL\n</code></pre> Source code in <code>sqlstream/core/types.py</code> <pre><code>def infer_type(value: Any) -&gt; DataType:\n    \"\"\"Infer the data type from a Python value.\n\n    Args:\n        value: Python value to infer type from\n\n    Returns:\n        Inferred DataType\n\n    Examples:\n        &gt;&gt;&gt; infer_type(42)\n        DataType.INTEGER\n        &gt;&gt;&gt; infer_type(3.14)\n        DataType.FLOAT\n        &gt;&gt;&gt; infer_type(\"hello\")\n        DataType.STRING\n        &gt;&gt;&gt; infer_type(None)\n        DataType.NULL\n    \"\"\"\n    if value is None:\n        return DataType.NULL\n\n    if isinstance(value, bool):\n        return DataType.BOOLEAN\n\n    if isinstance(value, int):\n        return DataType.INTEGER\n\n    if isinstance(value, float):\n        return DataType.FLOAT\n\n    if isinstance(value, (date, datetime)):\n        return DataType.DATE\n\n    if isinstance(value, str):\n        # Try to infer more specific types from string values\n        if value.lower() in (\"true\", \"false\"):\n            return DataType.BOOLEAN\n\n        # Try integer\n        try:\n            int(value)\n            return DataType.INTEGER\n        except ValueError:\n            pass\n\n        # Try float\n        try:\n            float(value)\n            return DataType.FLOAT\n        except ValueError:\n            pass\n\n        # Try date (ISO format YYYY-MM-DD)\n        try:\n            if len(value) == 10 and value[4] == \"-\" and value[7] == \"-\":\n                datetime.strptime(value, \"%Y-%m-%d\")\n                return DataType.DATE\n        except ValueError:\n            pass\n\n        # Default to string\n        return DataType.STRING\n\n    # Fallback to string for unknown types\n    return DataType.STRING\n</code></pre>"},{"location":"api/reference/utils/","title":"Utilities Reference","text":"<p>Utility functions and helpers.</p>"},{"location":"api/reference/utils/#aggregator","title":"Aggregator","text":""},{"location":"api/reference/utils/#sqlstream.utils.aggregates.Aggregator","title":"Aggregator","text":"<p>Base class for aggregators</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>class Aggregator:\n    \"\"\"Base class for aggregators\"\"\"\n\n    def update(self, value: Any) -&gt; None:\n        \"\"\"Update aggregator with a new value\"\"\"\n        raise NotImplementedError\n\n    def result(self) -&gt; Any:\n        \"\"\"Get final aggregated result\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.Aggregator.update","title":"update","text":"<pre><code>update(value: Any) -&gt; None\n</code></pre> <p>Update aggregator with a new value</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def update(self, value: Any) -&gt; None:\n    \"\"\"Update aggregator with a new value\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.Aggregator.result","title":"result","text":"<pre><code>result() -&gt; Any\n</code></pre> <p>Get final aggregated result</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def result(self) -&gt; Any:\n    \"\"\"Get final aggregated result\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/reference/utils/#avgaggregator","title":"AvgAggregator","text":""},{"location":"api/reference/utils/#sqlstream.utils.aggregates.AvgAggregator","title":"AvgAggregator","text":"<p>               Bases: <code>Aggregator</code></p> <p>AVG aggregator - computes average of numeric values</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>class AvgAggregator(Aggregator):\n    \"\"\"AVG aggregator - computes average of numeric values\"\"\"\n\n    def __init__(self):\n        self.sum = 0\n        self.count = 0\n\n    def update(self, value: Any) -&gt; None:\n        \"\"\"Add value to average calculation\"\"\"\n        if value is None:\n            return\n\n        try:\n            self.sum += value\n            self.count += 1\n        except TypeError:\n            # Skip non-numeric values\n            pass\n\n    def result(self) -&gt; Optional[float]:\n        \"\"\"Return average, or None if no valid values\"\"\"\n        if self.count == 0:\n            return None\n        return self.sum / self.count\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.AvgAggregator.update","title":"update","text":"<pre><code>update(value: Any) -&gt; None\n</code></pre> <p>Add value to average calculation</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def update(self, value: Any) -&gt; None:\n    \"\"\"Add value to average calculation\"\"\"\n    if value is None:\n        return\n\n    try:\n        self.sum += value\n        self.count += 1\n    except TypeError:\n        # Skip non-numeric values\n        pass\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.AvgAggregator.result","title":"result","text":"<pre><code>result() -&gt; Optional[float]\n</code></pre> <p>Return average, or None if no valid values</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def result(self) -&gt; Optional[float]:\n    \"\"\"Return average, or None if no valid values\"\"\"\n    if self.count == 0:\n        return None\n    return self.sum / self.count\n</code></pre>"},{"location":"api/reference/utils/#countaggregator","title":"CountAggregator","text":""},{"location":"api/reference/utils/#sqlstream.utils.aggregates.CountAggregator","title":"CountAggregator","text":"<p>               Bases: <code>Aggregator</code></p> <p>COUNT aggregator - counts non-NULL values</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>class CountAggregator(Aggregator):\n    \"\"\"COUNT aggregator - counts non-NULL values\"\"\"\n\n    def __init__(self, count_star: bool = False):\n        \"\"\"\n        Initialize COUNT aggregator\n\n        Args:\n            count_star: If True, counts all rows (COUNT(*))\n                       If False, counts non-NULL values (COUNT(column))\n        \"\"\"\n        self.count_star = count_star\n        self.count = 0\n\n    def update(self, value: Any) -&gt; None:\n        \"\"\"Update count\"\"\"\n        if self.count_star or value is not None:\n            self.count += 1\n\n    def result(self) -&gt; int:\n        \"\"\"Return total count\"\"\"\n        return self.count\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.CountAggregator.__init__","title":"__init__","text":"<pre><code>__init__(count_star: bool = False)\n</code></pre> <p>Initialize COUNT aggregator</p> <p>Parameters:</p> Name Type Description Default <code>count_star</code> <code>bool</code> <p>If True, counts all rows (COUNT(*))        If False, counts non-NULL values (COUNT(column))</p> <code>False</code> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def __init__(self, count_star: bool = False):\n    \"\"\"\n    Initialize COUNT aggregator\n\n    Args:\n        count_star: If True, counts all rows (COUNT(*))\n                   If False, counts non-NULL values (COUNT(column))\n    \"\"\"\n    self.count_star = count_star\n    self.count = 0\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.CountAggregator.update","title":"update","text":"<pre><code>update(value: Any) -&gt; None\n</code></pre> <p>Update count</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def update(self, value: Any) -&gt; None:\n    \"\"\"Update count\"\"\"\n    if self.count_star or value is not None:\n        self.count += 1\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.CountAggregator.result","title":"result","text":"<pre><code>result() -&gt; int\n</code></pre> <p>Return total count</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def result(self) -&gt; int:\n    \"\"\"Return total count\"\"\"\n    return self.count\n</code></pre>"},{"location":"api/reference/utils/#maxaggregator","title":"MaxAggregator","text":""},{"location":"api/reference/utils/#sqlstream.utils.aggregates.MaxAggregator","title":"MaxAggregator","text":"<p>               Bases: <code>Aggregator</code></p> <p>MAX aggregator - finds maximum value</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>class MaxAggregator(Aggregator):\n    \"\"\"MAX aggregator - finds maximum value\"\"\"\n\n    def __init__(self):\n        self.max: Optional[Any] = None\n\n    def update(self, value: Any) -&gt; None:\n        \"\"\"Update maximum\"\"\"\n        if value is None:\n            return\n\n        if self.max is None or value &gt; self.max:\n            self.max = value\n\n    def result(self) -&gt; Optional[Any]:\n        \"\"\"Return maximum value, or None if no valid values\"\"\"\n        return self.max\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.MaxAggregator.update","title":"update","text":"<pre><code>update(value: Any) -&gt; None\n</code></pre> <p>Update maximum</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def update(self, value: Any) -&gt; None:\n    \"\"\"Update maximum\"\"\"\n    if value is None:\n        return\n\n    if self.max is None or value &gt; self.max:\n        self.max = value\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.MaxAggregator.result","title":"result","text":"<pre><code>result() -&gt; Optional[Any]\n</code></pre> <p>Return maximum value, or None if no valid values</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def result(self) -&gt; Optional[Any]:\n    \"\"\"Return maximum value, or None if no valid values\"\"\"\n    return self.max\n</code></pre>"},{"location":"api/reference/utils/#minaggregator","title":"MinAggregator","text":""},{"location":"api/reference/utils/#sqlstream.utils.aggregates.MinAggregator","title":"MinAggregator","text":"<p>               Bases: <code>Aggregator</code></p> <p>MIN aggregator - finds minimum value</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>class MinAggregator(Aggregator):\n    \"\"\"MIN aggregator - finds minimum value\"\"\"\n\n    def __init__(self):\n        self.min: Optional[Any] = None\n\n    def update(self, value: Any) -&gt; None:\n        \"\"\"Update minimum\"\"\"\n        if value is None:\n            return\n\n        if self.min is None or value &lt; self.min:\n            self.min = value\n\n    def result(self) -&gt; Optional[Any]:\n        \"\"\"Return minimum value, or None if no valid values\"\"\"\n        return self.min\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.MinAggregator.update","title":"update","text":"<pre><code>update(value: Any) -&gt; None\n</code></pre> <p>Update minimum</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def update(self, value: Any) -&gt; None:\n    \"\"\"Update minimum\"\"\"\n    if value is None:\n        return\n\n    if self.min is None or value &lt; self.min:\n        self.min = value\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.MinAggregator.result","title":"result","text":"<pre><code>result() -&gt; Optional[Any]\n</code></pre> <p>Return minimum value, or None if no valid values</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def result(self) -&gt; Optional[Any]:\n    \"\"\"Return minimum value, or None if no valid values\"\"\"\n    return self.min\n</code></pre>"},{"location":"api/reference/utils/#sumaggregator","title":"SumAggregator","text":""},{"location":"api/reference/utils/#sqlstream.utils.aggregates.SumAggregator","title":"SumAggregator","text":"<p>               Bases: <code>Aggregator</code></p> <p>SUM aggregator - sums numeric values</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>class SumAggregator(Aggregator):\n    \"\"\"SUM aggregator - sums numeric values\"\"\"\n\n    def __init__(self):\n        self.sum: Optional[float] = None\n\n    def update(self, value: Any) -&gt; None:\n        \"\"\"Add value to sum\"\"\"\n        if value is None:\n            return\n\n        if self.sum is None:\n            self.sum = 0\n\n        try:\n            self.sum += value\n        except TypeError:\n            # Skip non-numeric values\n            pass\n\n    def result(self) -&gt; Optional[float]:\n        \"\"\"Return sum, or None if no valid values\"\"\"\n        return self.sum\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.SumAggregator.update","title":"update","text":"<pre><code>update(value: Any) -&gt; None\n</code></pre> <p>Add value to sum</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def update(self, value: Any) -&gt; None:\n    \"\"\"Add value to sum\"\"\"\n    if value is None:\n        return\n\n    if self.sum is None:\n        self.sum = 0\n\n    try:\n        self.sum += value\n    except TypeError:\n        # Skip non-numeric values\n        pass\n</code></pre>"},{"location":"api/reference/utils/#sqlstream.utils.aggregates.SumAggregator.result","title":"result","text":"<pre><code>result() -&gt; Optional[float]\n</code></pre> <p>Return sum, or None if no valid values</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def result(self) -&gt; Optional[float]:\n    \"\"\"Return sum, or None if no valid values\"\"\"\n    return self.sum\n</code></pre>"},{"location":"api/reference/utils/#create_aggregator","title":"create_aggregator","text":""},{"location":"api/reference/utils/#sqlstream.utils.aggregates.create_aggregator","title":"create_aggregator","text":"<pre><code>create_aggregator(function: str, column: str) -&gt; Aggregator\n</code></pre> <p>Factory function to create appropriate aggregator</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>str</code> <p>Aggregate function name (COUNT, SUM, AVG, MIN, MAX)</p> required <code>column</code> <code>str</code> <p>Column name (or '' for COUNT())</p> required <p>Returns:</p> Type Description <code>Aggregator</code> <p>Aggregator instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If function is not recognized</p> Source code in <code>sqlstream/utils/aggregates.py</code> <pre><code>def create_aggregator(function: str, column: str) -&gt; Aggregator:\n    \"\"\"\n    Factory function to create appropriate aggregator\n\n    Args:\n        function: Aggregate function name (COUNT, SUM, AVG, MIN, MAX)\n        column: Column name (or '*' for COUNT(*))\n\n    Returns:\n        Aggregator instance\n\n    Raises:\n        ValueError: If function is not recognized\n    \"\"\"\n    function = function.upper()\n\n    if function == \"COUNT\":\n        return CountAggregator(count_star=(column == \"*\"))\n    elif function == \"SUM\":\n        return SumAggregator()\n    elif function == \"AVG\":\n        return AvgAggregator()\n    elif function == \"MIN\":\n        return MinAggregator()\n    elif function == \"MAX\":\n        return MaxAggregator()\n    else:\n        raise ValueError(f\"Unknown aggregate function: {function}\")\n</code></pre>"},{"location":"architecture/design/","title":"Design Overview","text":"<p>SQLStream is designed as a lightweight, modular SQL query engine. It follows a classic database architecture but is optimized for querying files directly rather than managing storage.</p>"},{"location":"architecture/design/#components","title":"Components","text":"<ol> <li>Parser: Converts SQL strings into an Abstract Syntax Tree (AST).</li> <li>Planner: Converts the AST into a Logical Plan.</li> <li>Optimizer: Applies optimizations like predicate pushdown to the Logical Plan.</li> <li>Executor: Converts the optimized plan into a Physical Plan and executes it.<ul> <li>Volcano Executor: Pure Python, streaming iterator-based.</li> <li>Pandas Executor: Vectorized, in-memory execution.</li> </ul> </li> <li>Readers: Abstractions for reading data from various sources (CSV, Parquet, S3).</li> </ol>"},{"location":"architecture/design/#data-flow","title":"Data Flow","text":"<pre><code>graph LR\n    SQL[SQL Query] --&gt; Parser\n    Parser --&gt; AST\n    AST --&gt; Planner\n    Planner --&gt; LogicalPlan\n    LogicalPlan --&gt; Optimizer\n    Optimizer --&gt; OptimizedPlan\n    OptimizedPlan --&gt; Executor\n    Executor --&gt; Results\n    Readers --&gt; Executor\n</code></pre>"},{"location":"architecture/optimizations/","title":"Query Optimizations","text":"<p>SQLStream implements a pipeline-based optimization framework to ensure efficient query execution.</p>"},{"location":"architecture/optimizations/#optimizer-architecture","title":"Optimizer Architecture","text":"<p>The optimizer module uses a modular pipeline design where each optimization rule is a separate class:</p> <pre><code>from sqlstream.optimizers import QueryPlanner\n\nplanner = QueryPlanner()\nplanner.optimize(ast, reader)\nprint(planner.get_optimization_summary())\n</code></pre> <p>Pipeline Order: 1. Join Reordering (optimize join execution plan) 2. Partition Pruning (skip entire partitions/files) 3. Predicate Pushdown (reduce data read) 4. Column Pruning (narrow columns) 5. Limit Pushdown (early termination) 6. Projection Pushdown (transform at source - future)</p>"},{"location":"architecture/optimizations/#1-predicate-pushdown","title":"1. Predicate Pushdown","text":"<p>Filters (WHERE clauses) are \"pushed down\" to the data source for early filtering.</p> <p>Benefits: - Reduces I/O by filtering at the source - Reduces memory usage - Especially effective for columnar formats (Parquet) - Can leverage indexes if available</p> <p>Example: <pre><code>SELECT * FROM data.csv WHERE age &gt; 30\n</code></pre></p> <p>Without pushdown: <pre><code>Read all rows \u2192 Filter in memory \u2192 Return results\n</code></pre></p> <p>With pushdown: <pre><code>Filter while reading \u2192 Return only matching rows\n</code></pre></p> <p>Implementation by Reader:</p> <ul> <li>CSV: Rows are filtered immediately after reading, before processing</li> <li>Parquet: Filters can skip entire row groups based on statistics (min/max values), significantly reducing I/O</li> <li>HTTP: Filters applied after download but before buffering</li> </ul> <p>Limitations: - Currently only supports simple comparisons (column op value) - Does not support complex expressions (e.g., <code>LENGTH(name) &gt; 5</code>) - Does not support cross-column comparisons (e.g., <code>age &gt; salary</code>) - Disabled for JOIN queries (needs smarter per-table analysis)</p>"},{"location":"architecture/optimizations/#2-column-pruning","title":"2. Column Pruning","text":"<p>Only columns required for the query are read from disk.</p> <p>Benefits: - Massive I/O reduction for wide tables - Reduces memory usage - Critical for columnar formats (Parquet, ORC) - Can read 10x faster if selecting 1 column from 10</p> <p>Example: <pre><code>SELECT name, age FROM employees  -- 100 columns total\n</code></pre></p> <p>Without pruning: <pre><code>Read all 100 columns \u2192 Project to 2 columns\n</code></pre></p> <p>With pruning: <pre><code>Read only 2 columns \u2192 Much faster\n</code></pre></p> <p>Implementation by Reader:</p> <ul> <li>Parquet: Only decodes requested columns from file</li> <li>CSV: Whole line is read, but only relevant fields are parsed and kept</li> <li>HTTP: Entire response read, but only needed columns extracted</li> </ul> <p>Column Analysis:</p> <p>The optimizer analyzes which columns are needed from: - SELECT clause - WHERE clause - GROUP BY clause - ORDER BY clause - Aggregate functions - JOIN conditions</p> <p>Limitations: - Cannot prune with <code>SELECT *</code> - CSV still reads full lines (just parses fewer fields)</p>"},{"location":"architecture/optimizations/#3-limit-pushdown","title":"3. Limit Pushdown","text":"<p>LIMIT clauses enable early termination of data reading.</p> <p>Benefits: - Stop reading after N rows - Massive speedup for large files - Reduces memory usage</p> <p>Example: <pre><code>SELECT * FROM large_file.csv LIMIT 10\n</code></pre></p> <p>Without pushdown: <pre><code>Read entire file \u2192 Take first 10 rows\n</code></pre></p> <p>With pushdown: <pre><code>Stop reading after 10 rows \u2192 Much faster\n</code></pre></p> <p>Implementation: - \u2705 Fully implemented in CSVReader and ParquetReader - Early termination at reader level (stops reading after N rows) - Works seamlessly with filters (limit applied after filtering)</p> <p>Limitations: - Cannot push down with ORDER BY (need all rows to sort) - Cannot push down with GROUP BY (need all rows to group) - Cannot push down with aggregates (need all rows) - Cannot push down with JOINs (complex - may need all rows)</p> <p>Status: \u2705 Fully implemented and tested</p>"},{"location":"architecture/optimizations/#4-partition-pruning","title":"4. Partition Pruning","text":"<p>Skip entire partitions/files based on filter conditions for Hive-style partitioned datasets.</p> <p>Benefits: - Massive I/O reduction (can skip 10x-1000x data) - Critical for data lakes and partitioned datasets - Zero-cost filtering at partition level - Works with S3 and cloud storage</p> <p>Example: <pre><code>-- Dataset: s3://data/year=2023/month=01/data.parquet\n--          s3://data/year=2024/month=01/data.parquet\n--          s3://data/year=2024/month=02/data.parquet\n\nSELECT * FROM data WHERE year = 2024 AND month = 1\n</code></pre></p> <p>Without partition pruning: <pre><code>Read all 3 files \u2192 Filter rows \u2192 Return results\n</code></pre></p> <p>With partition pruning: <pre><code>Skip year=2023 files \u2192 Read only year=2024/month=1 \u2192 Return results\n</code></pre></p> <p>Implementation: - \u2705 Fully implemented in ParquetReader - Detects Hive-style partitioning (key=value in path) - Partition columns added as virtual columns to results - Filters on partition columns removed from row-level filtering</p> <p>How it works: 1. Parse partition info from file path (e.g., <code>year=2024/month=01/</code>) 2. Extract partition column filters from WHERE clause 3. Evaluate filters against partition values 4. Skip reading file if partition doesn't match 5. Add partition columns to output rows</p> <p>Status: \u2705 Fully implemented and tested</p>"},{"location":"architecture/optimizations/#5-join-reordering","title":"5. Join Reordering","text":"<p>Optimize join execution order to minimize intermediate result sizes.</p> <p>Benefits: - Smaller intermediate results = less memory - Faster execution (less data to process) - Better cache utilization</p> <p>Strategy: - Join smaller tables first - Apply filters early to reduce row counts - Future: Use table statistics for cost-based decisions</p> <p>Example: <pre><code>-- Tables: A (1M rows), B (100 rows), C (1K rows)\n-- Bad order:  A JOIN B JOIN C = huge intermediate result\n-- Good order: B JOIN C JOIN A = smaller intermediate result\n</code></pre></p> <p>Status: \u26a0\ufe0f Framework implemented, placeholder (not yet active)</p> <p>Note: Join reordering is complex and can break query correctness if done incorrectly. Current implementation is a placeholder that provides the infrastructure but doesn't actually reorder joins yet.</p>"},{"location":"architecture/optimizations/#6-cost-based-optimization","title":"6. Cost-Based Optimization","text":"<p>Framework for statistics-driven optimization decisions.</p> <p>Components: - Table statistics (row counts, cardinality, min/max values) - Cost models for operations (scan, filter, join, sort) - Selectivity estimation - Plan cost comparison</p> <p>Benefits: - Smarter optimization decisions - Better join ordering - Adaptive query execution (future) - Index selection (future)</p> <p>Example: <pre><code>from sqlstream.optimizers import CostModel, TableStatistics\n\n# Estimate join cost\ncost = CostModel.estimate_join_cost(\n    left_rows=1000000,\n    right_rows=100,\n    selectivity=0.1\n)\n\n# Estimate filter selectivity\nselectivity = CostModel.estimate_selectivity(condition)\n</code></pre></p> <p>Status: \u26a0\ufe0f Framework implemented (not yet active in query execution)</p> <p>Note: Full cost-based optimization requires statistics collection (expensive) and plan enumeration. Current implementation provides the cost models and infrastructure for future use.</p>"},{"location":"architecture/optimizations/#7-parallel-execution","title":"7. Parallel Execution","text":"<p>Multi-threaded data reading for improved performance.</p> <p>Benefits: - Faster data ingestion - Better CPU utilization - Overlap I/O with computation</p> <p>Implementation: - Thread pool for parallel reading - Queue-based producer-consumer pattern - Works with any reader (CSV, Parquet, HTTP)</p> <p>Example: <pre><code>from sqlstream.readers import enable_parallel_reading, CSVReader\n\nreader = CSVReader(\"large_file.csv\")\nparallel_reader = enable_parallel_reading(reader, num_threads=4)\n\nfor row in parallel_reader:\n    process(row)\n</code></pre></p> <p>Status: \u26a0\ufe0f Infrastructure implemented (basic functionality)</p> <p>Note: Python's GIL limits true parallelism for CPU-bound tasks. Parallel execution is most effective for I/O-bound operations. Parquet already has native parallel reading via PyArrow.</p>"},{"location":"architecture/optimizations/#8-projection-pushdown","title":"8. Projection Pushdown","text":"<p>Push computed expressions to the data source for evaluation.</p> <p>Benefits (when implemented): - Evaluate expressions at read time - Reduce data movement - Leverage native database/engine functions</p> <p>Example (future): <pre><code>SELECT UPPER(name), age * 2 FROM data\n</code></pre></p> <p>With pushdown: <pre><code>Reader evaluates UPPER() and age*2 \u2192 Return transformed data\n</code></pre></p> <p>Status: \u26a0\ufe0f Not yet implemented - placeholder for future work</p>"},{"location":"architecture/optimizations/#vectorized-execution-pandas-backend","title":"Vectorized Execution (Pandas Backend)","text":"<p>When using the Pandas backend (<code>backend=\"pandas\"</code>), SQLStream leverages:</p> <ul> <li>SIMD Instructions: CPU-level parallelism for array operations</li> <li>Efficient Memory Layout: Columnar storage in memory</li> <li>Optimized Algorithms: C-optimized implementations of joins and aggregations</li> <li>NumPy: Highly optimized numerical operations</li> </ul> <p>When to use: <pre><code># For large datasets or complex aggregations\nquery(\"data.csv\").sql(\"SELECT * FROM data\", backend=\"pandas\")\n</code></pre></p>"},{"location":"architecture/optimizations/#lazy-evaluation","title":"Lazy Evaluation","text":"<p>Query results are iterators - execution only happens when you consume results.</p> <p>Benefits: - Early Termination: <code>LIMIT</code> clauses stop execution as soon as enough rows are found - Streaming: Start processing first results while rest of query runs - Memory Efficient: Don't load entire result set into memory</p> <p>Example: <pre><code>results = query(\"data.csv\").sql(\"SELECT * FROM data LIMIT 10\")\n# No execution yet\n\nfor row in results:  # Execution starts here\n    print(row)\n</code></pre></p>"},{"location":"architecture/optimizations/#optimization-summary","title":"Optimization Summary","text":"<p>You can see which optimizations were applied:</p> <pre><code>plan = query(\"data.csv\").sql(\"\"\"\n    SELECT name, age\n    FROM data\n    WHERE age &gt; 30\n\"\"\", backend=\"python\").explain()\n\nprint(plan)\n</code></pre> <p>Output: <pre><code>Query Plan:\n  Scan: data.csv\n  Filter: age &gt; 30\n  Project: name, age\n\nOptimizations applied:\n  - Predicate pushdown: 1 condition(s)\n  - Column pruning: 3 column(s) selected\n</code></pre></p>"},{"location":"architecture/optimizations/#custom-optimizers","title":"Custom Optimizers","text":"<p>You can add custom optimization rules:</p> <pre><code>from sqlstream.optimizers import QueryPlanner, Optimizer\n\nclass MyCustomOptimizer(Optimizer):\n    def get_name(self) -&gt; str:\n        return \"My custom rule\"\n\n    def can_optimize(self, ast, reader) -&gt; bool:\n        # Check if optimization applies\n        return True\n\n    def optimize(self, ast, reader) -&gt; None:\n        # Apply optimization\n        self.applied = True\n        self.description = \"did something cool\"\n\nplanner = QueryPlanner()\nplanner.add_optimizer(MyCustomOptimizer())\n</code></pre>"},{"location":"architecture/optimizations/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use Parquet for wide tables: Column pruning is most effective</li> <li>Use WHERE early: Predicate pushdown reduces data read</li> <li>Select specific columns: Avoid <code>SELECT *</code> when possible</li> <li>Use LIMIT for exploration: Quick previews of large files</li> <li>Use Pandas backend for aggregations: Faster for GROUP BY queries</li> <li>Check explain plans: Use <code>.explain()</code> to see which optimizations applied</li> </ol>"},{"location":"architecture/optimizations/#implementation-status","title":"Implementation Status","text":""},{"location":"architecture/optimizations/#fully-implemented","title":"Fully Implemented \u2705","text":"<ul> <li>Predicate pushdown - Push WHERE filters to readers</li> <li>Column pruning - Read only required columns</li> <li>Limit pushdown - Early termination for LIMIT queries</li> <li>Partition pruning - Skip partitions based on filters (Parquet)</li> </ul>"},{"location":"architecture/optimizations/#framework-available","title":"Framework Available \u26a0\ufe0f","text":"<ul> <li>Join reordering - Infrastructure exists, not yet active</li> <li>Cost-based optimization - Cost models and statistics framework available</li> <li>Parallel execution - Basic thread pool implementation available</li> </ul>"},{"location":"architecture/optimizations/#future-enhancements","title":"Future Enhancements \ud83d\udd2e","text":"<ul> <li>\u23f3 Projection pushdown (push computed expressions to source)</li> <li>\u23f3 Aggregate pushdown (push GROUP BY to readers)</li> <li>\u23f3 Index usage (when indexes available)</li> <li>\u23f3 Adaptive query execution (runtime optimization)</li> <li>\u23f3 Query result caching</li> <li>\u23f3 Materialized views</li> <li>\u23f3 Advanced statistics collection (histograms, sketches)</li> </ul>"},{"location":"architecture/volcano-model/","title":"Volcano Model","text":"<p>The default execution model in SQLStream is based on the Volcano Iterator Model (also known as the Open-Next-Close model).</p>"},{"location":"architecture/volcano-model/#how-it-works","title":"How it Works","text":"<p>Each operator in the query plan (Scan, Filter, Project, Join) implements a standard interface with three methods:</p> <ol> <li><code>open()</code>: Initialize the operator.</li> <li><code>next()</code>: Retrieve the next tuple (row).</li> <li><code>close()</code>: Clean up resources.</li> </ol>"},{"location":"architecture/volcano-model/#execution-flow","title":"Execution Flow","text":"<p>When a query is executed, the top-level operator calls <code>next()</code> on its child, which calls <code>next()</code> on its child, and so on, down to the Scan operator which reads from the file.</p> <pre><code># Simplified representation\nclass FilterOperator:\n    def next(self):\n        while True:\n            row = self.child.next()\n            if row is None:\n                return None\n            if self.predicate(row):\n                return row\n</code></pre>"},{"location":"architecture/volcano-model/#benefits","title":"Benefits","text":"<ul> <li>Low Memory Footprint: Data is processed one row at a time. The entire dataset does not need to be loaded into memory.</li> <li>Pipelining: No intermediate results need to be materialized (except for blocking operators like Sort or Aggregate).</li> <li>Simplicity: Easy to implement and extend.</li> </ul>"},{"location":"architecture/volcano-model/#trade-offs","title":"Trade-offs","text":"<ul> <li>CPU Overhead: Function call overhead for every row can be significant in Python.</li> <li>Performance: Slower than vectorized execution for large datasets. This is why SQLStream also offers a Pandas backend.</li> </ul>"},{"location":"cli/interactive-mode/","title":"Interactive SQL Shell","text":"<p>SQLStream includes a powerful interactive shell built with Textual, providing a modern terminal UI for exploring and querying data.</p>"},{"location":"cli/interactive-mode/#features","title":"Features","text":"<ul> <li>\ud83c\udfa8 Syntax Highlighting - Dracula theme for SQL queries</li> <li>\ud83d\udcd1 Multiple Query Tabs - Work on multiple queries simultaneously (<code>Ctrl+T</code> to add, <code>Ctrl+W</code> to close)</li> <li>\ud83d\udcbe State Persistence - Automatically saves and restores tabs and queries between sessions</li> <li>\ud83d\udcca Scrollable Results - Zebra-striped table with smooth scrolling</li> <li>\ud83d\udcdc Query History - Navigate previous queries with keyboard shortcuts</li> <li>Word Deletion - Fast editing with <code>Ctrl+Delete</code> and <code>Ctrl+Backspace</code></li> <li>\ud83d\uddc2\ufe0f Tabbed Sidebar - Toggle between Schema browser and File explorer</li> <li>\ud83d\udcc1 File Browser - Tree-structured file navigation in sidebar</li> <li>\ud83d\udcc4 Pagination - Handle large result sets (100 rows per page)</li> <li>\ud83d\udd00 Column Sorting - Click headers to sort ascending/descending</li> <li>\ud83d\udcbe Multi-Format Export - Save results as CSV, JSON, and Parquet</li> <li>\ud83d\udd0d Filtering - Search across all columns</li> <li>\u2699\ufe0f Backend Toggle - Cycle through execution backends (<code>F5</code> or <code>Ctrl+B</code>: auto/duckdb/pandas/python)</li> <li>\u2601\ufe0f S3 Support - Query files directly from S3 buckets</li> <li>\u26a1 Fast - Execution time display and row counts</li> </ul>"},{"location":"cli/interactive-mode/#installation","title":"Installation","text":"<p>The interactive shell requires the <code>textual</code> library:</p> <pre><code># Install with CLI support\npip install \"sqlstream[cli]\"\n\n# Install full TUI\npip install \"sqlstream[interactive]\"\n\n# Or install all features\npip install \"sqlstream[all]\"\n</code></pre>"},{"location":"cli/interactive-mode/#getting-started","title":"Getting Started","text":""},{"location":"cli/interactive-mode/#launch-the-shell","title":"Launch the Shell","text":"<pre><code># Empty shell\nsqlstream shell\n\n# With initial file\nsqlstream shell employees.csv\n\n# Custom history location\nsqlstream shell --history-file ~/.my_sqlstream_history\n</code></pre>"},{"location":"cli/interactive-mode/#basic-usage","title":"Basic Usage","text":"<ol> <li>Write a query in the editor (supports multi-line)</li> <li>Execute with <code>Ctrl+Enter</code> or <code>Ctrl+E</code></li> <li>View results in the table below</li> <li>Navigate large result sets with pagination</li> <li>Export results with <code>Ctrl+X</code></li> </ol>"},{"location":"cli/interactive-mode/#keybindings","title":"Keybindings","text":"Key Action Description <code>Ctrl+Enter</code> Execute Query Run the query in editor <code>Ctrl+E</code> Execute Query Alternative execution key <code>Ctrl+L</code> Clear Editor Clear query text <code>Ctrl+Backspace</code> Delete Word Left Delete word to the left of cursor <code>Ctrl+Delete</code> Delete Word Right Delete word to the right of cursor <code>Ctrl+Q</code> Exit Close the shell (auto-saves state) <code>Ctrl+D</code> Exit Alternative exit key (auto-saves state) <code>Ctrl+T</code> New Tab Create a new query tab <code>Ctrl+W</code> Close Tab Close current query tab <code>Ctrl+S</code> Save State Manually save current state <code>F1</code> Help Show help message <code>F2</code> Toggle Sidebar Show/hide tabbed sidebar (Schema/Files) <code>F4</code> Explain Mode Show query plan <code>F5</code> Backend Toggle Cycle through backends (auto/duckdb/pandas/python) <code>Ctrl+B</code> Backend Toggle Alternative backend cycle key <code>Ctrl+O</code> Open Files Tab Switch to file browser in sidebar <code>Ctrl+X</code> Export Export with custom filename <code>Ctrl+F</code> Filter Filter current results <code>[</code> Previous Page Navigate to previous page <code>]</code> Next Page Navigate to next page <code>Ctrl+Up</code> Prev Query Load previous from history <code>Ctrl+Down</code> Next Query Load next from history Click Header Sort Column Sort by column (click again to reverse)"},{"location":"cli/interactive-mode/#query-examples","title":"Query Examples","text":""},{"location":"cli/interactive-mode/#local-files","title":"Local Files","text":"<pre><code>-- Simple query\nSELECT * FROM 'employees.csv' WHERE age &gt; 30\n\n-- Aggregations\nSELECT department, COUNT(*) as count, AVG(salary) as avg_salary\nFROM 'employees.csv'\nGROUP BY department\nORDER BY avg_salary DESC\n\n-- JOINs\nSELECT e.name, e.salary, d.department_name\nFROM 'employees.csv' e\nJOIN 'departments.csv' d ON e.dept_id = d.id\n</code></pre>"},{"location":"cli/interactive-mode/#s3-files","title":"S3 Files","text":"<pre><code>-- Query S3 CSV\nSELECT * FROM 's3://my-bucket/data.csv' WHERE date &gt; '2024-01-01'\n\n-- Query S3 Parquet with aggregation\nSELECT product_id, SUM(revenue) as total\nFROM 's3://my-bucket/sales.parquet'\nWHERE date &gt; '2024-01-01'\nGROUP BY product_id\nORDER BY total DESC\nLIMIT 10\n</code></pre>"},{"location":"cli/interactive-mode/#http-files","title":"HTTP Files","text":"<pre><code>SELECT * FROM 'https://example.com/data.csv'\nWHERE category = 'electronics'\n</code></pre>"},{"location":"cli/interactive-mode/#advanced-features","title":"Advanced Features","text":""},{"location":"cli/interactive-mode/#1-query-history","title":"1. Query History","text":"<p>The shell maintains a persistent history of your queries (up to 100 queries).</p> <p>Location: <code>~/.sqlstream_history</code> (or custom with <code>--history-file</code>)</p> <p>Navigation: - <code>Ctrl+Up</code> - Load previous query - <code>Ctrl+Down</code> - Load next query</p> <p>Behavior: - History loads automatically on startup - Each executed query is saved - Navigate through history without re-executing</p>"},{"location":"cli/interactive-mode/#2-multiple-query-tabs","title":"2. Multiple Query Tabs","text":"<p>Work on multiple queries simultaneously without losing your work.</p> <p>Creating Tabs: - <code>Ctrl+T</code> - Create a new tab (automatically named \"Query 1\", \"Query 2\", etc.) - Each tab has its own independent query editor</p> <p>Switching Tabs: - Click on tab labels to switch between them - Each tab maintains its own query text</p> <p>Closing Tabs: - <code>Ctrl+W</code> - Close the current tab - If you close the last tab, a new empty one is created automatically</p> <p>Features: - Tabs are saved automatically when you exit - Restored when you restart the shell - Work on complex queries in one tab while exploring data in another</p> <p>Example Workflow: <pre><code>Tab 1: \"Query 1\" - Exploratory SELECT * FROM 'data.csv' LIMIT 100\nTab 2: \"Query 2\" - Complex aggregation with GROUP BY\nTab 3: \"Query 3\" - JOIN query combining multiple files\n</code></pre></p>"},{"location":"cli/interactive-mode/#3-state-persistence","title":"3. State Persistence","text":"<p>Your work is automatically saved and restored between sessions.</p> <p>What's Saved: - All open tabs (titles and content) - Query text in each tab - Tab order</p> <p>Storage Location: <code>~/.sqlstream_state</code></p> <p>Behavior: - State saves automatically when you exit (<code>Ctrl+Q</code> or <code>Ctrl+D</code>) - Manual save available with <code>Ctrl+S</code> - State loads automatically on startup - If no saved state, starts with one empty tab</p> <p>Benefits: - Resume work exactly where you left off - Never lose in-progress queries - Maintain context across sessions</p>"},{"location":"cli/interactive-mode/#4-tabbed-sidebar","title":"4. Tabbed Sidebar","text":"<p>The sidebar now has two tabs: Schema and Files.</p> <p>Schema Tab: - Shows all loaded data sources - Displays column names and types - Updates when new files are queried</p> <p>Files Tab (<code>Ctrl+O</code> to activate): - Tree-structured file browser - Navigate your filesystem - Click files to insert <code>SELECT * FROM 'file_path'</code> into active tab</p> <p>Toggle: Press <code>F2</code> to show/hide the entire sidebar</p>"},{"location":"cli/interactive-mode/#5-file-browser","title":"5. File Browser","text":"<p>Browse and select files directly from the UI.</p> <p>Access: <code>Ctrl+O</code> or click \"Files\" tab in sidebar</p> <p>Features: - Tree view starting from current directory (<code>./</code>) - Expand/collapse directories - Click any file to load it into the active query tab</p> <p>Auto-Insert: Selecting a file inserts: <pre><code>SELECT * FROM 'path/to/file.csv'\n</code></pre></p> <p>Behavior: - Works with the currently active query tab - Inserts at cursor position if editor has existing content - Automatically shows sidebar if hidden</p>"},{"location":"cli/interactive-mode/#6-schema-browser","title":"6. Schema Browser","text":"<p>Press <code>F2</code> to toggle the sidebar, then switch to the Schema tab.</p> <p>Shows: - All loaded files - Column names (green) - Data types (dim text) - Errors (red)</p> <p>Examples: <pre><code>Data Sources\n\u251c\u2500 employees.csv\n\u2502  \u251c\u2500 name: string\n\u2502  \u251c\u2500 age: int\n\u2502  \u251c\u2500 city: string\n\u2502  \u2514\u2500 salary: float\n\u2514\u2500 sales.parquet\n   \u251c\u2500 product_id: int\n   \u251c\u2500 revenue: float\n   \u2514\u2500 date: date\n</code></pre></p> <p>Features: - Asynchronous schema loading (non-blocking) - Updates automatically when querying new files - Helps discover available columns before writing queries</p>"},{"location":"cli/interactive-mode/#7-pagination","title":"7. Pagination","text":"<p>When a query returns more than 100 rows, results are automatically paginated.</p> <p>Status Display: <pre><code>Showing 101-200 of 450 rows | Page 2/5\n</code></pre></p> <p>Navigation: - <code>Ctrl+N</code> or <code>]</code> - Next page - <code>Ctrl+P</code> or <code>[</code> - Previous page - Sorting and filtering reset to page 1</p> <p>Performance: - Only 100 rows rendered at a time - Instant navigation between pages - Handles millions of rows efficiently</p>"},{"location":"cli/interactive-mode/#8-column-sorting","title":"8. Column Sorting","text":"<p>Click any column header to sort results.</p> <p>Behavior: 1. First click: Sort ascending (\u2191) 2. Second click: Sort descending (\u2193) 3. Click another column: Sort by that column</p> <p>Status Display: <pre><code>Sorted by salary \u2193\n</code></pre></p> <p>Notes: - Sorting works across all pages - Resets current page to 1 - Works with filtered results</p>"},{"location":"cli/interactive-mode/#9-multi-format-export","title":"9. Multi-Format Export","text":"<p>Press <code>Ctrl+X</code> to export current results to multiple formats simultaneously.</p> <p>Exported Formats: - CSV: <code>results_YYYYMMDD_HHMMSS.csv</code> - JSON: <code>results_YYYYMMDD_HHMMSS.json</code> (pretty-printed) - Parquet: <code>results_YYYYMMDD_HHMMSS.parquet</code> (if <code>pyarrow</code> installed)</p> <p>Example: <pre><code>Exported to: CSV (results_20241130_143022.csv),\n             JSON (results_20241130_143022.json),\n             Parquet (results_20241130_143022.parquet)\n</code></pre></p> <p>Notes: - Exports current page or filtered results - Timestamped filenames prevent overwrites - Parquet export requires <code>pip install pyarrow</code></p>"},{"location":"cli/interactive-mode/#10-filtering","title":"10. Filtering","text":"<p>Press <code>Ctrl+F</code> to filter current results.</p> <p>Features: - Case-insensitive search - Searches across all columns - Updates row count in status bar</p> <p>Example: <pre><code>Filtered to 45 rows (from 450 total)\n</code></pre></p>"},{"location":"cli/interactive-mode/#11-backend-toggle","title":"11. Backend Toggle","text":"<p>Press <code>F5</code> or <code>Ctrl+B</code> to cycle through available execution backends on-the-fly.</p> <p>Available Backends: - auto - Automatically selects best backend (pandas &gt; duckdb &gt; python) - duckdb - Full SQL support with window functions, CTEs, subqueries - pandas - Fast execution for basic queries (10-100x faster than Python) - python - Educational Volcano model implementation</p> <p>Status Display: <pre><code>\u2699\ufe0f DUCKDB\n</code></pre></p> <p>Behavior: - Current backend shown in status bar - Press <code>F5</code> or <code>Ctrl+B</code> to cycle to next backend - Backend preference saved in state (persists between sessions) - Allows testing queries with different backends without restarting</p> <p>Example Workflow: <pre><code>1. Start with 'auto' backend\n2. Press F5 \u2192 switches to 'duckdb'\n3. Run complex query with window functions\n4. Press F5 \u2192 switches to 'pandas'\n5. Run simple aggregation\n6. Press F5 \u2192 switches to 'python'\n7. Inspect Volcano model behavior\n</code></pre></p> <p>Notes: - Some SQL features only work with specific backends - DuckDB backend required for window functions, CTEs, subqueries - If a backend is not installed, query will fail with helpful error message</p>"},{"location":"cli/interactive-mode/#performance","title":"Performance","text":"Feature Performance Pagination Shows first 100 rows instantly Sorting In-memory sort of all results Filtering Scans all rows once, then cached S3 Loading Streams data, doesn't load all into memory Schema Loading Async worker, doesn't block UI"},{"location":"cli/interactive-mode/#examples","title":"Examples","text":""},{"location":"cli/interactive-mode/#example-1-explore-large-dataset","title":"Example 1: Explore Large Dataset","text":"<pre><code>-- Load first 1000 rows\nSELECT * FROM 'big_file.csv' LIMIT 1000\n\n-- Results: 1000 rows \u2192 10 pages\n-- Use ] to navigate pages\n-- Click 'revenue' header to sort\n-- Use Ctrl+X to export\n</code></pre>"},{"location":"cli/interactive-mode/#example-2-s3-analytics","title":"Example 2: S3 Analytics","text":"<pre><code>-- Query S3 Parquet\nSELECT\n    category,\n    COUNT(*) as count,\n    AVG(price) as avg_price\nFROM 's3://my-bucket/products.parquet'\nGROUP BY category\nORDER BY count DESC\n\n-- Click 'count' to sort\n-- Export to CSV for sharing\n</code></pre>"},{"location":"cli/interactive-mode/#example-3-schema-exploration","title":"Example 3: Schema Exploration","text":"<pre><code>-- Press F2 to see schema\n-- Write query with column names visible\nSELECT name, age, city\nFROM 'employees.csv'\nWHERE age &gt; 25\nORDER BY name\n\n-- Sort by different columns using headers\n-- Export to JSON for API use\n</code></pre>"},{"location":"cli/interactive-mode/#tips-tricks","title":"Tips &amp; Tricks","text":"<ol> <li>Multiple Tabs: Use tabs to work on different queries simultaneously - one for exploration, one for final analysis</li> <li>State Persistence: Your tabs are automatically saved - feel confident closing the shell anytime</li> <li>Large Datasets: Use <code>LIMIT</code> to preview data quickly in a dedicated tab</li> <li>S3 Performance: Use partitioned Parquet files for best performance</li> <li>History: Use <code>Ctrl+Up</code> to quickly re-run previous queries in any tab</li> <li>File Browser: Use <code>Ctrl+O</code> to quickly add files to your query without typing paths</li> <li>Sorting: Click column headers to explore data patterns</li> <li>Export: Export to Parquet for best compression</li> <li>Sidebar: Toggle with <code>F2</code> to maximize editor space when needed</li> <li>Manual Save: Use <code>Ctrl+S</code> if you want to save state before experimenting</li> <li>Backend Toggle: Use <code>F5</code> to test complex SQL features - start with <code>auto</code>, switch to <code>duckdb</code> for window functions/CTEs</li> </ol>"},{"location":"cli/interactive-mode/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cli/interactive-mode/#issue-footer-visibility","title":"Issue: Footer Visibility","text":"<p>Problem: Footer may be clipped on small terminal windows</p> <p>Solution: Increase terminal height or scroll down</p>"},{"location":"cli/interactive-mode/#issue-keybinding-conflicts","title":"Issue: Keybinding Conflicts","text":"<p>Problem: Some keybindings don't work in VSCode terminal</p> <p>Solution: Use a native terminal (gnome-terminal, iTerm2, Windows Terminal)</p>"},{"location":"cli/interactive-mode/#issue-textual-not-installed","title":"Issue: Textual Not Installed","text":"<p>Problem: <code>ImportError: No module named 'textual'</code></p> <p>Solution: <code>pip install \"sqlstream[cli]\"</code></p>"},{"location":"cli/interactive-mode/#next-steps","title":"Next Steps","text":"<ul> <li>Query Command - Learn about non-interactive queries</li> <li>Output Formats - Formatting options</li> <li>S3 Support - Query cloud data</li> <li>SQL Support - Supported SQL syntax</li> </ul>"},{"location":"cli/output-formats/","title":"Output Formats","text":"<p>The SQLStream CLI supports multiple output formats to suit different needs, from human-readable tables to machine-parsable JSON.</p>"},{"location":"cli/output-formats/#specifying-output-format","title":"Specifying Output Format","text":"<p>Use the <code>--format</code> option with the <code>query</code> command.</p> <pre><code>sqlstream query \"SELECT * FROM 'data.csv'\" --format &lt;format&gt;\n</code></pre>"},{"location":"cli/output-formats/#supported-formats","title":"Supported Formats","text":""},{"location":"cli/output-formats/#table-default","title":"Table (Default)","text":"<p>Displays results in a formatted ASCII table using the <code>rich</code> library. Best for human inspection.</p> <pre><code>sqlstream query \"SELECT name, age FROM 'users.csv'\"\n</code></pre> <p>Output: <pre><code>\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 name \u2503 age \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Alice\u2502 30  \u2502\n\u2502 Bob  \u2502 25  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"cli/output-formats/#csv","title":"CSV","text":"<p>Outputs standard Comma-Separated Values. Useful for piping to other tools or saving to files.</p> <pre><code>sqlstream query \"SELECT * FROM 'data.csv'\" --format csv\n</code></pre> <p>Output: <pre><code>name,age\nAlice,30\nBob,25\n</code></pre></p>"},{"location":"cli/output-formats/#json","title":"JSON","text":"<p>Outputs a JSON array of objects. Ideal for web applications or processing with <code>jq</code>.</p> <pre><code>sqlstream query \"SELECT * FROM 'data.csv'\" --format json\n</code></pre> <p>Output: <pre><code>[\n  {\"name\": \"Alice\", \"age\": 30},\n  {\"name\": \"Bob\", \"age\": 25}\n]\n</code></pre></p>"},{"location":"cli/output-formats/#markdown","title":"Markdown","text":"<p>Outputs a Markdown-formatted table. Great for generating documentation.</p> <pre><code>sqlstream query \"SELECT * FROM 'data.csv'\" --format markdown\n</code></pre> <p>Output: <pre><code>| name | age |\n|------|-----|\n| Alice| 30  |\n| Bob  | 25  |\n</code></pre></p>"},{"location":"cli/output-formats/#interactive-shell-export","title":"Interactive Shell Export","text":"<p>In the interactive shell (<code>sqlstream shell</code>), you can export results using <code>Ctrl+X</code>. This automatically exports the current result set to CSV, JSON, and Parquet (if available) simultaneously, saving them with a timestamped filename.</p>"},{"location":"cli/overview/","title":"CLI Overview","text":"<p>The <code>sqlstream</code> command-line interface provides an easy way to query data files with SQL.</p>"},{"location":"cli/overview/#available-commands","title":"Available Commands","text":""},{"location":"cli/overview/#query-execute-sql-queries","title":"<code>query</code> - Execute SQL Queries","text":"<p>Execute SQL queries on data files and display results.</p> <pre><code>sqlstream query [FILE] &lt;SQL&gt; [OPTIONS]\n</code></pre> <p>Examples: <pre><code># Query a file\nsqlstream query data.csv \"SELECT * FROM data WHERE age &gt; 25\"\n\n# Inline file path\nsqlstream query \"SELECT * FROM 'data.csv' WHERE age &gt; 25\"\n\n# JSON output\nsqlstream query data.csv \"SELECT * FROM data\" --format json\n\n# S3 files\nsqlstream query \"SELECT * FROM 's3://bucket/data.parquet' LIMIT 100\"\n</code></pre></p> <p>See Query Command for full documentation.</p>"},{"location":"cli/overview/#shell-interactive-sql-shell","title":"<code>shell</code> - Interactive SQL Shell","text":"<p>Launch the interactive shell with full TUI (Terminal User Interface).</p> <pre><code>sqlstream shell [FILE]\n</code></pre> <p>Features: - Modal dialogs for filtering, export, file selection - File browser (<code>Ctrl+O</code>) to select files - Query execution plan visualization (<code>F4</code>) - Multi-format export (<code>Ctrl+X</code>) - Live filtering (<code>Ctrl+F</code>) - Schema browser (<code>F2</code>) - Query history with multiline support</p> <p>See Interactive Mode for full documentation.</p>"},{"location":"cli/overview/#global-options","title":"Global Options","text":"<ul> <li><code>--version</code> - Show SQLStream version</li> <li><code>--help</code> - Show help message</li> </ul>"},{"location":"cli/overview/#quick-start","title":"Quick Start","text":"<pre><code># Simple query\nsqlstream query employees.csv \"SELECT * FROM employees WHERE salary &gt; 80000\"\n\n# With pandas backend for performance\nsqlstream query large.csv \"SELECT * FROM large\" --backend pandas\n\n# Launch interactive shell\nsqlstream shell employees.csv\n</code></pre>"},{"location":"cli/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Query Command Reference - Detailed query command options</li> <li>Interactive Shell Guide - Full interactive shell documentation</li> <li>Output Formats - Available output formats</li> </ul>"},{"location":"cli/query-command/","title":"Query Command","text":"<p>Execute SQL queries on CSV and Parquet files.</p>"},{"location":"cli/query-command/#syntax","title":"Syntax","text":"<pre><code>sqlstream query [FILE_OR_SQL] [SQL] [OPTIONS]\n</code></pre>"},{"location":"cli/query-command/#arguments","title":"Arguments","text":"<ul> <li><code>FILE_OR_SQL</code> - File path or SQL query (optional if SQL contains inline paths)</li> <li><code>SQL</code> - SQL query string (optional if using inline paths)</li> </ul>"},{"location":"cli/query-command/#options","title":"Options","text":""},{"location":"cli/query-command/#output-format","title":"Output Format","text":"<ul> <li><code>-f, --format [table|json|csv]</code> - Output format (default: table)</li> <li><code>-o, --output FILE</code> - Write output to file</li> </ul>"},{"location":"cli/query-command/#performance","title":"Performance","text":"<ul> <li><code>-b, --backend [auto|pandas|python|duckdb]</code> - Execution backend (default: auto)</li> <li><code>-l, --limit N</code> - Limit displayed rows</li> </ul>"},{"location":"cli/query-command/#display","title":"Display","text":"<ul> <li><code>--no-color</code> - Disable colored output</li> <li><code>-i, --interactive</code> - Force interactive mode</li> <li><code>--no-interactive</code> - Disable interactive mode</li> <li><code>-t, --time</code> - Show execution time</li> </ul>"},{"location":"cli/query-command/#debugging","title":"Debugging","text":"<ul> <li><code>--explain</code> - Show query execution plan</li> </ul>"},{"location":"cli/query-command/#examples","title":"Examples","text":"<p>See Query Examples</p>"},{"location":"examples/aggregations/","title":"Aggregation Examples","text":""},{"location":"examples/aggregations/#basic-counting","title":"Basic Counting","text":"<p>Count total rows in a file.</p> <pre><code>SELECT COUNT(*) FROM 'logs.csv'\n</code></pre>"},{"location":"examples/aggregations/#grouping-by-category","title":"Grouping by Category","text":"<p>Calculate average salary by department.</p> <pre><code>SELECT \n    department, \n    AVG(salary) as avg_salary \nFROM 'employees.csv' \nGROUP BY department\n</code></pre>"},{"location":"examples/aggregations/#multiple-aggregations","title":"Multiple Aggregations","text":"<p>Compute multiple statistics at once.</p> <pre><code>SELECT \n    category, \n    COUNT(*) as item_count, \n    MIN(price) as min_price, \n    MAX(price) as max_price, \n    AVG(price) as avg_price \nFROM 'products.csv' \nGROUP BY category\n</code></pre>"},{"location":"examples/basic-queries/","title":"Basic Query Examples","text":"<p>Common query patterns and examples.</p>"},{"location":"examples/basic-queries/#filtering","title":"Filtering","text":"<pre><code>SELECT * FROM data WHERE age &gt; 25\nSELECT * FROM data WHERE city = 'NYC'\nSELECT * FROM data WHERE salary &gt;= 80000\n</code></pre>"},{"location":"examples/basic-queries/#sorting","title":"Sorting","text":"<pre><code>SELECT * FROM data ORDER BY age DESC\nSELECT * FROM data ORDER BY salary DESC LIMIT 10\n</code></pre>"},{"location":"examples/basic-queries/#aggregations","title":"Aggregations","text":"<pre><code>SELECT COUNT(*) FROM data\nSELECT AVG(salary) FROM data\nSELECT department, COUNT(*) FROM data GROUP BY department\n</code></pre> <p>See more: Join Examples | Aggregations</p>"},{"location":"examples/duckdb-backend/","title":"DuckDB Backend Examples","text":"<p>Complete examples demonstrating the DuckDB backend's advanced SQL capabilities.</p>"},{"location":"examples/duckdb-backend/#example-1-window-functions","title":"Example 1: Window Functions","text":"<p>Calculate running totals and rankings for employee salaries.</p> <pre><code>from sqlstream import query\n\n# Create sample data\nsample_data = \"\"\"\nname,department,salary,hire_date\nAlice,Engineering,95000,2020-01-15\nBob,Engineering,85000,2021-03-10\nCharlie,Engineering,105000,2019-06-20\nDiana,Sales,75000,2020-11-05\nEve,Sales,82000,2021-01-12\nFrank,Sales,68000,2022-02-18\nGrace,Marketing,72000,2020-08-25\nHenry,Marketing,78000,2021-07-14\n\"\"\"\n\nwith open(\"employees_sample.csv\", \"w\") as f:\n    f.write(sample_data)\n\n# Query with window functions\nresults = query().sql(\"\"\"\n    SELECT \n        name,\n        department,\n        salary,\n        ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as dept_rank,\n        RANK() OVER (ORDER BY salary DESC) as overall_rank,\n        AVG(salary) OVER (PARTITION BY department) as dept_avg_salary,\n        SUM(salary) OVER (ORDER BY hire_date) as running_total\n    FROM 'employees_sample.csv'\n    ORDER BY department, dept_rank\n\"\"\", backend=\"duckdb\")\n\nprint(\"Employee Rankings by Department:\")\nprint(\"-\" * 80)\nfor row in results:\n    print(f\"{row['name']:10} | {row['department']:12} | \"\n          f\"${row['salary']:,} | Dept Rank: {row['dept_rank']} | \"\n          f\"Dept Avg: ${row['dept_avg_salary']:,.0f}\")\n</code></pre> <p>Output: <pre><code>Employee Rankings by Department:\n--------------------------------------------------------------------------------\nCharlie    | Engineering  | $105,000 | Dept Rank: 1 | Dept Avg: $95,000\nAlice      | Engineering  | $95,000  | Dept Rank: 2 | Dept Avg: $95,000\nBob        | Engineering  | $85,000  | Dept Rank: 3 | Dept Avg: $95,000\nEve        | Sales        | $82,000  | Dept Rank: 1 | Dept Avg: $75,000\nDiana      | Sales        | $75,000  | Dept Rank: 2 | Dept Avg: $75,000\nFrank      | Sales        | $68,000  | Dept Rank: 3 | Dept Avg: $75,000\nHenry      | Marketing    | $78,000  | Dept Rank: 1 | Dept Avg: $75,000\nGrace      | Marketing    | $72,000  | Dept Rank: 2 | Dept Avg: $75,000\n</code></pre></p>"},{"location":"examples/duckdb-backend/#example-2-common-table-expressions-ctes","title":"Example 2: Common Table Expressions (CTEs)","text":"<p>Multi-step analytics using CTEs to find top performers.</p> <pre><code>from sqlstream import query\n\nresults = query().sql(\"\"\"\n    WITH dept_stats AS (\n        -- Calculate department statistics\n        SELECT \n            department,\n            AVG(salary) as avg_salary,\n            MAX(salary) as max_salary,\n            MIN(salary) as min_salary,\n            COUNT(*) as employee_count\n        FROM 'employees_sample.csv'\n        GROUP BY department\n    ),\n    top_earners AS (\n        -- Find employees earning above department average\n        SELECT \n            e.name,\n            e.department,\n            e.salary,\n            d.avg_salary\n        FROM 'employees_sample.csv' e\n        JOIN dept_stats d ON e.department = d.department\n        WHERE e.salary &gt; d.avg_salary\n    )\n    SELECT \n        name,\n        department,\n        salary,\n        avg_salary,\n        salary - avg_salary as above_average\n    FROM top_earners\n    ORDER BY above_average DESC\n\"\"\", backend=\"duckdb\")\n\nprint(\"Employees Earning Above Department Average:\")\nprint(\"-\" * 70)\nfor row in results:\n    print(f\"{row['name']:10} | {row['department']:12} | \"\n          f\"${row['salary']:,} | +${row['above_average']:,.0f}\")\n</code></pre>"},{"location":"examples/duckdb-backend/#example-3-complex-analytics-with-s3-data","title":"Example 3: Complex Analytics with S3 Data","text":"<p>Analyze sales data from S3 with window functions and CTEs.</p> <pre><code>from sqlstream import query\n\n# Simulate S3 sales data (in real use, this would be on S3)\nsales_data = \"\"\"\nsale_date,product_id,category,amount,customer_id\n2024-01-05,P001,Electronics,1200,C001\n2024-01-07,P002,Electronics,850,C002\n2024-01-10,P001,Electronics,1200,C003\n2024-02-03,P003,Clothing,320,C001\n2024-02-05,P002,Electronics,850,C004\n2024-02-12,P004,Clothing,280,C002\n2024-03-01,P001,Electronics,1200,C005\n2024-03-10,P005,Home,540,C003\n2024-03-15,P003,Clothing,320,C006\n\"\"\"\n\nwith open(\"sales_sample.csv\", \"w\") as f:\n    f.write(sales_data)\n\n# Complex analytics query\nresults = query().sql(\"\"\"\n    WITH monthly_sales AS (\n        SELECT \n            DATE_TRUNC('month', CAST(sale_date AS DATE)) as month,\n            product_id,\n            category,\n            SUM(amount) as total_sales,\n            COUNT(*) as sale_count,\n            COUNT(DISTINCT customer_id) as unique_customers\n        FROM 'sales_sample.csv'\n        GROUP BY 1, 2, 3\n    ),\n    ranked_products AS (\n        SELECT \n            *,\n            ROW_NUMBER() OVER (\n                PARTITION BY month, category \n                ORDER BY total_sales DESC\n            ) as category_rank,\n            SUM(total_sales) OVER (\n                PARTITION BY month\n            ) as monthly_total\n        FROM monthly_sales\n    )\n    SELECT \n        STRFTIME(month, '%Y-%m') as month,\n        category,\n        product_id,\n        total_sales,\n        sale_count,\n        unique_customers,\n        category_rank,\n        ROUND(100.0 * total_sales / monthly_total, 2) as pct_of_month\n    FROM ranked_products\n    WHERE category_rank &lt;= 2\n    ORDER BY month, category, category_rank\n\"\"\", backend=\"duckdb\")\n\nprint(\"Top Products by Category Each Month:\")\nprint(\"-\" * 100)\nprint(f\"{'Month':&lt;8} | {'Category':&lt;12} | {'Product':&lt;8} | \"\n      f\"{'Sales':&gt;8} | {'Count':&gt;5} | {'Customers':&gt;9} | {'% of Month':&gt;10}\")\nprint(\"-\" * 100)\n\nfor row in results:\n    print(f\"{row['month']:&lt;8} | {row['category']:&lt;12} | {row['product_id']:&lt;8} | \"\n          f\"${row['total_sales']:&gt;7,} | {row['sale_count']:&gt;5} | \"\n          f\"{row['unique_customers']:&gt;9} | {row['pct_of_month']:&gt;9.1f}%\")\n</code></pre>"},{"location":"examples/duckdb-backend/#example-4-subqueries-and-correlated-queries","title":"Example 4: Subqueries and Correlated Queries","text":"<p>Find employees earning above their department's median salary.</p> <pre><code>from sqlstream import query\n\nresults = query().sql(\"\"\"\n    SELECT \n        e.name,\n        e.department,\n        e.salary,\n        (\n            SELECT MEDIAN(salary) \n            FROM 'employees_sample.csv' sub\n            WHERE sub.department = e.department\n        ) as dept_median,\n        e.salary - (\n            SELECT MEDIAN(salary) \n            FROM 'employees_sample.csv' sub\n            WHERE sub.department = e.department\n        ) as above_median\n    FROM 'employees_sample.csv' e\n    WHERE e.salary &gt; (\n        SELECT MEDIAN(salary) \n        FROM 'employees_sample.csv' sub\n        WHERE sub.department = e.department\n    )\n    ORDER BY above_median DESC\n\"\"\", backend=\"duckdb\")\n\nprint(\"Employees Above Department Median:\")\nprint(\"-\" * 70)\nfor row in results:\n    print(f\"{row['name']:10} | {row['department']:12} | \"\n          f\"${row['salary']:,} (Median: ${row['dept_median']:,.0f}, \"\n          f\"+${row['above_median']:,.0f})\")\n</code></pre>"},{"location":"examples/duckdb-backend/#example-5-string-and-date-functions","title":"Example 5: String and Date Functions","text":"<p>Advanced text processing and date manipulation.</p> <pre><code>from sqlstream import query\n\nresults = query().sql(\"\"\"\n    SELECT \n        name,\n        UPPER(SUBSTRING(name, 1, 1)) || LOWER(SUBSTRING(name, 2)) as formatted_name,\n        department,\n        UPPER(department) as dept_code,\n        CONCAT(SUBSTRING(name, 1, 1), SUBSTRING(department, 1, 3)) as employee_code,\n        hire_date,\n        EXTRACT(YEAR FROM CAST(hire_date AS DATE)) as hire_year,\n        EXTRACT(MONTH FROM CAST(hire_date AS DATE)) as hire_month,\n        DATE_DIFF('day', CAST(hire_date AS DATE), CURRENT_DATE) as days_employed,\n        ROUND(DATE_DIFF('day', CAST(hire_date AS DATE), CURRENT_DATE) / 365.25, 1) as years_employed,\n        CASE \n            WHEN DATE_DIFF('year', CAST(hire_date AS DATE), CURRENT_DATE) &gt;= 3 \n            THEN 'Senior'\n            WHEN DATE_DIFF('year', CAST(hire_date AS DATE), CURRENT_DATE) &gt;= 1 \n            THEN 'Intermediate'\n            ELSE 'Junior'\n        END as seniority_level\n    FROM 'employees_sample.csv'\n    ORDER BY hire_date\n\"\"\", backend=\"duckdb\")\n\nprint(\"Employee Details with Computed Fields:\")\nprint(\"-\" * 100)\nfor row in results:\n    print(f\"{row['employee_code']:8} | {row['formatted_name']:10} | \"\n          f\"{row['dept_code']:12} | Hired: {row['hire_year']}-{row['hire_month']:02d} | \"\n          f\"{row['years_employed']:4.1f} yrs | {row['seniority_level']}\")\n</code></pre>"},{"location":"examples/duckdb-backend/#example-6-statistical-functions","title":"Example 6: Statistical Functions","text":"<p>Calculate comprehensive statistics for salary by department.</p> <pre><code>from sqlstream import query\n\nresults = query().sql(\"\"\"\n    SELECT \n        department,\n        COUNT(*) as employee_count,\n        MIN(salary) as min_salary,\n        MAX(salary) as max_salary,\n        AVG(salary) as mean_salary,\n        MEDIAN(salary) as median_salary,\n        STDDEV(salary) as salary_stddev,\n        VARIANCE(salary) as salary_variance,\n        PERCENTILE_CONT(0.25) WITHIN GROUP (ORDER BY salary) as Q1,\n        PERCENTILE_CONT(0.75) WITHIN GROUP (ORDER BY salary) as Q3,\n        PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY salary) as P95\n    FROM 'employees_sample.csv'\n    GROUP BY department\n    ORDER BY mean_salary DESC\n\"\"\", backend=\"duckdb\")\n\nprint(\"Salary Statistics by Department:\")\nprint(\"-\" * 120)\nprint(f\"{'Department':&lt;12} | {'Count':&gt;5} | {'Min':&gt;8} | {'Max':&gt;8} | \"\n      f\"{'Mean':&gt;8} | {'Median':&gt;8} | {'StdDev':&gt;8} | {'Q1':&gt;8} | {'Q3':&gt;8}\")\nprint(\"-\" * 120)\n\nfor row in results:\n    print(f\"{row['department']:&lt;12} | {row['employee_count']:&gt;5} | \"\n          f\"${row['min_salary']:&gt;7,} | ${row['max_salary']:&gt;7,} | \"\n          f\"${row['mean_salary']:&gt;7,.0f} | ${row['median_salary']:&gt;7,.0f} | \"\n          f\"${row['salary_stddev']:&gt;7,.0f} | ${row['Q1']:&gt;7,.0f} | ${row['Q3']:&gt;7,.0f}\")\n</code></pre>"},{"location":"examples/duckdb-backend/#example-7-union-and-set-operations","title":"Example 7: UNION and Set Operations","text":"<p>Combine data from multiple sources.</p> <pre><code>from sqlstream import query\n\n# Create contractor data\ncontractor_data = \"\"\"\nname,role,hourly_rate\nIan,Engineer,125\nJane,Designer,95\nKyle,Engineer,135\n\"\"\"\n\nwith open(\"contractors_sample.csv\", \"w\") as f:\n    f.write(contractor_data)\n\n# UNION example\nresults = query().sql(\"\"\"\n    -- Full-time employees\n    SELECT \n        name,\n        department as role_dept,\n        salary / 2080 as hourly_equivalent,\n        'Full-time' as employment_type\n    FROM 'employees_sample.csv'\n    WHERE department = 'Engineering'\n\n    UNION ALL\n\n    -- Contractors\n    SELECT \n        name,\n        role as role_dept,\n        hourly_rate as hourly_equivalent,\n        'Contractor' as employment_type\n    FROM 'contractors_sample.csv'\n    WHERE role = 'Engineer'\n\n    ORDER BY hourly_equivalent DESC\n\"\"\", backend=\"duckdb\")\n\nprint(\"Engineering Workforce (Full-time + Contractors):\")\nprint(\"-\" * 80)\nfor row in results:\n    print(f\"{row['name']:10} | {row['role_dept']:12} | \"\n          f\"${row['hourly_equivalent']:&gt;6.2f}/hr | {row['employment_type']}\")\n</code></pre>"},{"location":"examples/duckdb-backend/#example-8-join-multiple-files","title":"Example 8: JOIN Multiple Files","text":"<p>Combine employees, departments, and location data.</p> <pre><code>from sqlstream import query\n\n# Create additional data files\ndept_data = \"\"\"\ndept_id,department,budget\nENG,Engineering,500000\nSAL,Sales,300000\nMKT,Marketing,200000\n\"\"\"\n\nlocation_data = \"\"\"\ndepartment,city,country\nEngineering,San Francisco,USA\nSales,New York,USA\nMarketing,Austin,USA\n\"\"\"\n\nwith open(\"departments_sample.csv\", \"w\") as f:\n    f.write(dept_data)\n\nwith open(\"locations_sample.csv\", \"w\") as f:\n    f.write(location_data)\n\n# Three-way JOIN\nresults = query().sql(\"\"\"\n    SELECT \n        e.name,\n        e.department,\n        e.salary,\n        d.budget,\n        l.city,\n        l.country,\n        ROUND(100.0 * e.salary / d.budget, 2) as pct_of_budget\n    FROM 'employees_sample.csv' e\n    JOIN 'departments_sample.csv' d \n        ON UPPER(SUBSTRING(e.department, 1, 3)) = d.dept_id\n    JOIN 'locations_sample.csv' l \n        ON e.department = l.department\n    ORDER BY e.department, e.salary DESC\n\"\"\", backend=\"duckdb\")\n\nprint(\"Employee Details with Department and Location:\")\nprint(\"-\" * 100)\nfor row in results:\n    print(f\"{row['name']:10} | {row['department']:12} | \"\n          f\"${row['salary']:&gt;7,} | {row['city']:15} | \"\n          f\"{row['pct_of_budget']:&gt;5.2f}% of budget\")\n</code></pre>"},{"location":"examples/duckdb-backend/#best-practices","title":"Best Practices","text":""},{"location":"examples/duckdb-backend/#1-use-duckdb-for-complex-analytics","title":"1. Use DuckDB for Complex Analytics","text":"<pre><code># \u2705 Good: DuckDB excels at this\nresults = query().sql(\"\"\"\n    WITH ranked AS (\n        SELECT *, ROW_NUMBER() OVER (PARTITION BY category ORDER BY sales DESC) as rn\n        FROM 'sales.csv'\n    )\n    SELECT * FROM ranked WHERE rn &lt;= 10\n\"\"\", backend=\"duckdb\")\n</code></pre>"},{"location":"examples/duckdb-backend/#2-leverage-date-functions","title":"2. Leverage Date Functions","text":"<pre><code># \u2705 Good: Extract insights from dates\nresults = query().sql(\"\"\"\n    SELECT \n        DATE_TRUNC('week', order_date) as week,\n        COUNT(*) as orders,\n        SUM(amount) as revenue\n    FROM 'orders.csv'\n    WHERE order_date &gt;= CURRENT_DATE - INTERVAL 90 DAY\n    GROUP BY 1\n    ORDER BY 1\n\"\"\", backend=\"duckdb\")\n</code></pre>"},{"location":"examples/duckdb-backend/#3-use-window-functions-for-rankings","title":"3. Use Window Functions for Rankings","text":"<pre><code># \u2705 Good: Efficient ranking\nresults = query().sql(\"\"\"\n    SELECT \n        *,\n        DENSE_RANK() OVER (ORDER BY score DESC) as rank\n    FROM 'leaderboard.csv'\n\"\"\", backend=\"duckdb\")\n</code></pre>"},{"location":"examples/duckdb-backend/#see-also","title":"See Also","text":"<ul> <li>DuckDB Backend Guide - Complete backend documentation</li> <li>SQL Support - Supported SQL syntax</li> <li>Python API - Programmatic usage</li> <li>Performance Tips - Backend comparison</li> </ul>"},{"location":"examples/joins/","title":"JOIN Examples","text":""},{"location":"examples/joins/#joining-csv-files","title":"Joining CSV Files","text":"<p>Suppose you have <code>users.csv</code> and <code>orders.csv</code>.</p> <p>users.csv <pre><code>id,name,email\n1,Alice,alice@example.com\n2,Bob,bob@example.com\n</code></pre></p> <p>orders.csv <pre><code>order_id,user_id,amount\n101,1,50.00\n102,1,25.00\n103,2,100.00\n</code></pre></p> <p>Query: <pre><code>SELECT \n    u.name, \n    o.amount \nFROM 'users.csv' u \nJOIN 'orders.csv' o \nON u.id = o.user_id\n</code></pre></p>"},{"location":"examples/joins/#joining-csv-and-parquet","title":"Joining CSV and Parquet","text":"<p>Query: <pre><code>SELECT \n    p.product_name, \n    s.quantity \nFROM 'products.csv' p \nJOIN 'sales.parquet' s \nON p.id = s.product_id\n</code></pre></p>"},{"location":"examples/joins/#self-join","title":"Self Join","text":"<p>Query: <pre><code>SELECT \n    e1.name as employee, \n    e2.name as manager \nFROM 'employees.csv' e1 \nJOIN 'employees.csv' e2 \nON e1.manager_id = e2.id\n</code></pre></p>"},{"location":"examples/real-world/","title":"Real-World Use Cases","text":""},{"location":"examples/real-world/#sales-analysis","title":"Sales Analysis","text":"<p>Analyze sales data spread across daily CSV files.</p> <p><pre><code>SELECT \n    product_id, \n    SUM(amount) as total_sales \nFROM 'sales_*.csv' \nGROUP BY product_id \nORDER BY total_sales DESC \nLIMIT 5\n</code></pre> (Note: Wildcard support depends on shell expansion or specific reader implementation)</p>"},{"location":"examples/real-world/#log-analysis","title":"Log Analysis","text":"<p>Find the top 10 IP addresses with the most 404 errors from a web server log.</p> <pre><code>SELECT \n    ip_address, \n    COUNT(*) as error_count \nFROM 'access_logs.csv' \nWHERE status_code = 404 \nGROUP BY ip_address \nORDER BY error_count DESC \nLIMIT 10\n</code></pre>"},{"location":"examples/real-world/#data-quality-check","title":"Data Quality Check","text":"<p>Identify records with missing critical information.</p> <pre><code>SELECT * \nFROM 'users.parquet' \nWHERE email IS NULL OR phone IS NULL\n</code></pre>"},{"location":"examples/real-world/#cross-reference-data","title":"Cross-Reference Data","text":"<p>Check which users in your CSV database have placed orders recorded in an S3 Parquet data lake.</p> <pre><code>SELECT \n    u.email \nFROM 'local_users.csv' u \nJOIN 's3://datalake/orders.parquet' o \nON u.id = o.user_id \nGROUP BY u.email\n</code></pre>"},{"location":"features/advanced-formats/","title":"Advanced Data Formats","text":"<p>SQLStream goes beyond standard CSV and Parquet support to handle semi-structured and document-based data formats.</p>"},{"location":"features/advanced-formats/#html-tables","title":"HTML Tables","text":"<p>You can query HTML tables directly from files or URLs. SQLStream uses <code>pandas</code> to extract tables and makes them queryable.</p>"},{"location":"features/advanced-formats/#usage","title":"Usage","text":"<pre><code>-- Query a local HTML file\nSELECT * FROM \"data.html\";\n\n-- Query a remote HTML file\nSELECT * FROM \"https://example.com/data.html\";\n</code></pre>"},{"location":"features/advanced-formats/#multiple-tables","title":"Multiple Tables","text":"<p>If an HTML file contains multiple tables, you can select which one to query using the URL Fragment Syntax:</p> <pre><code>-- Select the second table (index 1)\nSELECT * FROM \"data.html#:1\";\n\n-- Select the last table\nSELECT * FROM \"data.html#:-1\";\n</code></pre>"},{"location":"features/advanced-formats/#schema-inference","title":"Schema Inference","text":"<p>Column names are extracted from the table header (<code>&lt;th&gt;</code> tags). Data types are inferred based on the content of the columns (Integer, Float, Boolean, String).</p>"},{"location":"features/advanced-formats/#markdown-tables","title":"Markdown Tables","text":"<p>SQLStream supports querying GitHub Flavored Markdown (GFM) tables. This is perfect for analyzing data embedded in <code>README.md</code> files or documentation.</p>"},{"location":"features/advanced-formats/#usage_1","title":"Usage","text":"<pre><code>-- Query a local Markdown file\nSELECT * FROM \"README.md\";\n\n-- Query a remote Markdown file\nSELECT * FROM \"https://raw.githubusercontent.com/user/repo/main/README.md\";\n</code></pre>"},{"location":"features/advanced-formats/#multiple-tables_1","title":"Multiple Tables","text":"<p>Like HTML, you can select specific tables if the file contains more than one:</p> <pre><code>-- Select the first table (default)\nSELECT * FROM \"README.md#:0\";\n\n-- Select the second table\nSELECT * FROM \"README.md#:1\";\n</code></pre>"},{"location":"features/advanced-formats/#features","title":"Features","text":"<ul> <li>Type Inference: Automatically detects Integers, Floats, and Booleans (e.g., <code>true</code>, <code>false</code>, <code>yes</code>, <code>no</code>).</li> <li>NULL Handling: Recognizes <code>null</code>, <code>none</code>, <code>n/a</code>, and <code>-</code> as NULL values.</li> <li>Escaping: Handles escaped pipe characters <code>\\|</code> in cell content.</li> </ul>"},{"location":"features/advanced-formats/#supported-formats","title":"Supported Formats","text":""},{"location":"features/advanced-formats/#1-html-tables","title":"1. HTML Tables","text":"<p>Query tables directly from HTML files or web pages using pandas' <code>read_html</code> functionality.</p> <pre><code>from sqlstream import query\n\n# Query HTML file\nresult = query(\"SELECT * FROM employees.html WHERE Department = 'Engineering'\")\n\n# Query specific table (if multiple tables in HTML)\nfrom sqlstream.readers.html_reader import HTMLReader\nreader = HTMLReader(\"data.html\", table_index=1)  # Select second table\n</code></pre> <p>Features: - Automatically extracts all <code>&lt;table&gt;</code> elements from HTML - Select specific tables by index (0-based) - Match tables by text content using <code>match</code> parameter - Supports both local files and HTTP URLs</p> <p>Example: <pre><code>from sqlstream.readers.html_reader import HTMLReader\n\n# Get the first table\nreader = HTMLReader(\"report.html\", table_index=0)\n\n# Find table containing specific text\nreader = HTMLReader(\"report.html\", match=\"Sales Data\")\n\n# List all tables in the HTML\nfor desc in reader.list_tables():\n    print(desc)\n</code></pre></p>"},{"location":"features/advanced-formats/#2-markdown-tables","title":"2. Markdown Tables","text":"<p>Parse and query GitHub Flavored Markdown tables directly.</p> <pre><code>from sqlstream import query\n\n# Query markdown file\nresult = query(\"SELECT Product, Sales FROM data.md WHERE Sales &gt; 100\")\n</code></pre> <p>Supported Markdown Table Format: <pre><code>| Column1 | Column2 | Column3 |\n|:--------|:-------:|--------:|\n| Value1  | Value2  | Value3  |\n| Value4  | Value5  | Value6  |\n</code></pre></p> <p>Features: - Automatic type inference (integers, floats, booleans, strings) - Support for alignment indicators (<code>:---</code>, <code>:---:</code>, <code>---:</code>) - Handles escaped pipe characters (<code>\\|</code>) - Multiple table support (select by index) - NULL value recognition (<code>null</code>, <code>NULL</code>, <code>N/A</code>, <code>-</code>)</p> <p>Example: <pre><code>from sqlstream.readers.markdown_reader import MarkdownReader\n\n# Query first table\nreader = MarkdownReader(\"data.md\", table_index=0)\n\n# List all tables in markdown\nfor desc in reader.list_tables():\n    print(desc)\n\n# Access data\nfor row in reader.read_lazy():\n    print(row)\n</code></pre></p>"},{"location":"features/advanced-formats/#3-format-specification-for-urls","title":"3. Format Specification for URLs","text":"<p>For URLs that don't have file extensions (like pastebin raw URLs), you can explicitly specify the format.</p> <pre><code>from sqlstream.readers.http_reader import HTTPReader\n\n# Pastebin raw URL without .csv extension\nreader = HTTPReader(\n    \"https://pastebin.com/raw/xxxxxx\",\n    format=\"csv\"  # Explicitly specify format\n)\n\n# Works with all supported formats\nreader = HTTPReader(url, format=\"parquet\")\nreader = HTTPReader(url, format=\"html\")\nreader = HTTPReader(url, format=\"markdown\")\n</code></pre> <p>With Pandas Executor: <pre><code>from sqlstream.core.pandas_executor import PandasExecutor\n\nexecutor = PandasExecutor()\ndf = executor._load_dataframe(\n    \"https://pastebin.com/raw/xxxxxx\",\n    format=\"csv\"\n)\n</code></pre></p>"},{"location":"features/advanced-formats/#format-auto-detection","title":"Format Auto-Detection","text":"<p>SQLstream intelligently detects file formats through multiple methods:</p>"},{"location":"features/advanced-formats/#1-file-extension-detection","title":"1. File Extension Detection","text":"<pre><code># Automatically detected from extension\nquery(\"SELECT * FROM data.csv\")      # \u2192 CSV\nquery(\"SELECT * FROM data.parquet\")  # \u2192 Parquet  \nquery(\"SELECT * FROM data.html\")     # \u2192 HTML\nquery(\"SELECT * FROM data.md\")       # \u2192 Markdown\n</code></pre>"},{"location":"features/advanced-formats/#2-content-based-detection","title":"2. Content-Based Detection","text":"<p>For files without extensions or ambiguous URLs, SQLstream peeks at the content:</p> <pre><code># File without extension - auto-detects from content\nreader = HTTPReader(\"https://example.com/data\")  # Checks content type\n\n# Detection logic:\n# - Looks for HTML tags: &lt;html&gt;, &lt;table&gt;, &lt;!doctype&gt;\n# - Looks for Markdown table markers: |, ---\n# - Checks for Parquet magic number: PAR1\n# - Defaults to CSV for others\n</code></pre>"},{"location":"features/advanced-formats/#3-http-content-type-headers","title":"3. HTTP Content-Type Headers","text":"<p>For HTTP URLs, the Content-Type header is used when available.</p>"},{"location":"features/advanced-formats/#complete-usage-examples","title":"Complete Usage Examples","text":""},{"location":"features/advanced-formats/#example-1-html-sales-report","title":"Example 1: HTML Sales Report","text":"<pre><code>from sqlstream import query\n\n# HTML file with sales table\nresult = query(\"\"\"\n    SELECT Region, SUM(Sales) as total_sales\n    FROM quarterly_report.html\n    WHERE Quarter = 'Q2'\n    GROUP BY Region\n    ORDER BY total_sales DESC\n\"\"\")\n\nprint(result)\n</code></pre>"},{"location":"features/advanced-formats/#example-2-markdown-documentation","title":"Example 2: Markdown Documentation","text":"<pre><code># API Metrics\n\n| Endpoint      | Requests | Avg_Latency | Error_Rate |\n|:--------------|:--------:|:-----------:|-----------:|\n| /api/users    | 1500     | 45.2        | 0.2        |\n| /api/products | 3200     | 32.1        | 0.1        |\n| /api/orders   | 890      | 120.5       | 1.5        |\n</code></pre> <pre><code>result = query(\"\"\"\n    SELECT Endpoint, Requests, Error_Rate \n    FROM api_metrics.md\n    WHERE Error_Rate &gt; 1.0\n\"\"\")\n</code></pre>"},{"location":"features/advanced-formats/#example-3-pastebin-data","title":"Example 3: Pastebin Data","text":"<pre><code>from sqlstream.readers.http_reader import HTTPReader\n\n# Pastebin raw URL (no .csv extension)\nreader = HTTPReader(\n    \"https://pastebin.com/raw/abc123\", \n    format=\"csv\"\n)\n\n# Now use it in queries\nfrom sqlstream import query_df\nimport pandas as pd\n\ndf = pd.read_csv(reader.local_path)\n# Or use with SQLstream directly\n</code></pre>"},{"location":"features/advanced-formats/#example-4-multiple-tables","title":"Example 4: Multiple Tables","text":"<pre><code>from sqlstream.readers.html_reader import HTMLReader\n\n# HTML with multiple tables\nreader = HTMLReader(\"report.html\")\n\n# List all tables\nprint(\"Available tables:\")\nfor i, desc in enumerate(reader.list_tables()):\n    print(f\"{i}: {desc}\")\n\n# Query specific table\nreader_table2 = HTMLReader(\"report.html\", table_index=2)\nfor row in reader_table2.read_lazy():\n    print(row)\n</code></pre>"},{"location":"features/advanced-formats/#advanced-options","title":"Advanced Options","text":""},{"location":"features/advanced-formats/#html-reader-options","title":"HTML Reader Options","text":"<pre><code>from sqlstream.readers.html_reader import HTMLReader\n\n# Pass options to pandas read_html\nreader = HTMLReader(\n    \"data.html\",\n    table_index=0,\n    match=\"Sales\",           # Find table containing \"Sales\"\n    header=0,                # Which row is header\n    encoding='utf-8',        # File encoding\n    thousands=',',           # Thousands separator\n    decimal='.'              # Decimal separator\n)\n</code></pre>"},{"location":"features/advanced-formats/#markdown-reader-options","title":"Markdown Reader Options","text":"<pre><code>from sqlstream.readers.markdown_reader import MarkdownReader\n\n# Select which table to read\nreader = MarkdownReader(\n    \"document.md\",\n    table_index=1  # Second table (0-indexed)\n)\n\n# Apply filters and column selection\nreader.set_filter([...])\nreader.set_columns(['col1', 'col2'])\n</code></pre>"},{"location":"features/advanced-formats/#requirements","title":"Requirements","text":"<ul> <li> <p>HTML Support: Requires <code>pandas</code>, <code>bs4</code> and <code>lxml</code> or <code>html5lib</code> <pre><code>pip install sqlstream[html]\n</code></pre></p> </li> <li> <p>Markdown Support: No additional dependencies (built-in parser)</p> </li> <li> <p>HTTP Support: Requires <code>httpx</code> <pre><code>pip install sqlstream[http]\n</code></pre></p> </li> </ul>"},{"location":"features/advanced-formats/#performance-tips","title":"Performance Tips","text":"<ol> <li> <p>HTML tables: For large HTML files with many tables, use <code>table_index</code> or <code>match</code> to avoid parsing unnecessary tables.</p> </li> <li> <p>Markdown tables: Markdown parsing is done in pure Python. For very large tables, consider converting to CSV first.</p> </li> <li> <p>URL caching: HTTPReader automatically caches downloaded files. Use <code>force_download=True</code> to refresh.</p> </li> </ol> <pre><code>reader = HTTPReader(url, force_download=True)  # Re-download\n</code></pre> <ol> <li>Clear cache when done: <pre><code>HTTPReader.clear_all_cache()\n</code></pre></li> </ol>"},{"location":"features/advanced-formats/#migration-guide","title":"Migration Guide","text":""},{"location":"features/advanced-formats/#old-way-limited-to-csvparquet","title":"Old Way (Limited to .csv/.parquet)","text":"<pre><code># Only worked with proper extensions\nquery(\"SELECT * FROM https://example.com/data.csv\")  # \u2705\nquery(\"SELECT * FROM https://pastebin.com/raw/xxx\")  # \u274c Failed!\n</code></pre>"},{"location":"features/advanced-formats/#new-way-flexible-format-support","title":"New Way (Flexible Format Support)","text":"<pre><code># Explicit format for extension-less URLs\nfrom sqlstream.readers.http_reader import HTTPReader\nreader = HTTPReader(\"https://pastebin.com/raw/xxx\", format=\"csv\")  # \u2705\n\n# HTML and Markdown support\nquery(\"SELECT * FROM report.html\")  # \u2705\nquery(\"SELECT * FROM data.md\")      # \u2705\n</code></pre>"},{"location":"features/aggregations/","title":"Aggregations","text":"<p>SQLStream supports standard SQL aggregation functions, allowing you to perform calculations on your data.</p>"},{"location":"features/aggregations/#supported-functions","title":"Supported Functions","text":"<ul> <li>COUNT(*): Counts the total number of rows.</li> <li>COUNT(column): Counts the number of non-null values in a column.</li> <li>SUM(column): Calculates the sum of a numeric column.</li> <li>AVG(column): Calculates the average value of a numeric column.</li> <li>MIN(column): Finds the minimum value.</li> <li>MAX(column): Finds the maximum value.</li> </ul>"},{"location":"features/aggregations/#group-by","title":"GROUP BY","text":"<p>You can group results by one or more columns using the <code>GROUP BY</code> clause.</p> <pre><code>SELECT \n    category, \n    COUNT(*) as count, \n    AVG(price) as avg_price \nFROM 'products.csv' \nGROUP BY category\n</code></pre>"},{"location":"features/aggregations/#backend-differences","title":"Backend Differences","text":"<ul> <li>Python Backend: Computes aggregations in a streaming fashion where possible, or accumulates state for grouping.</li> <li>Pandas Backend: Uses optimized Pandas <code>groupby</code> and aggregation methods for high performance.</li> </ul>"},{"location":"features/data-sources/","title":"Data Sources","text":"<p>SQLStream supports multiple data source types.</p>"},{"location":"features/data-sources/#csv-files","title":"CSV Files","text":"<pre><code>from sqlstream import query\n\n# Local CSV\nresults = query(\"data.csv\").sql(\"SELECT * FROM data\")\n\n# CSV with custom delimiter\n# Auto-detected: comma, tab, pipe, semicolon\n</code></pre>"},{"location":"features/data-sources/#parquet-files","title":"Parquet Files","text":"<pre><code>pip install \"sqlstream[parquet]\"\n</code></pre> <pre><code>results = query(\"data.parquet\").sql(\"SELECT * FROM data\")\n</code></pre>"},{"location":"features/data-sources/#http-urls","title":"HTTP URLs","text":"<pre><code>pip install \"sqlstream[http]\"\n</code></pre> <pre><code>results = query(\"https://example.com/data.csv\").sql(\"SELECT * FROM data\")\n</code></pre>"},{"location":"features/data-sources/#inline-paths","title":"Inline Paths","text":"<pre><code>sqlstream query \"SELECT * FROM 'data.csv'\"\nsqlstream query \"SELECT * FROM 'data.parquet'\"\nsqlstream query \"SELECT * FROM 'https://example.com/data.csv'\"\n</code></pre>"},{"location":"features/duckdb-backend/","title":"DuckDB Backend - Full SQL Support","text":"<p>The DuckDB backend provides complete SQL support by leveraging DuckDB's powerful SQL engine while maintaining SQLStream's simple interface.</p>"},{"location":"features/duckdb-backend/#why-duckdb-backend","title":"Why DuckDB Backend?","text":"<p>Choose DuckDB backend when you need: - \u2705 Full SQL compatibility (window functions, CTEs, subqueries, etc.) - \u2705 Maximum performance (10-1000x faster than Python backend) - \u2705 Complex analytical queries - \u2705 Production-ready SQL engine</p> <p>Comparison:</p> Feature Python Backend Pandas Backend DuckDB Backend Speed Baseline 10-100x faster 10-1000x faster Window Functions \u274c \u274c \u2705 CTEs (WITH clause) \u274c \u274c \u2705 Subqueries \u274c \u274c \u2705 HAVING clause \u274c \u274c \u2705 String Functions \u274c Limited \u274c Limited \u2705 Full Date Functions \u274c Limited \u274c Limited \u2705 Full Statistical Functions \u274c \u274c \u2705 Use Case Learning Fast queries Production &amp; Complex SQL"},{"location":"features/duckdb-backend/#installation","title":"Installation","text":"<pre><code># Install DuckDB support\npip install duckdb\n\n# Or install with all SQLStream features\npip install \"sqlstream[all]\"\n</code></pre>"},{"location":"features/duckdb-backend/#usage","title":"Usage","text":""},{"location":"features/duckdb-backend/#python-api","title":"Python API","text":"<pre><code>from sqlstream import query\n\n# Use DuckDB backend explicitly\nresult = query(\"data.csv\").sql(\"\"\"\n    SELECT \n        department,\n        AVG(salary) as avg_salary,\n        ROW_NUMBER() OVER (ORDER BY AVG(salary) DESC) as rank\n    FROM 'data.csv'\n    GROUP BY department\n    HAVING avg_salary &gt; 50000\n\"\"\", backend=\"duckdb\")\n\nfor row in result:\n    print(row)\n</code></pre>"},{"location":"features/duckdb-backend/#cli","title":"CLI","text":"<pre><code># Use DuckDB backend\nsqlstream query \"SELECT * FROM 'data.csv'\" --backend duckdb\n\n# Complex query with window functions\nsqlstream query \"\n    WITH ranked AS (\n        SELECT *, \n               ROW_NUMBER() OVER (PARTITION BY dept ORDER BY salary DESC) as rank\n        FROM 'employees.csv'\n    )\n    SELECT * FROM ranked WHERE rank &lt;= 3\n\" --backend duckdb\n</code></pre>"},{"location":"features/duckdb-backend/#supported-sql-features","title":"Supported SQL Features","text":""},{"location":"features/duckdb-backend/#window-functions","title":"Window Functions \u2705","text":"<pre><code>SELECT \n    name,\n    salary,\n    ROW_NUMBER() OVER (PARTITION BY department ORDER BY salary DESC) as rank,\n    AVG(salary) OVER (PARTITION BY department) as dept_avg,\n    salary - AVG(salary) OVER (PARTITION BY department) as diff_from_avg\nFROM 'employees.csv'\n</code></pre> <p>Supported window functions: - <code>ROW_NUMBER()</code>, <code>RANK()</code>, <code>DENSE_RANK()</code> - <code>LAG()</code>, <code>LEAD()</code> - <code>FIRST_VALUE()</code>, <code>LAST_VALUE()</code>, <code>NTH_VALUE()</code> - <code>SUM</code>, <code>AVG</code>, <code>MIN</code>, <code>MAX</code>, <code>COUNT</code> with <code>OVER</code></p>"},{"location":"features/duckdb-backend/#common-table-expressions-ctes","title":"Common Table Expressions (CTEs) \u2705","text":"<pre><code>WITH high_earners AS (\n    SELECT * FROM 'employees.csv'\n    WHERE salary &gt; 100000\n),\ndept_stats AS (\n    SELECT \n        department,\n        AVG(salary) as avg_salary\n    FROM high_earners\n    GROUP BY department\n)\nSELECT * FROM dept_stats\nWHERE avg_salary &gt; 120000\n</code></pre>"},{"location":"features/duckdb-backend/#subqueries","title":"Subqueries \u2705","text":"<pre><code>-- Subquery in FROM\nSELECT department, avg_salary\nFROM (\n    SELECT department, AVG(salary) as avg_salary\n    FROM 'employees.csv'\n    GROUP BY department\n) dept_avgs\nWHERE avg_salary &gt; 75000\n\n-- Subquery in WHERE\nSELECT * FROM 'employees.csv'\nWHERE salary &gt; (SELECT AVG(salary) FROM 'employees.csv')\n\n-- Correlated subquery\nSELECT e1.*\nFROM 'employees.csv' e1\nWHERE salary &gt; (\n    SELECT AVG(salary) \n    FROM 'employees.csv' e2 \n    WHERE e2.department = e1.department\n)\n</code></pre>"},{"location":"features/duckdb-backend/#having-clause","title":"HAVING Clause \u2705","text":"<pre><code>SELECT \n    department,\n    AVG(salary) as avg_salary,\n    COUNT(*) as employee_count\nFROM 'employees.csv'\nGROUP BY department\nHAVING COUNT(*) &gt; 10 AND AVG(salary) &gt; 75000\n</code></pre>"},{"location":"features/duckdb-backend/#string-functions","title":"String Functions \u2705","text":"<pre><code>SELECT \n    UPPER(name) as name_upper,\n    LOWER(department) as dept_lower,\n    LENGTH(name) as name_length,\n    SUBSTRING(name, 1, 5) as name_prefix,\n    CONCAT(first_name, ' ', last_name) as full_name,\n    TRIM(email) as email_clean,\n    REPLACE(phone, '-', '') as phone_digits\nFROM 'employees.csv'\n</code></pre> <p>Supported: <code>UPPER</code>, <code>LOWER</code>, <code>LENGTH</code>, <code>SUBSTRING</code>, <code>CONCAT</code>, <code>TRIM</code>, <code>REPLACE</code>, <code>SPLIT</code>, <code>REGEXP_MATCHES</code>, and many more.</p>"},{"location":"features/duckdb-backend/#datetime-functions","title":"Date/Time Functions \u2705","text":"<pre><code>SELECT \n    hire_date,\n    EXTRACT(YEAR FROM hire_date) as hire_year,\n    EXTRACT(MONTH FROM hire_date) as hire_month,\n    DATE_DIFF('day', hire_date, CURRENT_DATE) as days_since_hire,\n    DATE_ADD(hire_date, INTERVAL 1 YEAR) as first_anniversary,\n    STRFTIME(hire_date, '%Y-%m') as year_month\nFROM 'employees.csv'\n</code></pre> <p>Supported: <code>EXTRACT</code>, <code>DATE_DIFF</code>, <code>DATE_ADD</code>, <code>DATE_SUB</code>, <code>CURRENT_DATE</code>, <code>CURRENT_TIME</code>, <code>STRFTIME</code>, and more.</p>"},{"location":"features/duckdb-backend/#statistical-functions","title":"Statistical Functions \u2705","text":"<pre><code>SELECT \n    department,\n    AVG(salary) as mean_salary,\n    STDDEV(salary) as salary_stddev,\n    VARIANCE(salary) as salary_variance,\n    MEDIAN(salary) as median_salary,\n    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY salary) as p95_salary\nFROM 'employees.csv'\nGROUP BY department\n</code></pre>"},{"location":"features/duckdb-backend/#unionintersectexcept","title":"UNION/INTERSECT/EXCEPT \u2705","text":"<pre><code>-- UNION\nSELECT name FROM 'employees.csv' WHERE department = 'Engineering'\nUNION ALL\nSELECT name FROM 'contractors.csv' WHERE role = 'Engineer'\n\n-- INTERSECT\nSELECT email FROM 'employees.csv'\nINTERSECT\nSELECT email FROM 'active_users.csv'\n\n-- EXCEPT\nSELECT email FROM 'all_users.csv'\nEXCEPT\nSELECT email FROM 'blocked_users.csv'\n</code></pre>"},{"location":"features/duckdb-backend/#performance","title":"Performance","text":"<p>DuckDB backend provides exceptional performance:</p>"},{"location":"features/duckdb-backend/#benchmarks","title":"Benchmarks","text":"Query Type Python Pandas DuckDB Speedup Simple SELECT 1.2s 0.15s 0.02s 60x Complex JOIN 8.5s 0.8s 0.05s 170x Window Functions \u274c \u274c 0.12s \u221e Aggregations 2.1s 0.3s 0.04s 52x <p>Benchmark on 1M row dataset, Intel i7, 16GB RAM</p>"},{"location":"features/duckdb-backend/#file-format-support","title":"File Format Support","text":"<p>DuckDB backend supports all SQLStream file formats by leveraging the unified Reader architecture:</p>"},{"location":"features/duckdb-backend/#csv-files","title":"CSV Files","text":"<pre><code>query(\"data.csv\").sql(\"SELECT * FROM 'data.csv'\", backend=\"duckdb\")\n</code></pre>"},{"location":"features/duckdb-backend/#parquet-files","title":"Parquet Files","text":"<pre><code>query(\"data.parquet\").sql(\"SELECT * FROM 'data.parquet'\", backend=\"duckdb\")\n</code></pre>"},{"location":"features/duckdb-backend/#html-tables","title":"HTML Tables","text":"<pre><code># Query tables embedded in HTML files\nquery(\"report.html\").sql(\"SELECT * FROM 'report.html#html:0'\", backend=\"duckdb\")\n</code></pre>"},{"location":"features/duckdb-backend/#markdown-tables","title":"Markdown Tables","text":"<pre><code># Query tables in Markdown documents\nquery(\"README.md\").sql(\"SELECT * FROM 'README.md#markdown:0'\", backend=\"duckdb\")\n</code></pre>"},{"location":"features/duckdb-backend/#s3-files","title":"S3 Files","text":"<pre><code># Automatically handles S3 authentication via s3fs\nquery(\"s3://bucket/data.parquet\").sql(\n    \"SELECT * FROM 's3://bucket/data.parquet' WHERE date &gt; '2024-01-01'\",\n    backend=\"duckdb\"\n)\n</code></pre>"},{"location":"features/duckdb-backend/#http-urls","title":"HTTP URLs","text":"<pre><code># Works with CSV, Parquet, HTML, etc. over HTTP\nquery(\"https://example.com/data.csv\").sql(\n    \"SELECT * FROM 'https://example.com/data.csv'\",\n    backend=\"duckdb\"\n)\n</code></pre>"},{"location":"features/duckdb-backend/#advanced-usage","title":"Advanced Usage","text":""},{"location":"features/duckdb-backend/#multiple-files","title":"Multiple Files","text":"<pre><code>result = query(\"employees.csv\").sql(\"\"\"\n    SELECT \n        e.name,\n        e.salary,\n        d.department_name,\n        l.city\n    FROM 'employees.csv' e\n    JOIN 'departments.csv' d ON e.dept_id = d.id\n    JOIN 'locations.csv' l ON d.location_id = l.id\n    WHERE e.salary &gt; 75000\n    ORDER BY e.salary DESC\n\"\"\", backend=\"duckdb\")\n</code></pre>"},{"location":"features/duckdb-backend/#complex-analytics","title":"Complex Analytics","text":"<pre><code>result = query(\"sales.parquet\").sql(\"\"\"\n    WITH monthly_sales AS (\n        SELECT \n            DATE_TRUNC('month', sale_date) as month,\n            product_id,\n            SUM(amount) as total_sales,\n            COUNT(*) as sale_count\n        FROM 'sales.parquet'\n        WHERE sale_date &gt;= '2024-01-01'\n        GROUP BY 1, 2\n    ),\n    ranked_products AS (\n        SELECT \n            *,\n            ROW_NUMBER() OVER (PARTITION BY month ORDER BY total_sales DESC) as rank\n        FROM monthly_sales\n    )\n    SELECT \n        month,\n        product_id,\n        total_sales,\n        sale_count,\n        rank\n    FROM ranked_products\n    WHERE rank &lt;= 10\n    ORDER BY month, rank\n\"\"\", backend=\"duckdb\")\n</code></pre>"},{"location":"features/duckdb-backend/#backend-selection","title":"Backend Selection","text":""},{"location":"features/duckdb-backend/#auto-detection-default","title":"Auto-Detection (Default)","text":"<pre><code># Automatically selects: Pandas &gt; DuckDB &gt; Python\nresult = query(\"data.csv\").sql(\"SELECT * FROM 'data.csv'\")\n</code></pre> <p>Priority order: 1. Pandas (if installed) - for backward compatibility 2. DuckDB (if installed and pandas not available) 3. Python (fallback)</p>"},{"location":"features/duckdb-backend/#explicit-selection","title":"Explicit Selection","text":"<pre><code># Force DuckDB (raises error if not installed)\nresult = query(\"data.csv\").sql(\n    \"SELECT * FROM 'data.csv' WHERE ...\",\n    backend=\"duckdb\"\n)\n\n# Force Pandas\nresult = query(\"data.csv\").sql(\"...\", backend=\"pandas\")\n\n# Force Python (educational)\nresult = query(\"data.csv\").sql(\"...\", backend=\"python\")\n</code></pre>"},{"location":"features/duckdb-backend/#limitations","title":"Limitations","text":""},{"location":"features/duckdb-backend/#known-limitations","title":"Known Limitations","text":"<ol> <li>Requires DuckDB: Must install separately (<code>pip install duckdb</code>)</li> <li>Raw SQL Required: Currently requires storing raw SQL (no AST reconstruction)</li> <li>Read-Only: No INSERT/UPDATE/DELETE (same as other backends)</li> </ol>"},{"location":"features/duckdb-backend/#not-supported-duckdb-limitations","title":"Not Supported (DuckDB Limitations)","text":"<ul> <li>Write operations (INSERT, UPDATE, DELETE)</li> <li>User-defined functions (UDFs) from Python</li> <li>Temporary tables across queries</li> </ul>"},{"location":"features/duckdb-backend/#when-to-use-each-backend","title":"When to Use Each Backend","text":""},{"location":"features/duckdb-backend/#use-python-backend-when","title":"Use Python Backend when:","text":"<ul> <li>Learning query execution internals</li> <li>Understanding the Volcano model</li> <li>Educational purposes</li> <li>Query fits in SQLStream's limited SQL support</li> </ul>"},{"location":"features/duckdb-backend/#use-pandas-backend-when","title":"Use Pandas Backend when:","text":"<ul> <li>Need better performance than Python</li> <li>Query fits in SQLStream's parser</li> <li>Already using pandas in your project</li> <li>Want balance of speed and compatibility</li> </ul>"},{"location":"features/duckdb-backend/#use-duckdb-backend-when","title":"Use DuckDB Backend when:","text":"<ul> <li>\u2705 Need complex SQL (CTEs, window functions, subqueries)</li> <li>\u2705 Maximum performance required</li> <li>\u2705 Production workloads</li> <li>\u2705 Full SQL compatibility needed</li> <li>\u2705 Large datasets (multi-GB files)</li> <li>\u2705 Analytical queries</li> </ul>"},{"location":"features/duckdb-backend/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/duckdb-backend/#duckdb-backend-requested-but-duckdb-is-not-installed","title":"\"DuckDB backend requested but duckdb is not installed\"","text":"<p>Solution: <pre><code>pip install duckdb\n# or\npip install \"sqlstream[all]\"\n</code></pre></p>"},{"location":"features/duckdb-backend/#query-fails-with-duckdb-but-works-with-other-backends","title":"Query fails with DuckDB but works with other backends","text":"<p>DuckDB has stricter SQL syntax. Check: - Table/column names are case-sensitive - String literals use single quotes <code>'...'</code> - File paths are quoted: <code>'file.csv'</code> not <code>file.csv</code></p>"},{"location":"features/duckdb-backend/#s3http-queries-fail","title":"S3/HTTP queries fail","text":"<p>Solution: DuckDB needs httpfs extension (auto-loaded by SQLStream) <pre><code># Should work automatically\nquery(\"s3://bucket/data.parquet\").sql(\n    \"SELECT * FROM 's3://bucket/data.parquet'\",\n    backend=\"duckdb\"\n)\n</code></pre></p>"},{"location":"features/duckdb-backend/#examples","title":"Examples","text":"<p>See Examples for complete use cases.</p>"},{"location":"features/duckdb-backend/#see-also","title":"See Also","text":"<ul> <li>SQL Support (Python/Pandas) - Limited SQL for Python/Pandas backends</li> <li>Pandas Backend - Fast execution for supported SQL</li> <li>FAQ - Common questions about backends</li> <li>Limitations - What's not supported</li> </ul>"},{"location":"features/http-reader/","title":"HTTP Reader","text":"<p>The HTTP Reader allows SQLStream to query data directly from HTTP and HTTPS URLs. It supports intelligent caching, streaming downloads, and automatic format detection.</p>"},{"location":"features/http-reader/#features","title":"Features","text":"<ul> <li>Direct Querying: Query data from any public URL.</li> <li>Caching: Downloads are cached locally to speed up subsequent queries.</li> <li>Format Detection: Automatically detects format from file extension, URL fragments, or content.</li> <li>Streaming: Streams large files efficiently.</li> </ul>"},{"location":"features/http-reader/#usage","title":"Usage","text":""},{"location":"features/http-reader/#basic-usage","title":"Basic Usage","text":"<p>Simply use a URL as the table source in your SQL query:</p> <pre><code>SELECT * FROM \"https://raw.githubusercontent.com/datasets/population/master/data/population.csv\";\n</code></pre>"},{"location":"features/http-reader/#handling-extension-less-urls","title":"Handling Extension-less URLs","text":"<p>For URLs that don't end in a standard file extension (like Pastebin or API endpoints), use the URL Fragment Syntax to specify the format:</p> <pre><code>-- Read CSV data from Pastebin\nSELECT * FROM \"https://pastebin.com/raw/xxxxx#csv\";\n\n-- Read Markdown table from a raw URL\nSELECT * FROM \"https://pastebin.com/raw/cnkgQp1t#markdown\";\n</code></pre>"},{"location":"features/http-reader/#caching","title":"Caching","text":"<p>Downloaded files are cached in the system's temporary directory (e.g., <code>/tmp/sqlstream_cache</code> on Linux). The cache key is based on the URL hash.</p> <ul> <li>Persistent Cache: Files remain in cache until cleared.</li> <li>Force Download: Currently, the CLI doesn't expose a flag to force download, but you can manually clear the cache directory.</li> </ul>"},{"location":"features/http-reader/#supported-formats","title":"Supported Formats","text":"<p>The HTTP Reader supports all formats supported by SQLStream, including: - CSV - Parquet - JSON - HTML (tables) - Markdown (tables)</p>"},{"location":"features/http-reader/#advanced-usage","title":"Advanced Usage","text":""},{"location":"features/http-reader/#reading-specific-tables","title":"Reading Specific Tables","text":"<p>You can combine format specification with table selection for multi-table formats like HTML and Markdown:</p> <pre><code>-- Read the second table from a remote Markdown file\nSELECT * FROM \"https://example.com/data.md#markdown:1\";\n</code></pre>"},{"location":"features/http-reader/#content-type-detection","title":"Content-Type Detection","text":"<p>The reader attempts to detect the format from the file content if the extension is missing or unknown. It checks for: - Parquet: Magic bytes <code>PAR1</code>. - HTML: Tags like <code>&lt;html&gt;</code>, <code>&lt;table&gt;</code>. - Markdown: Table syntax <code>| ... |</code> and <code>|---</code>.</p>"},{"location":"features/inline-paths/","title":"Inline File Paths","text":"<p>One of SQLStream's most powerful features is the ability to query files directly by specifying their paths in the SQL query. This eliminates the need to define \"tables\" or load data beforehand.</p>"},{"location":"features/inline-paths/#syntax","title":"Syntax","text":"<p>Simply enclose the file path in single quotes <code>'</code> within the <code>FROM</code> clause.</p> <pre><code>SELECT * FROM 'path/to/file.csv'\n</code></pre>"},{"location":"features/inline-paths/#supported-path-types","title":"Supported Path Types","text":""},{"location":"features/inline-paths/#local-files","title":"Local Files","text":"<p>Relative or absolute paths to local files.</p> <pre><code>SELECT * FROM 'data.csv'\nSELECT * FROM '/home/user/datasets/sales.parquet'\n</code></pre>"},{"location":"features/inline-paths/#httphttps-urls","title":"HTTP/HTTPS URLs","text":"<p>You can query data directly from the web.</p> <pre><code>SELECT * FROM 'https://raw.githubusercontent.com/datasets/covid-19/master/data/countries-aggregated.csv'\n</code></pre>"},{"location":"features/inline-paths/#s3-buckets","title":"S3 Buckets","text":"<p>If you have the <code>s3fs</code> library installed (<code>pip install sqlstream[s3]</code>), you can query files directly from S3.</p> <pre><code>SELECT * FROM 's3://my-bucket/my-data.parquet'\n</code></pre>"},{"location":"features/inline-paths/#multi-file-queries","title":"Multi-File Queries","text":"<p>You can join data from different locations and formats in a single query.</p> <pre><code>SELECT \n    local.id, \n    remote.value \nFROM 'local_data.csv' local\nJOIN 's3://bucket/remote_data.parquet' remote\nON local.id = remote.id\n</code></pre>"},{"location":"features/joins/","title":"JOIN Operations","text":"<p>SQLStream supports joining data from multiple files, allowing you to combine datasets based on common columns. This is particularly powerful as it allows you to treat separate files (CSV, Parquet) as if they were tables in a relational database.</p>"},{"location":"features/joins/#syntax","title":"Syntax","text":"<p>The syntax follows standard SQL conventions. You can specify the files directly in the query using string literals.</p> <pre><code>SELECT \n    t1.col1, \n    t2.col2 \nFROM 'file1.csv' t1 \nJOIN 'file2.csv' t2 \nON t1.id = t2.id\n</code></pre>"},{"location":"features/joins/#supported-join-types","title":"Supported Join Types","text":"<p>SQLStream currently supports:</p> <ul> <li>INNER JOIN: Returns records that have matching values in both tables.</li> <li>LEFT JOIN: Returns all records from the left table, and the matched records from the right table.</li> </ul>"},{"location":"features/joins/#cross-format-joins","title":"Cross-Format Joins","text":"<p>You can join files of different formats. For example, you can join a CSV file with a Parquet file:</p> <pre><code>SELECT \n    users.name, \n    orders.amount \nFROM 'users.csv' users \nJOIN 'orders.parquet' orders \nON users.user_id = orders.user_id\n</code></pre>"},{"location":"features/joins/#performance-considerations","title":"Performance Considerations","text":"<ul> <li>Backend: Using the <code>pandas</code> backend is generally faster for joins on larger datasets as it leverages optimized merge algorithms.</li> <li>Memory: The Python backend uses a nested-loop or hash join implementation which streams data, making it memory efficient but potentially slower for large datasets compared to Pandas.</li> </ul>"},{"location":"features/pandas-backend/","title":"Pandas Backend","text":"<p>SQLStream includes a high-performance execution backend powered by pandas. This backend is designed for scenarios where performance is critical and the dataset fits into memory.</p>"},{"location":"features/pandas-backend/#enabling-the-pandas-backend","title":"Enabling the Pandas Backend","text":"<p>You can specify the backend when executing a query:</p>"},{"location":"features/pandas-backend/#python-api","title":"Python API","text":"<pre><code>from sqlstream import query\n\n# Force pandas backend\nresult = query(\"data.csv\").sql(\"SELECT * FROM data\", backend=\"pandas\")\n\n# Auto mode (default) - uses pandas if available\nresult = query(\"data.csv\").sql(\"SELECT * FROM data\", backend=\"auto\")\n</code></pre>"},{"location":"features/pandas-backend/#cli","title":"CLI","text":"<p>The CLI automatically attempts to use the pandas backend if pandas is installed.</p>"},{"location":"features/pandas-backend/#benefits","title":"Benefits","text":"<ol> <li>Vectorized Execution: Operations are performed on entire arrays at once rather than row-by-row, leading to significant speedups.</li> <li>Optimized Joins: Leverages pandas' highly optimized merge algorithms.</li> <li>Efficient Aggregations: Grouping and aggregation are much faster.</li> </ol>"},{"location":"features/pandas-backend/#fallback-mechanism","title":"Fallback Mechanism","text":"<p>When <code>backend=\"auto\"</code> is used (the default), SQLStream checks if <code>pandas</code> is installed. - If installed: It uses the PandasExecutor. - If not installed: It falls back to the pure Python VolcanoExecutor.</p> <p>This ensures that SQLStream remains lightweight and functional even without heavy dependencies, while offering performance when they are available.</p>"},{"location":"features/pandas-backend/#limitations","title":"Limitations","text":"<ul> <li>Memory Usage: The pandas backend loads data into memory (DataFrames). For datasets larger than available RAM, the streaming Python backend might be more appropriate (though slower).</li> </ul>"},{"location":"features/s3-support/","title":"S3 Support","text":"<p>SQLStream can read CSV and Parquet files directly from Amazon S3 buckets, enabling you to query cloud-stored data without downloading files locally.</p>"},{"location":"features/s3-support/#installation","title":"Installation","text":"<p>S3 support requires the <code>s3fs</code> library:</p> <pre><code># Install with S3 support\npip install \"sqlstream[s3]\"\n\n# Or install all features\npip install \"sqlstream[all]\"\n</code></pre>"},{"location":"features/s3-support/#authentication","title":"Authentication","text":"<p>SQLStream uses your AWS credentials through <code>s3fs</code>. Configure credentials using any of these methods:</p>"},{"location":"features/s3-support/#option-1-aws-credentials-file","title":"Option 1: AWS Credentials File","text":"<pre><code># ~/.aws/credentials\n[default]\naws_access_key_id = YOUR_ACCESS_KEY\naws_secret_access_key = YOUR_SECRET_KEY\n</code></pre>"},{"location":"features/s3-support/#option-2-environment-variables","title":"Option 2: Environment Variables","text":"<pre><code>export AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY\nexport AWS_SECRET_ACCESS_KEY=YOUR_SECRET_KEY\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre>"},{"location":"features/s3-support/#option-3-iam-roles","title":"Option 3: IAM Roles","text":"<p>When running on EC2 or ECS, SQLStream automatically uses IAM role credentials.</p>"},{"location":"features/s3-support/#basic-usage","title":"Basic Usage","text":""},{"location":"features/s3-support/#cli-usage","title":"CLI Usage","text":"<p>Query S3 files using <code>s3://</code> URLs:</p> <pre><code># CSV files\nsqlstream query \"SELECT * FROM 's3://my-bucket/data.csv' WHERE age &gt; 25\"\n\n# Parquet files\nsqlstream query \"SELECT * FROM 's3://my-bucket/data.parquet' LIMIT 100\"\n\n# With output formatting\nsqlstream query \"SELECT * FROM 's3://my-bucket/sales.csv'\" --format json\n\n# Using pandas backend for performance\nsqlstream query \"SELECT * FROM 's3://my-bucket/large.parquet'\" --backend pandas\n</code></pre>"},{"location":"features/s3-support/#python-api","title":"Python API","text":"<pre><code>from sqlstream import query\n\n# Query S3 CSV\nresults = query(\"s3://my-bucket/employees.csv\").sql(\"\"\"\n    SELECT name, salary\n    FROM data\n    WHERE department = 'Engineering'\n    ORDER BY salary DESC\n\"\"\")\n\nfor row in results:\n    print(row)\n</code></pre>"},{"location":"features/s3-support/#interactive-shell","title":"Interactive Shell","text":"<pre><code># Launch shell and query S3\nsqlstream shell\n\n# Then run queries\nSELECT * FROM 's3://my-bucket/data.csv' WHERE date &gt; '2024-01-01'\n</code></pre>"},{"location":"features/s3-support/#advanced-examples","title":"Advanced Examples","text":""},{"location":"features/s3-support/#example-1-aggregations-on-s3-data","title":"Example 1: Aggregations on S3 Data","text":"<pre><code>from sqlstream import query\n\n# Sales analysis from S3\nresults = query(\"s3://analytics-bucket/sales-2024.parquet\").sql(\"\"\"\n    SELECT\n        product_category,\n        COUNT(*) as num_sales,\n        SUM(amount) as total_revenue,\n        AVG(amount) as avg_sale\n    FROM data\n    WHERE sale_date &gt;= '2024-01-01'\n    GROUP BY product_category\n    ORDER BY total_revenue DESC\n\"\"\", backend=\"pandas\")\n\nfor row in results:\n    print(f\"{row['product_category']}: ${row['total_revenue']:,.2f}\")\n</code></pre>"},{"location":"features/s3-support/#example-2-join-s3-and-local-files","title":"Example 2: JOIN S3 and Local Files","text":"<pre><code>from sqlstream.core.query import QueryInline\n\nq = QueryInline()\n\n# Join S3 data with local reference data\nresults = q.sql(\"\"\"\n    SELECT\n        s.customer_id,\n        s.order_total,\n        c.customer_name,\n        c.region\n    FROM 's3://orders-bucket/orders.parquet' s\n    JOIN 'customers.csv' c ON s.customer_id = c.id\n    WHERE s.order_date = '2024-11-30'\n\"\"\")\n\nfor row in results:\n    print(row)\n</code></pre>"},{"location":"features/s3-support/#performance-tips","title":"Performance Tips","text":""},{"location":"features/s3-support/#1-use-parquet-for-large-datasets","title":"1. Use Parquet for Large Datasets","text":"<p>Parquet files offer: - Faster queries (columnar format, only read needed columns) - Smaller size (better compression than CSV) - Row group pruning (skip irrelevant data blocks)</p>"},{"location":"features/s3-support/#2-leverage-column-pruning","title":"2. Leverage Column Pruning","text":"<pre><code># \u2705 GOOD: Select specific columns\nSELECT name, email FROM data\n\n# \u274c SLOW: Select all columns\nSELECT * FROM data\n</code></pre>"},{"location":"features/s3-support/#3-use-pandas-backend-for-aggregations","title":"3. Use Pandas Backend for Aggregations","text":"<pre><code>results = query(\"s3://bucket/sales.parquet\").sql(\"\"\"\n    SELECT region, SUM(revenue) as total\n    FROM data\n    GROUP BY region\n\"\"\", backend=\"pandas\")  # 10-100x faster!\n</code></pre>"},{"location":"features/s3-support/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/s3-support/#missing-s3fs-package","title":"Missing s3fs Package","text":"<pre><code>ImportError: s3fs is required for S3 support. Install with: pip install sqlstream[s3]\n</code></pre> <p>Solution: <code>pip install \"sqlstream[s3]\"</code></p>"},{"location":"features/s3-support/#access-denied","title":"Access Denied","text":"<p>Ensure your AWS credentials have <code>s3:GetObject</code> permission for the bucket.</p>"},{"location":"features/s3-support/#no-credentials-configured","title":"No Credentials Configured","text":"<p>Configure AWS credentials using one of the methods described in the Authentication section.</p>"},{"location":"features/s3-support/#next-steps","title":"Next Steps","text":"<ul> <li>Interactive Shell - Query S3 interactively</li> <li>Data Sources - Learn about other supported formats</li> <li>Performance - Optimize your queries</li> </ul>"},{"location":"features/sql-support/","title":"SQL Support","text":"<p>SQLStream supports a practical subset of SQL designed for data exploration and ETL tasks.</p> <p>Backend-Specific Support</p> <p>The features listed below are supported by Python and Pandas backends. For advanced SQL features (window functions, CTEs, subqueries, HAVING, etc.), use the DuckDB backend which provides full SQL support. See DuckDB Backend Guide for details.</p>"},{"location":"features/sql-support/#supported-syntax","title":"Supported Syntax","text":""},{"location":"features/sql-support/#select","title":"SELECT","text":"<pre><code>-- Select all columns\nSELECT * FROM data\n\n-- Select specific columns\nSELECT name, age, city FROM data\n\n-- With table alias\nSELECT d.name, d.age FROM data d\n</code></pre>"},{"location":"features/sql-support/#where","title":"WHERE","text":"<pre><code>-- Simple conditions\nSELECT * FROM data WHERE age &gt; 25\nSELECT * FROM data WHERE name = 'Alice'\nSELECT * FROM data WHERE salary &gt;= 80000\n\n-- Multiple conditions with AND\nSELECT * FROM data WHERE age &gt; 25 AND city = 'NYC'\nSELECT * FROM data WHERE salary &gt; 80000 AND department = 'Engineering'\n</code></pre> <p>Supported operators: <code>=</code>, <code>&gt;</code>, <code>&lt;</code>, <code>&gt;=</code>, <code>&lt;=</code>, <code>!=</code>, <code>&lt;&gt;</code></p>"},{"location":"features/sql-support/#group-by","title":"GROUP BY","text":"<pre><code>-- Simple grouping\nSELECT city, COUNT(*) FROM data GROUP BY city\n\n-- Multiple columns\nSELECT department, city, AVG(salary) FROM data GROUP BY department, city\n\n-- With WHERE\nSELECT city, COUNT(*) FROM data WHERE age &gt; 25 GROUP BY city\n</code></pre>"},{"location":"features/sql-support/#aggregate-functions","title":"Aggregate Functions","text":"<pre><code>SELECT COUNT(*) FROM data\nSELECT COUNT(id) FROM data\nSELECT SUM(salary) FROM data\nSELECT AVG(age) FROM data\nSELECT MIN(salary) FROM data\nSELECT MAX(salary) FROM data\n</code></pre> <p>With aliases: <pre><code>SELECT department, COUNT(*) AS employee_count, AVG(salary) AS avg_salary\nFROM data\nGROUP BY department\n</code></pre></p>"},{"location":"features/sql-support/#join","title":"JOIN","text":"<pre><code>-- INNER JOIN\nSELECT * FROM employees e\nINNER JOIN departments d ON e.dept_id = d.id\n\n-- LEFT JOIN\nSELECT * FROM employees e\nLEFT JOIN departments d ON e.dept_id = d.id\n\n-- RIGHT JOIN\nSELECT * FROM employees e\nRIGHT JOIN departments d ON e.dept_id = d.id\n</code></pre>"},{"location":"features/sql-support/#order-by","title":"ORDER BY","text":"<pre><code>-- Ascending (default)\nSELECT * FROM data ORDER BY age\nSELECT * FROM data ORDER BY age ASC\n\n-- Descending\nSELECT * FROM data ORDER BY salary DESC\n\n-- Multiple columns\nSELECT * FROM data ORDER BY city ASC, age DESC\n</code></pre>"},{"location":"features/sql-support/#limit","title":"LIMIT","text":"<pre><code>-- Top 10 rows\nSELECT * FROM data LIMIT 10\n\n-- With ORDER BY\nSELECT * FROM data ORDER BY salary DESC LIMIT 5\n</code></pre>"},{"location":"features/sql-support/#complete-example","title":"Complete Example","text":"<pre><code>SELECT\n    department,\n    COUNT(*) AS employee_count,\n    AVG(salary) AS avg_salary,\n    MIN(salary) AS min_salary,\n    MAX(salary) AS max_salary\nFROM employees\nWHERE hire_date &gt; '2020-01-01'\n  AND status = 'active'\nGROUP BY department\nORDER BY avg_salary DESC\nLIMIT 10\n</code></pre>"},{"location":"features/sql-support/#inline-file-paths-phase-76","title":"Inline File Paths (Phase 7.6)","text":"<pre><code># Single file\nsqlstream query \"SELECT * FROM 'data.csv' WHERE age &gt; 25\"\n\n# Multiple files with JOIN\nsqlstream query \"SELECT c.name, o.total FROM 'customers.csv' c JOIN 'orders.csv' o ON c.id = o.customer_id\"\n\n# Quoted paths (for spaces)\nsqlstream query \"SELECT * FROM '/path/with spaces/data.csv'\"\n</code></pre>"},{"location":"features/sql-support/#advanced-features-duckdb-backend-only","title":"Advanced Features (DuckDB Backend Only)","text":"<p>The following SQL features are not supported by Python/Pandas backends but are available with DuckDB backend:</p> <ul> <li>\u2705 Subqueries (with DuckDB) - In FROM, WHERE, and SELECT clauses</li> <li>\u2705 UNION/INTERSECT/EXCEPT (with DuckDB) - Set operations</li> <li>\u2705 HAVING clause (with DuckDB) - Filter aggregated results</li> <li>\u2705 CASE expressions (with DuckDB) - Conditional logic</li> <li>\u2705 String functions (with DuckDB) - UPPER, LOWER, SUBSTRING, CONCAT, etc.</li> <li>\u2705 Date functions (with DuckDB) - EXTRACT, DATE_DIFF, DATE_TRUNC, etc.</li> <li>\u2705 Window functions (with DuckDB) - ROW_NUMBER, RANK, LAG, LEAD, etc.</li> <li>\u2705 Common Table Expressions (with DuckDB) - WITH clause</li> </ul> <p>To use these features, specify <code>backend=\"duckdb\"</code>:</p> <pre><code>from sqlstream import query\n\n# Example with window function\nresults = query().sql(\"\"\"\n    SELECT \n        name,\n        salary,\n        ROW_NUMBER() OVER (ORDER BY salary DESC) as rank\n    FROM 'employees.csv'\n\"\"\", backend=\"duckdb\")\n</code></pre> <p>See DuckDB Backend Guide for comprehensive examples and documentation.</p>"},{"location":"features/sql-support/#next-steps","title":"Next Steps","text":"<ul> <li>JOIN Examples</li> <li>Aggregation Examples</li> <li>Inline File Paths</li> </ul>"},{"location":"features/type-system/","title":"Type System &amp; Schema Inference","text":"<p>SQLStream includes a robust type system that automatically infers data types from your files and validates operations.</p>"},{"location":"features/type-system/#supported-types","title":"Supported Types","text":"<p>SQLStream supports six core data types:</p> Type Python Types Example Values <code>INTEGER</code> <code>int</code> <code>42</code>, <code>-100</code>, <code>0</code> <code>FLOAT</code> <code>float</code> <code>3.14</code>, <code>-2.5</code>, <code>0.0</code> <code>STRING</code> <code>str</code> <code>\"hello\"</code>, <code>\"Alice\"</code> <code>BOOLEAN</code> <code>bool</code> <code>true</code>, <code>false</code> <code>DATE</code> <code>date</code>, <code>datetime</code> <code>2024-01-15</code>, <code>2023-12-31T12:34:56.789</code> <code>NULL</code> <code>None</code> Empty values"},{"location":"features/type-system/#automatic-type-inference","title":"Automatic Type Inference","text":"<p>SQLStream automatically infers types from your data:</p>"},{"location":"features/type-system/#from-python-values","title":"From Python Values","text":"<pre><code>from sqlstream.core.types import infer_type, DataType\n\ninfer_type(42)           # DataType.INTEGER\ninfer_type(3.14)         # DataType.FLOAT\ninfer_type(\"hello\")      # DataType.STRING\ninfer_type(True)         # DataType.BOOLEAN\ninfer_type(None)         # DataType.NULL\n</code></pre>"},{"location":"features/type-system/#from-string-values","title":"From String Values","text":"<p>When reading CSV files, SQLStream tries to infer the most specific type:</p> <pre><code>infer_type(\"42\")         # DataType.INTEGER (not STRING)\ninfer_type(\"3.14\")       # DataType.FLOAT\ninfer_type(\"true\")       # DataType.BOOLEAN\ninfer_type(\"2024-01-15\") # DataType.DATE\ninfer_type(\"hello\")      # DataType.STRING\n</code></pre>"},{"location":"features/type-system/#handling-mixed-types","title":"Handling Mixed Types","text":"<p>When a column has mixed types, SQLStream promotes to the most general compatible type:</p> <pre><code>from sqlstream.core.types import infer_common_type\n\n# Mixed integers and floats -&gt; FLOAT\ninfer_common_type([1, 2.5, 3])           # DataType.FLOAT\n\n# Mixed types -&gt; STRING\ninfer_common_type([1, \"hello\", 3])       # DataType.STRING\n\n# NULL values are ignored\ninfer_common_type([1, None, 3])          # DataType.INTEGER\n</code></pre>"},{"location":"features/type-system/#schema-inference","title":"Schema Inference","text":"<p>SQLStream automatically infers the schema (column names and types) when reading files.</p>"},{"location":"features/type-system/#basic-usage","title":"Basic Usage","text":"<pre><code>from sqlstream import query\n\n# Create query object\nq = query(\"employees.csv\")\n\n# Get inferred schema\nschema = q.schema()\n\n# Check column types\nprint(schema[\"name\"])    # DataType.STRING\nprint(schema[\"age\"])     # DataType.INTEGER\nprint(schema[\"salary\"])  # DataType.FLOAT\n</code></pre>"},{"location":"features/type-system/#schema-object","title":"Schema Object","text":"<p>The <code>Schema</code> object provides helpful methods:</p> <pre><code># Get all column names\ncolumns = schema.get_column_names()\n# ['name', 'age', 'salary', 'hire_date']\n\n# Get type of a column\nage_type = schema.get_column_type(\"age\")\n# DataType.INTEGER\n\n# Check if column exists\nif \"email\" in schema:\n    print(\"Email column exists\")\n\n# Validate column\ntry:\n    schema.validate_column(\"invalid_column\")\nexcept ValueError as e:\n    print(e)  # Column 'invalid_column' not found\n</code></pre>"},{"location":"features/type-system/#sample-size","title":"Sample Size","text":"<p>By default, SQLStream samples 100 rows to infer types. You can adjust this:</p> <pre><code>from sqlstream.readers.csv_reader import CSVReader\n\nreader = CSVReader(\"large_file.csv\")\n\n# Sample only 10 rows (faster)\nschema = reader.get_schema(sample_size=10)\n\n# Sample 1000 rows (more accurate)\nschema = reader.get_schema(sample_size=1000)\n</code></pre>"},{"location":"features/type-system/#type-checking","title":"Type Checking","text":""},{"location":"features/type-system/#numeric-types","title":"Numeric Types","text":"<p>Check if a type is numeric:</p> <pre><code>DataType.INTEGER.is_numeric()  # True\nDataType.FLOAT.is_numeric()    # True\nDataType.STRING.is_numeric()   # False\n</code></pre>"},{"location":"features/type-system/#type-compatibility","title":"Type Compatibility","text":"<p>Check if two types can be compared:</p> <pre><code># Same types are compatible\nDataType.INTEGER.is_comparable(DataType.INTEGER)  # True\n\n# Numeric types are compatible\nDataType.INTEGER.is_comparable(DataType.FLOAT)    # True\n\n# String and number are not compatible\nDataType.STRING.is_comparable(DataType.INTEGER)   # False\n\n# NULL is compatible with everything\nDataType.NULL.is_comparable(DataType.STRING)      # True\n</code></pre>"},{"location":"features/type-system/#type-coercion","title":"Type Coercion","text":"<p>When mixing types, SQLStream promotes to the more general type:</p> <pre><code># INT + FLOAT -&gt; FLOAT\nDataType.INTEGER.coerce_to(DataType.FLOAT)  # DataType.FLOAT\n\n# NULL + anything -&gt; that type\nDataType.NULL.coerce_to(DataType.INTEGER)   # DataType.INTEGER\n\n# Incompatible types -&gt; STRING\nDataType.INTEGER.coerce_to(DataType.STRING) # DataType.STRING\n</code></pre>"},{"location":"features/type-system/#practical-examples","title":"Practical Examples","text":""},{"location":"features/type-system/#example-1-validate-query-columns","title":"Example 1: Validate Query Columns","text":"<pre><code>from sqlstream import query\n\nq = query(\"employees.csv\")\nschema = q.schema()\n\n# Validate SELECT columns before executing\nselect_cols = [\"name\", \"age\", \"salary\"]\nfor col in select_cols:\n    try:\n        schema.validate_column(col)\n    except ValueError:\n        print(f\"Column '{col}' doesn't exist!\")\n</code></pre>"},{"location":"features/type-system/#example-2-check-column-types","title":"Example 2: Check Column Types","text":"<pre><code>from sqlstream import query\n\nq = query(\"sales.csv\")\nschema = q.schema()\n\n# Find all numeric columns\nnumeric_cols = [\n    col for col in schema.get_column_names()\n    if schema[col].is_numeric()\n]\nprint(f\"Numeric columns: {numeric_cols}\")\n</code></pre>"},{"location":"features/type-system/#example-3-type-safe-filtering","title":"Example 3: Type-Safe Filtering","text":"<pre><code>from sqlstream import query\nfrom sqlstream.core.types import DataType\n\nq = query(\"products.csv\")\nschema = q.schema()\n\n# Only filter on numeric columns\nif schema[\"price\"].is_numeric():\n    results = q.sql(\"SELECT * FROM data WHERE price &gt; 100\")\nelse:\n    print(\"Price column is not numeric!\")\n</code></pre>"},{"location":"features/type-system/#schema-merging","title":"Schema Merging","text":"<p>When working with multiple files (e.g., in JOINs), SQLStream can merge schemas:</p> <pre><code>from sqlstream.core.types import Schema, DataType\n\n# Two schemas with overlapping columns\nschema1 = Schema({\n    \"id\": DataType.INTEGER,\n    \"value\": DataType.INTEGER\n})\n\nschema2 = Schema({\n    \"id\": DataType.INTEGER,\n    \"value\": DataType.FLOAT  # Different type!\n})\n\n# Merge schemas\nmerged = schema1.merge(schema2)\n\n# 'value' column is promoted to FLOAT\nprint(merged[\"value\"])  # DataType.FLOAT\n</code></pre>"},{"location":"features/type-system/#best-practices","title":"Best Practices","text":""},{"location":"features/type-system/#1-check-schema-before-querying","title":"1. Check Schema Before Querying","text":"<pre><code>schema = query(\"data.csv\").schema()\n\n# Verify expected columns exist\nrequired = [\"id\", \"name\", \"amount\"]\nfor col in required:\n    schema.validate_column(col)  # Raises error if missing\n</code></pre>"},{"location":"features/type-system/#2-use-type-information","title":"2. Use Type Information","text":"<pre><code>schema = query(\"data.csv\").schema()\n\n# Only perform numeric operations on numeric columns\nif schema[\"age\"].is_numeric():\n    results = query(\"data.csv\").sql(\"SELECT AVG(age) FROM data\")\n</code></pre>"},{"location":"features/type-system/#3-handle-null-values","title":"3. Handle NULL Values","text":"<pre><code>from sqlstream.core.types import DataType\n\nschema = query(\"data.csv\").schema()\n\n# Check if column might have nulls\nif schema[\"optional_field\"] == DataType.NULL:\n    print(\"This column is all nulls!\")\n</code></pre>"},{"location":"features/type-system/#4-sample-size-tradeoff","title":"4. Sample Size Tradeoff","text":"<pre><code>from sqlstream.readers.csv_reader import CSVReader\n\n# Small sample (fast, less accurate)\nschema = CSVReader(\"file.csv\").get_schema(sample_size=10)\n\n# Large sample (slower, more accurate)\nschema = CSVReader(\"file.csv\").get_schema(sample_size=1000)\n</code></pre>"},{"location":"features/type-system/#type-system-api-reference","title":"Type System API Reference","text":""},{"location":"features/type-system/#datatype-methods","title":"DataType Methods","text":"<ul> <li><code>is_numeric()</code> - Check if type is INTEGER or FLOAT</li> <li><code>is_comparable(other)</code> - Check if compatible for comparison</li> <li><code>coerce_to(other)</code> - Determine result type of coercion</li> </ul>"},{"location":"features/type-system/#schema-methods","title":"Schema Methods","text":"<ul> <li><code>get_column_names()</code> - Get list of column names</li> <li><code>get_column_type(column)</code> - Get type of column (or None)</li> <li><code>validate_column(column)</code> - Raise error if column doesn't exist</li> <li><code>merge(other)</code> - Merge two schemas with type coercion</li> <li><code>from_row(row)</code> - Create schema from single row</li> <li><code>from_rows(rows)</code> - Create schema from multiple rows (more accurate)</li> </ul>"},{"location":"features/type-system/#type-inference-functions","title":"Type Inference Functions","text":"<ul> <li><code>infer_type(value)</code> - Infer type from Python value</li> <li><code>infer_common_type(values)</code> - Infer common type from list of values</li> </ul>"},{"location":"features/type-system/#next-steps","title":"Next Steps","text":"<ul> <li>SQL Support - See what SQL features use the type system</li> <li>Data Sources - Learn about different file formats</li> <li>Python API - Use the type system programmatically</li> </ul>"},{"location":"getting-started/core-concepts/","title":"Core Concepts","text":"<p>Understanding the key concepts behind SQLStream will help you use it more effectively.</p>"},{"location":"getting-started/core-concepts/#architecture-overview","title":"Architecture Overview","text":"<p>SQLStream is built around these core components:</p> <pre><code>graph LR\n    A[SQL Query] --&gt; B[Parser]\n    B --&gt; C[AST]\n    C --&gt; D[Planner]\n    D --&gt; E[Optimized Plan]\n    E --&gt; F[Executor]\n    F --&gt; G[Operators]\n    G --&gt; H[Readers]\n    H --&gt; I[Data Sources]\n</code></pre>"},{"location":"getting-started/core-concepts/#the-volcano-model","title":"The Volcano Model","text":"<p>SQLStream uses the Volcano iterator model for query execution:</p>"},{"location":"getting-started/core-concepts/#what-is-it","title":"What is it?","text":"<p>Each operator in the query plan implements two methods:</p> <ul> <li><code>open()</code>: Initialize the operator</li> <li><code>next()</code>: Return the next row (or None when done)</li> </ul>"},{"location":"getting-started/core-concepts/#why-volcano","title":"Why Volcano?","text":"<p>\u2705 Lazy Evaluation: Rows are produced on-demand \u2705 Low Memory: Only one row in memory at a time \u2705 Composable: Operators stack like LEGO blocks \u2705 Predictable: Easy to understand and debug</p>"},{"location":"getting-started/core-concepts/#example","title":"Example","text":"<pre><code>SELECT name FROM employees WHERE salary &gt; 80000 LIMIT 5\n</code></pre> <p>Execution flow:</p> <pre><code>Limit(5)\n  \u2193 next()\nProject(name)\n  \u2193 next()\nFilter(salary &gt; 80000)\n  \u2193 next()\nScan(employees.csv)\n  \u2193 next()\nCSV Reader\n</code></pre> <p>Each operator calls <code>next()</code> on the operator below it until it gets a row.</p>"},{"location":"getting-started/core-concepts/#execution-backends","title":"Execution Backends","text":"<p>SQLStream offers two execution backends:</p>"},{"location":"getting-started/core-concepts/#python-backend","title":"Python Backend","text":"<p>How it works: Pure Python implementation using the Volcano model</p> <p>Pros:</p> <ul> <li>\u2705 No dependencies</li> <li>\u2705 Easy to understand</li> <li>\u2705 Works everywhere</li> </ul> <p>Cons:</p> <ul> <li>\u274c Slower for large datasets</li> <li>\u274c Row-at-a-time processing</li> </ul> <p>Best for:</p> <ul> <li>Learning and education</li> <li>Small files (&lt;100K rows)</li> <li>Quick prototyping</li> </ul>"},{"location":"getting-started/core-concepts/#pandas-backend","title":"Pandas Backend","text":"<p>How it works: Translates SQL to pandas operations</p> <p>Pros:</p> <ul> <li>\u2705 10-100x faster than Python backend</li> <li>\u2705 Vectorized operations</li> <li>\u2705 Optimized C code</li> </ul> <p>Cons:</p> <ul> <li>\u274c Requires pandas dependency</li> <li>\u274c Loads full dataset into memory</li> </ul> <p>Best for:</p> <ul> <li>Production workloads</li> <li>Large files (&gt;100K rows)</li> <li>Performance-critical applications</li> </ul>"},{"location":"getting-started/core-concepts/#query-optimizations","title":"Query Optimizations","text":"<p>SQLStream applies several optimizations automatically:</p>"},{"location":"getting-started/core-concepts/#1-column-pruning","title":"1. Column Pruning","text":"<p>What: Only read columns that are actually used</p> <p>Example:</p> <pre><code>SELECT name FROM employees\n</code></pre> <p>SQLStream only reads the <code>name</code> column from the CSV, not all columns.</p> <p>Benefit: Faster I/O, less memory</p>"},{"location":"getting-started/core-concepts/#2-predicate-pushdown","title":"2. Predicate Pushdown","text":"<p>What: Apply filters as early as possible</p> <p>Example:</p> <pre><code>SELECT name FROM employees WHERE department = 'Engineering'\n</code></pre> <p>The filter is applied during the scan, not after loading all rows.</p> <p>Benefit: Fewer rows to process</p>"},{"location":"getting-started/core-concepts/#3-lazy-evaluation","title":"3. Lazy Evaluation","text":"<p>What: Only compute results when needed</p> <p>Example:</p> <pre><code>results = query(\"data.csv\").sql(\"SELECT * FROM data LIMIT 10\")\n# Nothing executed yet!\n\nfor row in results:\n    print(row)  # Now it executes, one row at a time\n</code></pre> <p>Benefit: Save computation for unused results</p>"},{"location":"getting-started/core-concepts/#data-sources","title":"Data Sources","text":"<p>SQLStream supports multiple data sources:</p>"},{"location":"getting-started/core-concepts/#csv-files","title":"CSV Files","text":"<pre><code>query(\"data.csv\")\n</code></pre> <ul> <li>Automatic delimiter detection</li> <li>Header row inference</li> <li>Type inference (strings, numbers)</li> </ul>"},{"location":"getting-started/core-concepts/#parquet-files","title":"Parquet Files","text":"<pre><code>query(\"data.parquet\")  # Requires: pip install \"sqlstream[parquet]\"\n</code></pre> <ul> <li>Columnar storage</li> <li>Better compression</li> <li>Schema included</li> </ul>"},{"location":"getting-started/core-concepts/#http-urls","title":"HTTP URLs","text":"<pre><code>query(\"https://example.com/data.csv\")  # Requires: pip install \"sqlstream[http]\"\n</code></pre> <ul> <li>Streaming support</li> <li>Automatic format detection</li> <li>Caching (planned)</li> </ul>"},{"location":"getting-started/core-concepts/#inline-paths-phase-76","title":"Inline Paths (Phase 7.6)","text":"<pre><code>sqlstream query \"SELECT * FROM 'data.csv'\"\n</code></pre> <ul> <li>No need to pre-specify file</li> <li>Multi-file queries</li> <li>More intuitive</li> </ul>"},{"location":"getting-started/core-concepts/#lazy-vs-eager-evaluation","title":"Lazy vs. Eager Evaluation","text":""},{"location":"getting-started/core-concepts/#lazy-default","title":"Lazy (Default)","text":"<pre><code>result = query(\"data.csv\").sql(\"SELECT * FROM data\")\n# \u2705 Nothing executed yet\n\nfor row in result:  # Executes one row at a time\n    print(row)\n</code></pre> <p>Advantages:</p> <ul> <li>Low memory usage</li> <li>Can process infinite streams</li> <li>Early termination possible</li> </ul>"},{"location":"getting-started/core-concepts/#eager","title":"Eager","text":"<pre><code>result = query(\"data.csv\").sql(\"SELECT * FROM data\").to_list()\n# \u274c Executes immediately, loads all data\n</code></pre> <p>Advantages:</p> <ul> <li>Random access to results</li> <li>Easier to work with</li> <li>Can get length with <code>len()</code></li> </ul> <p>Choose lazy when:</p> <ul> <li>Processing large files</li> <li>Only need first N results</li> <li>Streaming to another system</li> </ul> <p>Choose eager when:</p> <ul> <li>Results fit in memory</li> <li>Need to access results multiple times</li> <li>Using with pandas/numpy</li> </ul>"},{"location":"getting-started/core-concepts/#query-lifecycle","title":"Query Lifecycle","text":"<ol> <li>Parse: SQL string \u2192 AST (Abstract Syntax Tree)</li> <li>Plan: AST \u2192 Execution plan</li> <li>Optimize: Apply optimizations (column pruning, etc.)</li> <li>Execute: Build operator pipeline</li> <li>Iterate: Pull rows through the pipeline</li> </ol>"},{"location":"getting-started/core-concepts/#memory-model","title":"Memory Model","text":""},{"location":"getting-started/core-concepts/#python-backend_1","title":"Python Backend","text":"<pre><code>Memory Usage = O(1)  # One row at a time\n</code></pre> <p>Perfect for:</p> <ul> <li>Large files that don't fit in RAM</li> <li>Streaming applications</li> <li>Long-running processes</li> </ul>"},{"location":"getting-started/core-concepts/#pandas-backend_1","title":"Pandas Backend","text":"<pre><code>Memory Usage = O(n)  # Full dataset in memory\n</code></pre> <p>Perfect for:</p> <ul> <li>Files that fit in RAM</li> <li>Multiple passes over data</li> <li>Complex aggregations</li> </ul>"},{"location":"getting-started/core-concepts/#error-handling","title":"Error Handling","text":"<p>SQLStream provides helpful error messages:</p>"},{"location":"getting-started/core-concepts/#parse-errors","title":"Parse Errors","text":"<pre><code>SELECT * FORM data  # Typo: FORM instead of FROM\n</code></pre> <pre><code>Error: Expected 'FROM' but got 'FORM' at position 9\n</code></pre>"},{"location":"getting-started/core-concepts/#file-not-found","title":"File Not Found","text":"<pre><code>query(\"missing.csv\")\n</code></pre> <pre><code>Error: File not found - missing.csv\n</code></pre>"},{"location":"getting-started/core-concepts/#type-errors","title":"Type Errors","text":"<pre><code>SELECT * FROM data WHERE age &gt; 'thirty'  # Comparing number to string\n</code></pre> <pre><code>Error: Cannot compare age (int) with 'thirty' (str)\n</code></pre>"},{"location":"getting-started/core-concepts/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/core-concepts/#1-use-column-names","title":"1. Use Column Names","text":"<p>\u2705 Good: <pre><code>SELECT name, age FROM employees\n</code></pre></p> <p>\u274c Bad: <pre><code>SELECT * FROM employees\n</code></pre></p>"},{"location":"getting-started/core-concepts/#2-add-where-clauses","title":"2. Add WHERE Clauses","text":"<p>\u2705 Good: <pre><code>SELECT * FROM logs WHERE date = '2024-01-01'\n</code></pre></p> <p>\u274c Bad: <pre><code>SELECT * FROM logs  # Processes all rows\n</code></pre></p>"},{"location":"getting-started/core-concepts/#3-choose-the-right-backend","title":"3. Choose the Right Backend","text":"<pre><code># Small files (&lt;100K rows)\nquery(\"data.csv\").sql(\"SELECT * FROM data\", backend=\"python\")\n\n# Large files (&gt;100K rows)\nquery(\"data.csv\").sql(\"SELECT * FROM data\", backend=\"pandas\")\n</code></pre>"},{"location":"getting-started/core-concepts/#4-limit-results-early","title":"4. Limit Results Early","text":"<p>\u2705 Good: <pre><code>SELECT * FROM data WHERE active = true LIMIT 100\n</code></pre></p> <p>\u274c Bad: <pre><code>results = query(\"data.csv\").sql(\"SELECT * FROM data\").to_list()[:100]\n</code></pre></p>"},{"location":"getting-started/core-concepts/#5-use-lazy-evaluation","title":"5. Use Lazy Evaluation","text":"<p>\u2705 Good: <pre><code>for row in query(\"large.csv\").sql(\"SELECT * FROM large\"):\n    process(row)  # One row at a time\n</code></pre></p> <p>\u274c Bad: <pre><code>rows = query(\"large.csv\").sql(\"SELECT * FROM large\").to_list()\nfor row in rows:  # All rows in memory!\n    process(row)\n</code></pre></p>"},{"location":"getting-started/core-concepts/#next-steps","title":"Next Steps","text":"<ul> <li>SQL Support - Learn supported SQL syntax</li> <li>Pandas Backend - Deep dive into performance</li> <li>Architecture - Understand the internals</li> <li>Optimizations - How optimizations work</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>SQLStream offers multiple installation options depending on your needs.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.8 or higher</li> <li>OS: Linux, macOS, Windows</li> </ul>"},{"location":"getting-started/installation/#installation-options","title":"Installation Options","text":""},{"location":"getting-started/installation/#basic-installation-csv-only","title":"Basic Installation (CSV only)","text":"<p>For querying CSV files only:</p> <pre><code>pip install sqlstream\n</code></pre> <p>This gives you:</p> <ul> <li>\u2705 Core SQL engine</li> <li>\u2705 CSV file support</li> <li>\u2705 Basic CLI commands</li> <li>\u274c No Parquet support</li> <li>\u274c No performance optimizations</li> </ul>"},{"location":"getting-started/installation/#with-parquet-support","title":"With Parquet Support","text":"<p>To query both CSV and Parquet files:</p> <pre><code>pip install \"sqlstream[parquet]\"\n</code></pre> <p>Additional features:</p> <ul> <li>\u2705 Everything from basic install</li> <li>\u2705 Parquet file support via PyArrow</li> <li>\u2705 Better compression and performance</li> </ul>"},{"location":"getting-started/installation/#with-pandas-backend-recommended","title":"With Pandas Backend (Recommended)","text":"<p>For 10-100x performance boost with large files:</p> <pre><code>pip install \"sqlstream[pandas]\"\n</code></pre> <p>Additional features:</p> <ul> <li>\u2705 Everything from basic install</li> <li>\u2705 Parquet support</li> <li>\u2705 Pandas-powered execution (10-100x faster)</li> <li>\u2705 Optimized for large datasets (&gt;100K rows)</li> </ul>"},{"location":"getting-started/installation/#with-http-support","title":"With HTTP Support","text":"<p>To query CSV/Parquet files from URLs:</p> <pre><code>pip install \"sqlstream[http]\"\n</code></pre> <p>Additional features:</p> <ul> <li>\u2705 Query files from HTTP/HTTPS URLs</li> <li>\u2705 Automatic format detection</li> <li>\u2705 Streaming support for large remote files</li> </ul>"},{"location":"getting-started/installation/#with-cli-features","title":"With CLI Features","text":"<p>For beautiful terminal output and interactive mode:</p> <pre><code>pip install \"sqlstream[cli]\"\n</code></pre> <p>Additional features:</p> <ul> <li>\u2705 Rich table formatting</li> <li>\u2705 Interactive scrollable table viewer</li> <li>\u2705 Syntax highlighting</li> <li>\u2705 Multiple output formats (JSON, CSV, table)</li> </ul>"},{"location":"getting-started/installation/#all-features","title":"All Features","text":"<p>To install everything:</p> <pre><code>pip install \"sqlstream[all]\"\n</code></pre> <p>This includes:</p> <ul> <li>\u2705 CSV and Parquet support</li> <li>\u2705 Pandas backend</li> <li>\u2705 HTTP data sources</li> <li>\u2705 Full CLI with interactive mode</li> <li>\u2705 All output formats</li> </ul>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing or development:</p> <pre><code># Clone the repository\ngit clone https://github.com/subhayu99/sqlstream.git\ncd sqlstream\n\n# Install in development mode with all dev dependencies\npip install -e \".[dev]\"\n</code></pre> <p>This includes:</p> <ul> <li>Testing: pytest, pytest-cov</li> <li>Linting: ruff</li> <li>Type checking: mypy</li> <li>Documentation: mkdocs, mkdocs-material</li> </ul>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>After installation, verify SQLStream is working:</p> <pre><code># Check version\nsqlstream --version\n\n# Run a quick query (requires a CSV file)\necho \"name,age\\nAlice,30\\nBob,25\" &gt; test.csv\nsqlstream query test.csv \"SELECT * FROM test\"\n</code></pre> <p>Expected output:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name  \u2502 age \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Alice \u2502  30 \u2502\n\u2502 Bob   \u2502  25 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2518\n\n2 rows\n</code></pre>"},{"location":"getting-started/installation/#upgrading","title":"Upgrading","text":"<p>To upgrade to the latest version:</p> <pre><code>pip install --upgrade sqlstream\n</code></pre> <p>To upgrade with all features:</p> <pre><code>pip install --upgrade \"sqlstream[all]\"\n</code></pre>"},{"location":"getting-started/installation/#uninstalling","title":"Uninstalling","text":"<p>To remove SQLStream:</p> <pre><code>pip uninstall sqlstream\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>If you see <code>ModuleNotFoundError</code>:</p> <pre><code># Reinstall with verbose output\npip install --verbose \"sqlstream[all]\"\n</code></pre>"},{"location":"getting-started/installation/#pandas-not-found","title":"Pandas Not Found","text":"<p>If you get \"pandas backend requested but pandas is not installed\":</p> <pre><code>pip install \"sqlstream[pandas]\"\n</code></pre>"},{"location":"getting-started/installation/#cli-not-working","title":"CLI Not Working","text":"<p>If <code>sqlstream</code> command is not found:</p> <pre><code># Check if it's in your PATH\nwhich sqlstream\n\n# Try running as module\npython -m sqlstream.cli.main --help\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get started in 5 minutes</li> <li>Core Concepts - Understand the basics</li> <li>SQL Support - Learn supported SQL syntax</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get up and running with SQLStream in 5 minutes!</p>"},{"location":"getting-started/quickstart/#step-1-install-sqlstream","title":"Step 1: Install SQLStream","text":"<pre><code>pip install \"sqlstream[all]\"\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-create-sample-data","title":"Step 2: Create Sample Data","text":"<p>Create a sample CSV file:</p> <pre><code>cat &gt; employees.csv &lt;&lt; EOF\nid,name,department,salary,hire_date\n1,Alice,Engineering,95000,2020-01-15\n2,Bob,Sales,75000,2019-06-01\n3,Charlie,Engineering,105000,2018-03-20\n4,Diana,Marketing,68000,2021-02-14\n5,Eve,Sales,82000,2020-09-10\nEOF\n</code></pre>"},{"location":"getting-started/quickstart/#step-3-your-first-query","title":"Step 3: Your First Query","text":""},{"location":"getting-started/quickstart/#cli-usage","title":"CLI Usage","text":"<pre><code># Select all rows\n$ sqlstream query employees.csv \"SELECT * FROM employees\"\n\n# Filter by department\n$ sqlstream query employees.csv \"SELECT name, salary FROM employees WHERE department = 'Engineering'\"\n\n# Sort by salary\n$ sqlstream query employees.csv \"SELECT * FROM employees ORDER BY salary DESC LIMIT 3\"\n\n# Query with inline file path (no source argument needed)\n$ sqlstream query \"SELECT * FROM 'employees.csv' WHERE salary &gt; 80000\"\n</code></pre>"},{"location":"getting-started/quickstart/#python-api","title":"Python API","text":"<pre><code>from sqlstream import query\n\n# Simple query with explicit source\nresults = query(\"employees.csv\").sql(\"SELECT * FROM employees WHERE salary &gt; 80000\")\n\n# Query with inline source (extracted from SQL)\nresults = query().sql(\"SELECT * FROM 'employees.csv' WHERE salary &gt; 80000\")\n\n# Print results\nfor row in results:\n    print(f\"{row['name']}: ${row['salary']:,}\")\n</code></pre> <p>Output: <pre><code>Alice: $95,000\nCharlie: $105,000\nEve: $82,000\n</code></pre></p>"},{"location":"getting-started/quickstart/#step-4-advanced-features","title":"Step 4: Advanced Features","text":""},{"location":"getting-started/quickstart/#aggregations","title":"Aggregations","text":"<pre><code># Count employees by department\n$ sqlstream query employees.csv \"SELECT department, COUNT(*) AS count FROM employees GROUP BY department\"\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 department  \u2502 count \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Engineering \u2502     2 \u2502\n\u2502 Sales       \u2502     2 \u2502\n\u2502 Marketing   \u2502     1 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"getting-started/quickstart/#joins","title":"Joins","text":"<p>Create another file for orders:</p> <pre><code>cat &gt; orders.csv &lt;&lt; EOF\norder_id,employee_id,amount\n101,1,1500\n102,2,2300\n103,1,1800\n104,3,2100\nEOF\n</code></pre> <p>Join the two files:</p> <pre><code>$ sqlstream query \"SELECT e.name, o.amount FROM 'employees.csv' e JOIN 'orders.csv' o ON e.id = o.employee_id\"\n</code></pre> <p>Output: <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 name    \u2502 amount \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Alice   \u2502   1500 \u2502\n\u2502 Bob     \u2502   2300 \u2502\n\u2502 Alice   \u2502   1800 \u2502\n\u2502 Charlie \u2502   2100 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p>"},{"location":"getting-started/quickstart/#output-formats","title":"Output Formats","text":"Table (default)JSONCSV <pre><code>$ sqlstream query employees.csv \"SELECT * FROM employees LIMIT 2\"\n</code></pre> <pre><code>\u250c\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 id \u2502 name  \u2502 department  \u2502 salary \u2502 hire_date  \u2502\n\u251c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  1 \u2502 Alice \u2502 Engineering \u2502  95000 \u2502 2020-01-15 \u2502\n\u2502  2 \u2502 Bob   \u2502 Sales       \u2502  75000 \u2502 2019-06-01 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>$ sqlstream query employees.csv \"SELECT * FROM employees LIMIT 2\" --format json\n</code></pre> <pre><code>[\n  {\n    \"id\": 1,\n    \"name\": \"Alice\",\n    \"department\": \"Engineering\",\n    \"salary\": 95000,\n    \"hire_date\": \"2020-01-15\"\n  },\n  {\n    \"id\": 2,\n    \"name\": \"Bob\",\n    \"department\": \"Sales\",\n    \"salary\": 75000,\n    \"hire_date\": \"2019-06-01\"\n  }\n]\n</code></pre> <pre><code>$ sqlstream query employees.csv \"SELECT * FROM employees LIMIT 2\" --format csv\n</code></pre> <pre><code>id,name,department,salary,hire_date\n1,Alice,Engineering,95000,2020-01-15\n2,Bob,Sales,75000,2019-06-01\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-performance-boost","title":"Step 5: Performance Boost","text":"<p>For large files, use the pandas backend:</p> <pre><code>$ sqlstream query large_file.csv \"SELECT * FROM large_file WHERE amount &gt; 1000\" --backend pandas\n</code></pre> <p>Performance comparison:</p> Rows Python Backend Pandas Backend Speedup 10K 0.5s 0.05s 10x 100K 5.2s 0.15s 35x 1M 52s 0.8s 65x <p>For complex SQL queries, use the DuckDB backend: <pre><code>$ sqlstream query \"SELECT * FROM 'data.parquet'\" --backend duckdb\n</code></pre></p>"},{"location":"getting-started/quickstart/#step-6-interactive-shell","title":"Step 6: Interactive Shell","text":"<p>For a full interactive experience, use the shell command:</p> <pre><code>$ sqlstream shell employees.csv\n</code></pre> <p>This launches a powerful TUI (Terminal User Interface) with:</p> <ul> <li>Query Editor: Multi-line editing with syntax highlighting and multiple tabs (<code>Ctrl+T</code> to add, <code>Ctrl+W</code> to close).</li> <li>Word Deletion: Use <code>Ctrl+Delete</code> and <code>Ctrl+Backspace</code> for fast editing.</li> <li>Results Viewer: Scrollable table with pagination.</li> <li>Sidebar: Toggle between Schema and Files browser (<code>F2</code> to toggle, <code>Ctrl+O</code> to open files).</li> <li>Backend Toggle: Press <code>F5</code> or <code>Ctrl+B</code> to cycle through backends (auto/duckdb/pandas/python).</li> <li>Save Progress: Save your work with <code>Ctrl+S</code>.</li> <li>Export: Press <code>Ctrl+X</code> to export results to CSV/JSON/Parquet.</li> <li>History: Use <code>Ctrl+Up</code>/<code>Down</code> to navigate previous queries.</li> <li>State Persistence: Your open tabs and queries are saved automatically on exit.</li> </ul>"},{"location":"getting-started/quickstart/#common-workflows","title":"Common Workflows","text":""},{"location":"getting-started/quickstart/#data-exploration","title":"Data Exploration","text":"<pre><code># Check file structure\n$ head -5 data.csv\n\n# Count rows\n$ sqlstream query data.csv \"SELECT COUNT(*) FROM data\"\n\n# Show unique values\n$ sqlstream query data.csv \"SELECT DISTINCT category FROM data\"\n\n# Summary statistics\n$ sqlstream query data.csv \"SELECT MIN(price), MAX(price), AVG(price) FROM data\"\n</code></pre>"},{"location":"getting-started/quickstart/#data-cleaning","title":"Data Cleaning","text":"<pre><code>from sqlstream import query\n\n# Remove duplicates and filter nulls\nresults = query(\"messy_data.csv\").sql(\"\"\"\n    SELECT DISTINCT *\n    FROM messy_data\n    WHERE name IS NOT NULL\n      AND age &gt; 0\n    ORDER BY id\n\"\"\")\n\n# Export cleaned data\nimport csv\nwith open(\"clean_data.csv\", \"w\") as f:\n    writer = csv.DictWriter(f, fieldnames=results.to_list()[0].keys())\n    writer.writeheader()\n    writer.writerows(results.to_list())\n</code></pre>"},{"location":"getting-started/quickstart/#etl-pipeline","title":"ETL Pipeline","text":"<pre><code>from sqlstream import query\n\n# Extract\ncustomers = query(\"customers.csv\")\norders = query(\"orders.csv\")\n\n# Transform: Calculate total orders per customer\nresult = query(\"customers.csv\").sql(\"\"\"\n    SELECT c.name, COUNT(o.order_id) as total_orders\n    FROM customers c\n    JOIN orders o ON c.id = o.customer_id\n    GROUP BY c.name\n    ORDER BY total_orders DESC\n\"\"\")\n\n# Load\nfor row in result:\n    # Send to database, API, etc.\n    print(row)\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<p>Now that you're familiar with the basics, explore:</p> <ul> <li>Core Concepts - Understand how SQLStream works</li> <li>SQL Support - Learn all supported SQL features</li> <li>CLI Reference - Master the command-line interface</li> <li>Python API - Deep dive into the programmatic API</li> <li>Examples - More real-world examples</li> </ul>"},{"location":"getting-started/quickstart/#need-help","title":"Need Help?","text":"<ul> <li>\ud83d\udcd6 Documentation</li> <li>\ud83d\udc1b Report Issues</li> <li>\ud83d\udcac Discussions</li> </ul>"},{"location":"reference/url-fragments/","title":"URL Fragment Syntax","text":"<p>SQLStream supports a powerful URL fragment syntax that allows you to specify the data format and table index directly in the source URL. This is particularly useful for URLs that don't have file extensions (like Pastebin) or files that contain multiple tables (like HTML or Markdown).</p>"},{"location":"reference/url-fragments/#syntax","title":"Syntax","text":"<p>The general syntax is:</p> <pre><code>source_url#format:table\n</code></pre> <ul> <li>source_url: The path to the file or HTTP URL.</li> <li>format: (Optional) The data format (e.g., <code>csv</code>, <code>json</code>, <code>parquet</code>, <code>html</code>, <code>markdown</code>).</li> <li>table: (Optional) The index of the table to read (0-indexed).</li> </ul>"},{"location":"reference/url-fragments/#components","title":"Components","text":"Component Description Example <code>format</code> Explicitly sets the parser to use. Overrides file extension detection. <code>#csv</code>, <code>#html</code> <code>table</code> Selects a specific table from a multi-table file. <code>:0</code>, <code>:1</code>, <code>:-1</code> Separator The <code>:</code> character separates format and table. <code>#html:1</code>"},{"location":"reference/url-fragments/#examples","title":"Examples","text":""},{"location":"reference/url-fragments/#specifying-format-only","title":"Specifying Format Only","text":"<p>Use this when the URL doesn't have a file extension or you want to override the detected format.</p> <pre><code>-- Read raw text from Pastebin as CSV\nSELECT * FROM \"https://pastebin.com/raw/xxxxx#csv\";\n\n-- Force CSV parsing for a .txt file\nSELECT * FROM \"data.txt#csv\";\n</code></pre>"},{"location":"reference/url-fragments/#specifying-table-only","title":"Specifying Table Only","text":"<p>Use this when the format is correctly detected (e.g., by extension) but you want to read a specific table.</p> <pre><code>-- Read the second table from an HTML file (index 1)\nSELECT * FROM \"data.html#:1\";\n\n-- Read the last table from a Markdown file\nSELECT * FROM \"README.md#:-1\";\n</code></pre>"},{"location":"reference/url-fragments/#specifying-format-and-table","title":"Specifying Format and Table","text":"<p>Use this for full control, especially for extension-less URLs containing multi-table formats.</p> <pre><code>-- Read the first table from a raw HTML URL\nSELECT * FROM \"https://example.com/raw/data#html:0\";\n\n-- Read the second table from a raw Markdown URL\nSELECT * FROM \"https://pastebin.com/raw/cnkgQp1t#markdown:1\";\n</code></pre>"},{"location":"reference/url-fragments/#supported-formats","title":"Supported Formats","text":"<p>The following formats support the fragment syntax:</p> <ul> <li>html: Supports table selection.</li> <li>markdown: Supports table selection.</li> <li>csv: Format specification only (table index ignored).</li> <li>parquet: Format specification only.</li> <li>json: Format specification only.</li> </ul>"},{"location":"reference/url-fragments/#table-indexing","title":"Table Indexing","text":"<ul> <li>Positive Index: <code>0</code> is the first table, <code>1</code> is the second, etc.</li> <li>Negative Index: <code>-1</code> is the last table, <code>-2</code> is the second to last, etc.</li> </ul>"},{"location":"reference/url-fragments/#error-handling","title":"Error Handling","text":"<ul> <li>If the specified format is invalid, an error will be raised.</li> <li>If the table index is out of range (e.g., requesting table 5 when only 2 exist), a <code>ValueError</code> will be raised with a helpful message indicating the number of available tables.</li> </ul>"}]}